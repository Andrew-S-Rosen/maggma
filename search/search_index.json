{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Maggma \u00b6 What is Maggma \u00b6 Maggma is a framework to build data pipelines from files on disk all the way to a REST API in scientific environments. Maggma has been developed by the Materials Project (MP) team at Lawrence Berkeley Labs. Maggma is written in Python and supports Python 3.6+. Installation from PyPI \u00b6 Maggma is published on the Python Package Index . The preferred tool for installing packages from PyPi is pip . This tool is provided with all modern versions of Python. Open your terminal and run the following command. pip install --upgrade maggma Installation from source \u00b6 You can install Maggma directly from a clone of the Git repository . This can be done either by cloning the repo and installing from the local clone, or simply installing directly via git . Local Clone git clone https://github.com//materialsproject/maggma cd maggma python setup.py install Direct Git pip install git+https://github.com/materialsproject/maggma","title":"Home"},{"location":"#maggma","text":"","title":"Maggma"},{"location":"#what-is-maggma","text":"Maggma is a framework to build data pipelines from files on disk all the way to a REST API in scientific environments. Maggma has been developed by the Materials Project (MP) team at Lawrence Berkeley Labs. Maggma is written in Python and supports Python 3.6+.","title":"What is Maggma"},{"location":"#installation-from-pypi","text":"Maggma is published on the Python Package Index . The preferred tool for installing packages from PyPi is pip . This tool is provided with all modern versions of Python. Open your terminal and run the following command. pip install --upgrade maggma","title":"Installation from PyPI"},{"location":"#installation-from-source","text":"You can install Maggma directly from a clone of the Git repository . This can be done either by cloning the repo and installing from the local clone, or simply installing directly via git . Local Clone git clone https://github.com//materialsproject/maggma cd maggma python setup.py install Direct Git pip install git+https://github.com/materialsproject/maggma","title":"Installation from source"},{"location":"CHANGELOG/","text":"Changelog \u00b6 v0.47.1 (2022-05-24) \u00b6 Full Changelog Merged pull requests: Fix gridfs URI store #667 ( utf ) v0.47.0 (2022-05-23) \u00b6 Full Changelog Merged pull requests: FileStore: a Store for files on disk #619 ( rkingsbury ) v0.46.2 (2022-05-23) \u00b6 Full Changelog Merged pull requests: allow s3 resource kwargs #665 ( jmmshn ) v0.46.1 (2022-04-21) \u00b6 Full Changelog Merged pull requests: Prefix fields input for read resource key endpoint #636 ( munrojm ) v0.46.0 (2022-04-19) \u00b6 Full Changelog Merged pull requests: S3 store and resource additions #635 ( munrojm ) v0.45.1 (2022-04-18) \u00b6 Full Changelog Merged pull requests: minor bug fix in remove_docs #626 ( jmmshn ) v0.45.0 (2022-04-14) \u00b6 Full Changelog Merged pull requests: Changes to core query operators and API #620 ( munrojm ) v0.44.5 (2022-04-12) \u00b6 Full Changelog Merged pull requests: JSONStore: file_writable -> read_only #625 ( rkingsbury ) v0.44.4 (2022-04-12) \u00b6 Full Changelog Merged pull requests: JSONStore: write file on init, add descriptive KeyError, add tests #624 ( rkingsbury ) v0.44.3 (2022-04-11) \u00b6 Full Changelog Merged pull requests: MemoryStore: fix groupby ignoring properties #621 ( rkingsbury ) v0.44.2 (2022-04-05) \u00b6 Full Changelog Merged pull requests: Force post-process method to take in query params #618 ( munrojm ) v0.44.1 (2022-03-08) \u00b6 Full Changelog Merged pull requests: added localhost test for MongoURIStore #595 ( jmmshn ) v0.44.0 (2022-03-07) \u00b6 Full Changelog v0.43.0 (2022-03-07) \u00b6 Full Changelog v0.42.0 (2022-03-07) \u00b6 Full Changelog Merged pull requests: Remove python3.6 support and fix tests #579 ( munrojm ) typo #576 ( jmmshn ) v0.41.1 (2022-03-05) \u00b6 Full Changelog Merged pull requests: mongoclient_kwargs #575 ( jmmshn ) change cleint -> resource in aws tests #574 ( jmmshn ) v0.41.0 (2022-02-15) \u00b6 Full Changelog Merged pull requests: Add header processing abilities to certain Resource classes #569 ( munrojm ) v0.40.0 (2022-02-10) \u00b6 Full Changelog Merged pull requests: Add authsource option for mongo and gridfs stores #567 ( utf ) v0.39.1 (2022-01-27) \u00b6 Full Changelog Fixed bugs: Single import-dependence on pynng causes M1 Mac install error #528 Merged pull requests: Add boto3 to required packages #544 ( munrojm ) v0.39.0 (2022-01-26) \u00b6 Full Changelog Merged pull requests: Replace pynng functionality with pyzmq #543 ( munrojm ) Encode _ as -- in metadata when using S3Store.write_doc_to_s3 #532 ( mkhorton ) v0.38.1 (2021-12-10) \u00b6 Full Changelog Merged pull requests: Add ability to input index hints to count method #524 ( munrojm ) v0.38.0 (2021-12-09) \u00b6 Full Changelog Merged pull requests: Fix issue with close and MongoStore and update _collection attribute #523 ( munrojm ) v0.37.0 (2021-12-07) \u00b6 Full Changelog Merged pull requests: Revert broken MongoStore auth testing #522 ( munrojm ) Fix authentication for MongoStore to work with pymongo==4.0 #521 ( munrojm ) v0.36.0 (2021-12-06) \u00b6 Full Changelog Merged pull requests: Added on-disk MongoDB compatible MontyStore #514 ( utf ) v0.35.0 (2021-12-01) \u00b6 Full Changelog v0.34.0 (2021-12-01) \u00b6 Full Changelog Merged pull requests: Changes to accommodate new pymongo #517 ( munrojm ) v0.33.2 (2021-12-01) \u00b6 Full Changelog Merged pull requests: Patch mongo store connect methods #516 ( munrojm ) v0.33.1 (2021-12-01) \u00b6 Full Changelog Merged pull requests: Patch memory store connect method #515 ( munrojm ) v0.33.0 (2021-11-30) \u00b6 Full Changelog Merged pull requests: MongoDB hint support #513 ( munrojm ) v0.32.3 (2021-11-25) \u00b6 Full Changelog Merged pull requests: Added option for writable JSONStores (for single JSON files only). #507 ( davidwaroquiers ) v0.32.2 (2021-11-23) \u00b6 Full Changelog Merged pull requests: Alter sorting query operator to take comma delimited string #510 ( munrojm ) v0.32.1 (2021-11-10) \u00b6 Full Changelog Merged pull requests: Default to yaml full loader to fix tests #505 ( munrojm ) Add GridFSURIStore with support for URI connections #504 ( utf ) v0.32.0 (2021-10-11) \u00b6 Full Changelog Merged pull requests: Update sorting query operator to take multiple fields #500 ( munrojm ) Change to S3Store serialization behavior in update() and other Mongolike Store changes #493 ( sivonxay ) v0.31.0 (2021-08-14) \u00b6 Full Changelog Merged pull requests: Add from_launchpad_file classmethod to MongoStore #476 ( sivonxay ) v0.30.4 (2021-08-04) \u00b6 Full Changelog Merged pull requests: Fix documentation in aggregation and sparse fields #469 ( munrojm ) v0.30.3 (2021-08-04) \u00b6 Full Changelog Merged pull requests: Enable enhanced documentation #468 ( munrojm ) v0.30.2 (2021-07-09) \u00b6 Full Changelog Merged pull requests: orjson added to setup.py #465 ( munrojm ) v0.30.1 (2021-07-09) \u00b6 Full Changelog Merged pull requests: Switch from monty to orjson for serialization #464 ( munrojm ) v0.30.0 (2021-07-06) \u00b6 Full Changelog Merged pull requests: Enable monty encoded direct responses #463 ( munrojm ) v0.29.4 (2021-06-23) \u00b6 Full Changelog Merged pull requests: BugFix: Manual distinct in MongoStore not using Criteria #461 ( shyamd ) v0.29.3 (2021-06-21) \u00b6 Full Changelog Merged pull requests: Sort query and query operator meta bug fixes #453 ( munrojm ) v0.29.2 (2021-06-18) \u00b6 Full Changelog Merged pull requests: Fix API Sanitizing MSONable types in combined types #454 ( shyamd ) v0.29.1 (2021-06-15) \u00b6 Full Changelog Merged pull requests: Switch from classic bson to pymongo bson #452 ( shyamd ) v0.29.0 (2021-06-08) \u00b6 Full Changelog Merged pull requests: Maggma API additions #448 ( munrojm ) v0.28.1 (2021-06-08) \u00b6 Full Changelog Closed issues: Indescriptive error when not specifying any builders in CLI #446 Add port auto-negotiation #445 Merged pull requests: New release wflow #450 ( shyamd ) Ensure Store.name is a property #449 ( utf ) v0.28.0 (2021-05-26) \u00b6 Full Changelog Merged pull requests: Updates the CLI Runner #447 ( shyamd ) v0.27.0 (2021-05-12) \u00b6 Full Changelog Closed issues: Python 3.6 compatability #336 Merged pull requests: Fix aws module import #435 ( utf ) coverage #430 ( jmmshn ) Update AWS Bucket Detection #429 ( jmmshn ) Add Object Hash to S3Store #427 ( jmmshn ) Rebuild API module #423 ( shyamd ) updated documentaion. #419 ( jmmshn ) Revert \"Bump ipython from 7.16.1 to 7.21.0\" #406 ( shyamd ) update gridfs store #381 ( gpetretto ) v0.26.0 (2021-01-16) \u00b6 Full Changelog Merged pull requests: No Progress bars #382 ( shyamd ) v0.25.0 (2020-12-04) \u00b6 Full Changelog Closed issues: FEATURE: Jupyter Commands #276 Merged pull requests: Python 3.6 Compatibility #352 ( shyamd ) Automatically parse the dbname from the URI #350 ( jmmshn ) Setup: msgpack-python was renamed to msgpack #344 ( jan-janssen ) Ensure MongoStore can safely continue updating when documents are too large #338 ( shyamd ) v0.24.2 (2020-11-17) \u00b6 Full Changelog Merged pull requests: Fix array unwrapping in distinct #335 ( shyamd ) v0.24.1 (2020-11-17) \u00b6 Full Changelog Closed issues: mrun failure with 'dict' object has no attribute 'connect' #316 FEATURE: Serialized SSH Tunnel #290 Merged pull requests: Fix Distinct in MongoStore #332 ( shyamd ) Direct passing of AWS login to S3Store #326 ( jmmshn ) Wrap SSHTunnelForward and make it MSONable #320 ( shyamd ) v0.24.0 (2020-11-02) \u00b6 Full Changelog Merged pull requests: Small fix to make sure searchable_fields are updated #303 ( jmmshn ) v0.23.3 (2020-09-23) \u00b6 Full Changelog v0.23.2 (2020-09-23) \u00b6 Full Changelog v0.23.1 (2020-09-21) \u00b6 Full Changelog Closed issues: FEATURE: Python file runner #277 v0.23.0 (2020-09-14) \u00b6 Full Changelog Closed issues: Separate out S3 Object reference keys from search keys #206 Merged pull requests: Add custom source loading #278 ( shyamd ) Inject metadata via fields rather than by indicies #265 ( shyamd ) v0.22.3 (2020-08-26) \u00b6 Full Changelog Merged pull requests: added context manager for stores #258 ( jmmshn ) v0.22.2 (2020-08-21) \u00b6 Full Changelog Merged pull requests: Minor bug fixes to S3Store #253 ( jmmshn ) v0.22.1 (2020-08-11) \u00b6 Full Changelog Merged pull requests: accept int as sort keys instead of Sort() in .query() and .groupby() #241 ( rkingsbury ) Update setup.py #225 ( jmmshn ) v0.22.0 (2020-07-16) \u00b6 Full Changelog Merged pull requests: Ensure Metadata in Documents from GridFS #222 ( shyamd ) Projection_Builder tests #213 ( acrutt ) [WIP] Proper multithreading and msgpack fix #205 ( jmmshn ) Fix projection_builder.update_targets() #179 ( acrutt ) v0.21.0 (2020-06-22) \u00b6 Full Changelog Merged pull requests: Reconstruct metadata from index in S3 Store #182 ( jmmshn ) MapBuilder retry_failed Fix #180 ( acrutt ) MapBuilder retry_failed bug #111 ( acrutt ) v0.20.0 (2020-05-02) \u00b6 Full Changelog Merged pull requests: Initial Drone Implementation #145 ( wuxiaohua1011 ) parallel s3 store wrting #130 ( jmmshn ) Make GridFSStore query check files store first. #128 ( munrojm ) v0.19.1 (2020-04-06) \u00b6 Full Changelog v0.19.0 (2020-04-06) \u00b6 Full Changelog Closed issues: ISSUE: newer_in method incompatible with GridFSStore #113 Merged pull requests: Fix async #129 ( shyamd ) small fixes #115 ( jmmshn ) Store updates #114 ( jmmshn ) [WIP] Add EndpointCluster and ClusterManager to maggma #66 ( wuxiaohua1011 ) v0.18.0 (2020-03-23) \u00b6 Full Changelog Merged pull requests: Amazon S3 store update #110 ( munrojm ) v0.17.3 (2020-03-18) \u00b6 Full Changelog v0.17.2 (2020-03-13) \u00b6 Full Changelog v0.17.1 (2020-03-12) \u00b6 Full Changelog Merged pull requests: Various Bug Fixes #109 ( shyamd ) Addition of Projection Builder #99 ( acrutt ) Fix issues with last_updated in MapBuilder #98 ( shyamd ) autonotebook for tqdm #97 ( shyamd ) v0.16.1 (2020-01-28) \u00b6 Full Changelog v0.16.0 (2020-01-28) \u00b6 Full Changelog Closed issues: Onotology generation from builder #59 Merged pull requests: Add MongoURIStore #93 ( shyamd ) Update distinct to be more like mongo distinct #92 ( shyamd ) Add count to maggma store #86 ( shyamd ) v0.15.0 (2020-01-23) \u00b6 Full Changelog Closed issues: Builder Reporting #78 ZeroMQ based multi-node processing #76 Add time limits to process_item? (Possibly just in MapBuilder?) #45 Merged pull requests: [WIP] Builder Reporting #80 ( shyamd ) Updated GroupBuilder #79 ( shyamd ) New Distributed Processor #77 ( shyamd ) v0.14.1 (2020-01-10) \u00b6 Full Changelog v0.14.0 (2020-01-10) \u00b6 Full Changelog Closed issues: Preserve last_updated for MapBuilder #58 Move away from mpi4py #51 Run serial processor directly from builder #48 Update while processing #42 Running JSONStore.connect() multiple times leads to undefined behavior #40 get_criteria directly invokes mongo commands #38 Cursor timeouts common #35 Possible solution to \"stalled\" Runner.run ? #29 Merged pull requests: Release Workflow for Github #75 ( shyamd ) Documentation #74 ( shyamd ) Reorg code #69 ( shyamd ) Updates for new monitoring services #67 ( shyamd ) fix GridFSStore #64 ( gpetretto ) Massive refactoring to get ready for v1.0 #62 ( shyamd ) Bug Fixes #61 ( shyamd ) GridFSStore bug fix #60 ( munrojm ) Fix Store serialization with @version #57 ( mkhorton ) Update builder to work with new monty #56 ( mkhorton ) v0.13.0 (2019-03-29) \u00b6 Full Changelog Merged pull requests: Add timeout to MapBuilder, store process time #54 ( mkhorton ) Can update pyyaml req? #50 ( dwinston ) Concat store #47 ( shyamd ) v0.12.0 (2018-11-19) \u00b6 Full Changelog v0.11.0 (2018-11-01) \u00b6 Full Changelog Merged pull requests: Better printing of validation erorrs #46 ( mkhorton ) Updates to JointStore and MapBuilder #44 ( shyamd ) v0.9.0 (2018-10-01) \u00b6 Full Changelog Closed issues: Non-obvious error message when trying to query a Store that hasn't been connected #41 Criteria/properties order of MongoStore.query #37 tqdm in Jupyter #33 query args order #31 Merged pull requests: Simplification of Validator class + tests #39 ( mkhorton ) Fix for Jupyter detection for tqdm #36 ( mkhorton ) Add tqdm widget inside Jupyter #34 ( mkhorton ) Change update_targets log level from debug to exception #32 ( mkhorton ) Jointstore #23 ( montoyjh ) v0.8.0 (2018-08-22) \u00b6 Full Changelog Merged pull requests: [WIP] Improve/refactor examples and move inside maggma namespace #30 ( dwinston ) Fix mrun with default num_workers. Add test. #28 ( dwinston ) v0.6.5 (2018-06-07) \u00b6 Full Changelog v0.6.4 (2018-06-07) \u00b6 Full Changelog v0.6.3 (2018-06-07) \u00b6 Full Changelog Merged pull requests: Add MongograntStore #27 ( dwinston ) v0.6.2 (2018-06-01) \u00b6 Full Changelog v0.6.1 (2018-06-01) \u00b6 Full Changelog Merged pull requests: Help user if e.g. target store built without lu_field #26 ( dwinston ) v0.6.0 (2018-05-01) \u00b6 Full Changelog Implemented enhancements: Progress Bar #21 Query Engine equivalent #9 Merged pull requests: Progress Bars for Multiprocess Runner #24 ( shyamd ) GridFS Store update: use metadata field, update removes old file(s) #20 ( dwinston ) v0.5.0 (2018-03-31) \u00b6 Full Changelog Closed issues: Need from pymongo collection #18 Merged pull requests: Useability updates #19 ( shyamd ) 0.4.0 (2018-02-28) \u00b6 Full Changelog Merged pull requests: New Multiprocessor and MPI Processor #17 ( shyamd ) groupby change for memory/jsonstore #16 ( montoyjh ) Rename Schema to Validator #15 ( mkhorton ) 0.3.0 (2018-02-01) \u00b6 Full Changelog Implemented enhancements: Vault enabled Store #8 Merged pull requests: PR for generic Schema class #14 ( mkhorton ) Issue 8 vault store #13 ( shreddd ) adds grouping function and test to make aggregation-based builds #12 ( montoyjh ) v0.2.0 (2018-01-01) \u00b6 Full Changelog Closed issues: LU translation functions don't serialize #11 Merged pull requests: Mongolike mixin #10 ( montoyjh ) v0.1.0 (2017-11-08) \u00b6 Full Changelog Closed issues: ditch python 2 and support only 3? #3 Seeking clarifications #1 Merged pull requests: Do not wait until all items are processed to update targets #7 ( dwinston ) Run builder with either MPI or multiprocessing #6 ( matk86 ) add lava code and tool execution script #5 ( gilbertozp ) Add eclipse project files to .gitignore #2 ( gilbertozp ) * This Changelog was automatically generated by github_changelog_generator","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"","title":"Changelog"},{"location":"CHANGELOG/#v0471-2022-05-24","text":"Full Changelog Merged pull requests: Fix gridfs URI store #667 ( utf )","title":"v0.47.1 (2022-05-24)"},{"location":"CHANGELOG/#v0470-2022-05-23","text":"Full Changelog Merged pull requests: FileStore: a Store for files on disk #619 ( rkingsbury )","title":"v0.47.0 (2022-05-23)"},{"location":"CHANGELOG/#v0462-2022-05-23","text":"Full Changelog Merged pull requests: allow s3 resource kwargs #665 ( jmmshn )","title":"v0.46.2 (2022-05-23)"},{"location":"CHANGELOG/#v0461-2022-04-21","text":"Full Changelog Merged pull requests: Prefix fields input for read resource key endpoint #636 ( munrojm )","title":"v0.46.1 (2022-04-21)"},{"location":"CHANGELOG/#v0460-2022-04-19","text":"Full Changelog Merged pull requests: S3 store and resource additions #635 ( munrojm )","title":"v0.46.0 (2022-04-19)"},{"location":"CHANGELOG/#v0451-2022-04-18","text":"Full Changelog Merged pull requests: minor bug fix in remove_docs #626 ( jmmshn )","title":"v0.45.1 (2022-04-18)"},{"location":"CHANGELOG/#v0450-2022-04-14","text":"Full Changelog Merged pull requests: Changes to core query operators and API #620 ( munrojm )","title":"v0.45.0 (2022-04-14)"},{"location":"CHANGELOG/#v0445-2022-04-12","text":"Full Changelog Merged pull requests: JSONStore: file_writable -> read_only #625 ( rkingsbury )","title":"v0.44.5 (2022-04-12)"},{"location":"CHANGELOG/#v0444-2022-04-12","text":"Full Changelog Merged pull requests: JSONStore: write file on init, add descriptive KeyError, add tests #624 ( rkingsbury )","title":"v0.44.4 (2022-04-12)"},{"location":"CHANGELOG/#v0443-2022-04-11","text":"Full Changelog Merged pull requests: MemoryStore: fix groupby ignoring properties #621 ( rkingsbury )","title":"v0.44.3 (2022-04-11)"},{"location":"CHANGELOG/#v0442-2022-04-05","text":"Full Changelog Merged pull requests: Force post-process method to take in query params #618 ( munrojm )","title":"v0.44.2 (2022-04-05)"},{"location":"CHANGELOG/#v0441-2022-03-08","text":"Full Changelog Merged pull requests: added localhost test for MongoURIStore #595 ( jmmshn )","title":"v0.44.1 (2022-03-08)"},{"location":"CHANGELOG/#v0440-2022-03-07","text":"Full Changelog","title":"v0.44.0 (2022-03-07)"},{"location":"CHANGELOG/#v0430-2022-03-07","text":"Full Changelog","title":"v0.43.0 (2022-03-07)"},{"location":"CHANGELOG/#v0420-2022-03-07","text":"Full Changelog Merged pull requests: Remove python3.6 support and fix tests #579 ( munrojm ) typo #576 ( jmmshn )","title":"v0.42.0 (2022-03-07)"},{"location":"CHANGELOG/#v0411-2022-03-05","text":"Full Changelog Merged pull requests: mongoclient_kwargs #575 ( jmmshn ) change cleint -> resource in aws tests #574 ( jmmshn )","title":"v0.41.1 (2022-03-05)"},{"location":"CHANGELOG/#v0410-2022-02-15","text":"Full Changelog Merged pull requests: Add header processing abilities to certain Resource classes #569 ( munrojm )","title":"v0.41.0 (2022-02-15)"},{"location":"CHANGELOG/#v0400-2022-02-10","text":"Full Changelog Merged pull requests: Add authsource option for mongo and gridfs stores #567 ( utf )","title":"v0.40.0 (2022-02-10)"},{"location":"CHANGELOG/#v0391-2022-01-27","text":"Full Changelog Fixed bugs: Single import-dependence on pynng causes M1 Mac install error #528 Merged pull requests: Add boto3 to required packages #544 ( munrojm )","title":"v0.39.1 (2022-01-27)"},{"location":"CHANGELOG/#v0390-2022-01-26","text":"Full Changelog Merged pull requests: Replace pynng functionality with pyzmq #543 ( munrojm ) Encode _ as -- in metadata when using S3Store.write_doc_to_s3 #532 ( mkhorton )","title":"v0.39.0 (2022-01-26)"},{"location":"CHANGELOG/#v0381-2021-12-10","text":"Full Changelog Merged pull requests: Add ability to input index hints to count method #524 ( munrojm )","title":"v0.38.1 (2021-12-10)"},{"location":"CHANGELOG/#v0380-2021-12-09","text":"Full Changelog Merged pull requests: Fix issue with close and MongoStore and update _collection attribute #523 ( munrojm )","title":"v0.38.0 (2021-12-09)"},{"location":"CHANGELOG/#v0370-2021-12-07","text":"Full Changelog Merged pull requests: Revert broken MongoStore auth testing #522 ( munrojm ) Fix authentication for MongoStore to work with pymongo==4.0 #521 ( munrojm )","title":"v0.37.0 (2021-12-07)"},{"location":"CHANGELOG/#v0360-2021-12-06","text":"Full Changelog Merged pull requests: Added on-disk MongoDB compatible MontyStore #514 ( utf )","title":"v0.36.0 (2021-12-06)"},{"location":"CHANGELOG/#v0350-2021-12-01","text":"Full Changelog","title":"v0.35.0 (2021-12-01)"},{"location":"CHANGELOG/#v0340-2021-12-01","text":"Full Changelog Merged pull requests: Changes to accommodate new pymongo #517 ( munrojm )","title":"v0.34.0 (2021-12-01)"},{"location":"CHANGELOG/#v0332-2021-12-01","text":"Full Changelog Merged pull requests: Patch mongo store connect methods #516 ( munrojm )","title":"v0.33.2 (2021-12-01)"},{"location":"CHANGELOG/#v0331-2021-12-01","text":"Full Changelog Merged pull requests: Patch memory store connect method #515 ( munrojm )","title":"v0.33.1 (2021-12-01)"},{"location":"CHANGELOG/#v0330-2021-11-30","text":"Full Changelog Merged pull requests: MongoDB hint support #513 ( munrojm )","title":"v0.33.0 (2021-11-30)"},{"location":"CHANGELOG/#v0323-2021-11-25","text":"Full Changelog Merged pull requests: Added option for writable JSONStores (for single JSON files only). #507 ( davidwaroquiers )","title":"v0.32.3 (2021-11-25)"},{"location":"CHANGELOG/#v0322-2021-11-23","text":"Full Changelog Merged pull requests: Alter sorting query operator to take comma delimited string #510 ( munrojm )","title":"v0.32.2 (2021-11-23)"},{"location":"CHANGELOG/#v0321-2021-11-10","text":"Full Changelog Merged pull requests: Default to yaml full loader to fix tests #505 ( munrojm ) Add GridFSURIStore with support for URI connections #504 ( utf )","title":"v0.32.1 (2021-11-10)"},{"location":"CHANGELOG/#v0320-2021-10-11","text":"Full Changelog Merged pull requests: Update sorting query operator to take multiple fields #500 ( munrojm ) Change to S3Store serialization behavior in update() and other Mongolike Store changes #493 ( sivonxay )","title":"v0.32.0 (2021-10-11)"},{"location":"CHANGELOG/#v0310-2021-08-14","text":"Full Changelog Merged pull requests: Add from_launchpad_file classmethod to MongoStore #476 ( sivonxay )","title":"v0.31.0 (2021-08-14)"},{"location":"CHANGELOG/#v0304-2021-08-04","text":"Full Changelog Merged pull requests: Fix documentation in aggregation and sparse fields #469 ( munrojm )","title":"v0.30.4 (2021-08-04)"},{"location":"CHANGELOG/#v0303-2021-08-04","text":"Full Changelog Merged pull requests: Enable enhanced documentation #468 ( munrojm )","title":"v0.30.3 (2021-08-04)"},{"location":"CHANGELOG/#v0302-2021-07-09","text":"Full Changelog Merged pull requests: orjson added to setup.py #465 ( munrojm )","title":"v0.30.2 (2021-07-09)"},{"location":"CHANGELOG/#v0301-2021-07-09","text":"Full Changelog Merged pull requests: Switch from monty to orjson for serialization #464 ( munrojm )","title":"v0.30.1 (2021-07-09)"},{"location":"CHANGELOG/#v0300-2021-07-06","text":"Full Changelog Merged pull requests: Enable monty encoded direct responses #463 ( munrojm )","title":"v0.30.0 (2021-07-06)"},{"location":"CHANGELOG/#v0294-2021-06-23","text":"Full Changelog Merged pull requests: BugFix: Manual distinct in MongoStore not using Criteria #461 ( shyamd )","title":"v0.29.4 (2021-06-23)"},{"location":"CHANGELOG/#v0293-2021-06-21","text":"Full Changelog Merged pull requests: Sort query and query operator meta bug fixes #453 ( munrojm )","title":"v0.29.3 (2021-06-21)"},{"location":"CHANGELOG/#v0292-2021-06-18","text":"Full Changelog Merged pull requests: Fix API Sanitizing MSONable types in combined types #454 ( shyamd )","title":"v0.29.2 (2021-06-18)"},{"location":"CHANGELOG/#v0291-2021-06-15","text":"Full Changelog Merged pull requests: Switch from classic bson to pymongo bson #452 ( shyamd )","title":"v0.29.1 (2021-06-15)"},{"location":"CHANGELOG/#v0290-2021-06-08","text":"Full Changelog Merged pull requests: Maggma API additions #448 ( munrojm )","title":"v0.29.0 (2021-06-08)"},{"location":"CHANGELOG/#v0281-2021-06-08","text":"Full Changelog Closed issues: Indescriptive error when not specifying any builders in CLI #446 Add port auto-negotiation #445 Merged pull requests: New release wflow #450 ( shyamd ) Ensure Store.name is a property #449 ( utf )","title":"v0.28.1 (2021-06-08)"},{"location":"CHANGELOG/#v0280-2021-05-26","text":"Full Changelog Merged pull requests: Updates the CLI Runner #447 ( shyamd )","title":"v0.28.0 (2021-05-26)"},{"location":"CHANGELOG/#v0270-2021-05-12","text":"Full Changelog Closed issues: Python 3.6 compatability #336 Merged pull requests: Fix aws module import #435 ( utf ) coverage #430 ( jmmshn ) Update AWS Bucket Detection #429 ( jmmshn ) Add Object Hash to S3Store #427 ( jmmshn ) Rebuild API module #423 ( shyamd ) updated documentaion. #419 ( jmmshn ) Revert \"Bump ipython from 7.16.1 to 7.21.0\" #406 ( shyamd ) update gridfs store #381 ( gpetretto )","title":"v0.27.0 (2021-05-12)"},{"location":"CHANGELOG/#v0260-2021-01-16","text":"Full Changelog Merged pull requests: No Progress bars #382 ( shyamd )","title":"v0.26.0 (2021-01-16)"},{"location":"CHANGELOG/#v0250-2020-12-04","text":"Full Changelog Closed issues: FEATURE: Jupyter Commands #276 Merged pull requests: Python 3.6 Compatibility #352 ( shyamd ) Automatically parse the dbname from the URI #350 ( jmmshn ) Setup: msgpack-python was renamed to msgpack #344 ( jan-janssen ) Ensure MongoStore can safely continue updating when documents are too large #338 ( shyamd )","title":"v0.25.0 (2020-12-04)"},{"location":"CHANGELOG/#v0242-2020-11-17","text":"Full Changelog Merged pull requests: Fix array unwrapping in distinct #335 ( shyamd )","title":"v0.24.2 (2020-11-17)"},{"location":"CHANGELOG/#v0241-2020-11-17","text":"Full Changelog Closed issues: mrun failure with 'dict' object has no attribute 'connect' #316 FEATURE: Serialized SSH Tunnel #290 Merged pull requests: Fix Distinct in MongoStore #332 ( shyamd ) Direct passing of AWS login to S3Store #326 ( jmmshn ) Wrap SSHTunnelForward and make it MSONable #320 ( shyamd )","title":"v0.24.1 (2020-11-17)"},{"location":"CHANGELOG/#v0240-2020-11-02","text":"Full Changelog Merged pull requests: Small fix to make sure searchable_fields are updated #303 ( jmmshn )","title":"v0.24.0 (2020-11-02)"},{"location":"CHANGELOG/#v0233-2020-09-23","text":"Full Changelog","title":"v0.23.3 (2020-09-23)"},{"location":"CHANGELOG/#v0232-2020-09-23","text":"Full Changelog","title":"v0.23.2 (2020-09-23)"},{"location":"CHANGELOG/#v0231-2020-09-21","text":"Full Changelog Closed issues: FEATURE: Python file runner #277","title":"v0.23.1 (2020-09-21)"},{"location":"CHANGELOG/#v0230-2020-09-14","text":"Full Changelog Closed issues: Separate out S3 Object reference keys from search keys #206 Merged pull requests: Add custom source loading #278 ( shyamd ) Inject metadata via fields rather than by indicies #265 ( shyamd )","title":"v0.23.0 (2020-09-14)"},{"location":"CHANGELOG/#v0223-2020-08-26","text":"Full Changelog Merged pull requests: added context manager for stores #258 ( jmmshn )","title":"v0.22.3 (2020-08-26)"},{"location":"CHANGELOG/#v0222-2020-08-21","text":"Full Changelog Merged pull requests: Minor bug fixes to S3Store #253 ( jmmshn )","title":"v0.22.2 (2020-08-21)"},{"location":"CHANGELOG/#v0221-2020-08-11","text":"Full Changelog Merged pull requests: accept int as sort keys instead of Sort() in .query() and .groupby() #241 ( rkingsbury ) Update setup.py #225 ( jmmshn )","title":"v0.22.1 (2020-08-11)"},{"location":"CHANGELOG/#v0220-2020-07-16","text":"Full Changelog Merged pull requests: Ensure Metadata in Documents from GridFS #222 ( shyamd ) Projection_Builder tests #213 ( acrutt ) [WIP] Proper multithreading and msgpack fix #205 ( jmmshn ) Fix projection_builder.update_targets() #179 ( acrutt )","title":"v0.22.0 (2020-07-16)"},{"location":"CHANGELOG/#v0210-2020-06-22","text":"Full Changelog Merged pull requests: Reconstruct metadata from index in S3 Store #182 ( jmmshn ) MapBuilder retry_failed Fix #180 ( acrutt ) MapBuilder retry_failed bug #111 ( acrutt )","title":"v0.21.0 (2020-06-22)"},{"location":"CHANGELOG/#v0200-2020-05-02","text":"Full Changelog Merged pull requests: Initial Drone Implementation #145 ( wuxiaohua1011 ) parallel s3 store wrting #130 ( jmmshn ) Make GridFSStore query check files store first. #128 ( munrojm )","title":"v0.20.0 (2020-05-02)"},{"location":"CHANGELOG/#v0191-2020-04-06","text":"Full Changelog","title":"v0.19.1 (2020-04-06)"},{"location":"CHANGELOG/#v0190-2020-04-06","text":"Full Changelog Closed issues: ISSUE: newer_in method incompatible with GridFSStore #113 Merged pull requests: Fix async #129 ( shyamd ) small fixes #115 ( jmmshn ) Store updates #114 ( jmmshn ) [WIP] Add EndpointCluster and ClusterManager to maggma #66 ( wuxiaohua1011 )","title":"v0.19.0 (2020-04-06)"},{"location":"CHANGELOG/#v0180-2020-03-23","text":"Full Changelog Merged pull requests: Amazon S3 store update #110 ( munrojm )","title":"v0.18.0 (2020-03-23)"},{"location":"CHANGELOG/#v0173-2020-03-18","text":"Full Changelog","title":"v0.17.3 (2020-03-18)"},{"location":"CHANGELOG/#v0172-2020-03-13","text":"Full Changelog","title":"v0.17.2 (2020-03-13)"},{"location":"CHANGELOG/#v0171-2020-03-12","text":"Full Changelog Merged pull requests: Various Bug Fixes #109 ( shyamd ) Addition of Projection Builder #99 ( acrutt ) Fix issues with last_updated in MapBuilder #98 ( shyamd ) autonotebook for tqdm #97 ( shyamd )","title":"v0.17.1 (2020-03-12)"},{"location":"CHANGELOG/#v0161-2020-01-28","text":"Full Changelog","title":"v0.16.1 (2020-01-28)"},{"location":"CHANGELOG/#v0160-2020-01-28","text":"Full Changelog Closed issues: Onotology generation from builder #59 Merged pull requests: Add MongoURIStore #93 ( shyamd ) Update distinct to be more like mongo distinct #92 ( shyamd ) Add count to maggma store #86 ( shyamd )","title":"v0.16.0 (2020-01-28)"},{"location":"CHANGELOG/#v0150-2020-01-23","text":"Full Changelog Closed issues: Builder Reporting #78 ZeroMQ based multi-node processing #76 Add time limits to process_item? (Possibly just in MapBuilder?) #45 Merged pull requests: [WIP] Builder Reporting #80 ( shyamd ) Updated GroupBuilder #79 ( shyamd ) New Distributed Processor #77 ( shyamd )","title":"v0.15.0 (2020-01-23)"},{"location":"CHANGELOG/#v0141-2020-01-10","text":"Full Changelog","title":"v0.14.1 (2020-01-10)"},{"location":"CHANGELOG/#v0140-2020-01-10","text":"Full Changelog Closed issues: Preserve last_updated for MapBuilder #58 Move away from mpi4py #51 Run serial processor directly from builder #48 Update while processing #42 Running JSONStore.connect() multiple times leads to undefined behavior #40 get_criteria directly invokes mongo commands #38 Cursor timeouts common #35 Possible solution to \"stalled\" Runner.run ? #29 Merged pull requests: Release Workflow for Github #75 ( shyamd ) Documentation #74 ( shyamd ) Reorg code #69 ( shyamd ) Updates for new monitoring services #67 ( shyamd ) fix GridFSStore #64 ( gpetretto ) Massive refactoring to get ready for v1.0 #62 ( shyamd ) Bug Fixes #61 ( shyamd ) GridFSStore bug fix #60 ( munrojm ) Fix Store serialization with @version #57 ( mkhorton ) Update builder to work with new monty #56 ( mkhorton )","title":"v0.14.0 (2020-01-10)"},{"location":"CHANGELOG/#v0130-2019-03-29","text":"Full Changelog Merged pull requests: Add timeout to MapBuilder, store process time #54 ( mkhorton ) Can update pyyaml req? #50 ( dwinston ) Concat store #47 ( shyamd )","title":"v0.13.0 (2019-03-29)"},{"location":"CHANGELOG/#v0120-2018-11-19","text":"Full Changelog","title":"v0.12.0 (2018-11-19)"},{"location":"CHANGELOG/#v0110-2018-11-01","text":"Full Changelog Merged pull requests: Better printing of validation erorrs #46 ( mkhorton ) Updates to JointStore and MapBuilder #44 ( shyamd )","title":"v0.11.0 (2018-11-01)"},{"location":"CHANGELOG/#v090-2018-10-01","text":"Full Changelog Closed issues: Non-obvious error message when trying to query a Store that hasn't been connected #41 Criteria/properties order of MongoStore.query #37 tqdm in Jupyter #33 query args order #31 Merged pull requests: Simplification of Validator class + tests #39 ( mkhorton ) Fix for Jupyter detection for tqdm #36 ( mkhorton ) Add tqdm widget inside Jupyter #34 ( mkhorton ) Change update_targets log level from debug to exception #32 ( mkhorton ) Jointstore #23 ( montoyjh )","title":"v0.9.0 (2018-10-01)"},{"location":"CHANGELOG/#v080-2018-08-22","text":"Full Changelog Merged pull requests: [WIP] Improve/refactor examples and move inside maggma namespace #30 ( dwinston ) Fix mrun with default num_workers. Add test. #28 ( dwinston )","title":"v0.8.0 (2018-08-22)"},{"location":"CHANGELOG/#v065-2018-06-07","text":"Full Changelog","title":"v0.6.5 (2018-06-07)"},{"location":"CHANGELOG/#v064-2018-06-07","text":"Full Changelog","title":"v0.6.4 (2018-06-07)"},{"location":"CHANGELOG/#v063-2018-06-07","text":"Full Changelog Merged pull requests: Add MongograntStore #27 ( dwinston )","title":"v0.6.3 (2018-06-07)"},{"location":"CHANGELOG/#v062-2018-06-01","text":"Full Changelog","title":"v0.6.2 (2018-06-01)"},{"location":"CHANGELOG/#v061-2018-06-01","text":"Full Changelog Merged pull requests: Help user if e.g. target store built without lu_field #26 ( dwinston )","title":"v0.6.1 (2018-06-01)"},{"location":"CHANGELOG/#v060-2018-05-01","text":"Full Changelog Implemented enhancements: Progress Bar #21 Query Engine equivalent #9 Merged pull requests: Progress Bars for Multiprocess Runner #24 ( shyamd ) GridFS Store update: use metadata field, update removes old file(s) #20 ( dwinston )","title":"v0.6.0 (2018-05-01)"},{"location":"CHANGELOG/#v050-2018-03-31","text":"Full Changelog Closed issues: Need from pymongo collection #18 Merged pull requests: Useability updates #19 ( shyamd )","title":"v0.5.0 (2018-03-31)"},{"location":"CHANGELOG/#040-2018-02-28","text":"Full Changelog Merged pull requests: New Multiprocessor and MPI Processor #17 ( shyamd ) groupby change for memory/jsonstore #16 ( montoyjh ) Rename Schema to Validator #15 ( mkhorton )","title":"0.4.0 (2018-02-28)"},{"location":"CHANGELOG/#030-2018-02-01","text":"Full Changelog Implemented enhancements: Vault enabled Store #8 Merged pull requests: PR for generic Schema class #14 ( mkhorton ) Issue 8 vault store #13 ( shreddd ) adds grouping function and test to make aggregation-based builds #12 ( montoyjh )","title":"0.3.0 (2018-02-01)"},{"location":"CHANGELOG/#v020-2018-01-01","text":"Full Changelog Closed issues: LU translation functions don't serialize #11 Merged pull requests: Mongolike mixin #10 ( montoyjh )","title":"v0.2.0 (2018-01-01)"},{"location":"CHANGELOG/#v010-2017-11-08","text":"Full Changelog Closed issues: ditch python 2 and support only 3? #3 Seeking clarifications #1 Merged pull requests: Do not wait until all items are processed to update targets #7 ( dwinston ) Run builder with either MPI or multiprocessing #6 ( matk86 ) add lava code and tool execution script #5 ( gilbertozp ) Add eclipse project files to .gitignore #2 ( gilbertozp ) * This Changelog was automatically generated by github_changelog_generator","title":"v0.1.0 (2017-11-08)"},{"location":"concepts/","text":"Concepts \u00b6 MSONable \u00b6 One challenge in building complex data-transformation codes is keeping track of all the settings necessary to make some output database. One bad solution is to hard-code these settings, but then any modification is difficult to keep track of. Maggma solves this by putting the configuration with the pipeline definition in JSON or YAML files. This is done using the MSONable pattern, which requires that any Maggma object (the databases and transformation steps) can convert itself to a python dictionary with it's configuration parameters in a process called serialization. These dictionaries can then be converted back to the origianl Maggma object without having to know what class it belonged. MSONable does this by injecting in @class and @module keys that tell it where to find the original python code for that Maggma object. Store \u00b6 Another challenge is dealing with all the different types of databases out there. Maggma was originally built off MongoDB, so it's interface looks a lot like PyMongo . Still, there are a number of usefull new object databases that can be used to store large quantities of data you don't need to search in such as Amazon S3 and Google Cloud. It would be nice to have a single interface to all of these so you could write your datapipeline only once. Stores are databases containing organized document-based data. They represent either a data source or a data sink. They are modeled around the MongoDB collection although they can represent more complex data sources that auto-alias keys without the user knowing, or even providing concatenation or joining of Stores. Stores implement methods to connect , query , find distinct values, groupby fields, update documents, and remove documents. Stores also implement a number of critical fields for Maggma that help in efficient document processing: the key and the last_updated_field . key is the field that is used to uniquely index the underlying data source. last_updated_field is the timestamp of when that document was last modified. Builder \u00b6 Builders represent a data processing step. Builders break down each transformation into 3 phases: get_items , process_item , and update_targets : get_items : Retrieve items from the source Store(s) for processing by the next phase process_item : Manipulate the input item and create an output document that is sent to the next phase for storage. update_target : Add the processed item to the target Store(s). Both get_items and update_targets can perform IO (input/output) to the data stores. process_item is expected to not perform any IO so that it can be parallelized by Maggma. Builders can be chained together into an array and then saved as a JSON file to be run on a production system. Drone \u00b6 Drone is a standardized class to synchronize local files and data in your database. It breaks down the process in 4 steps: get_items Given a folder path to a data folder, read all the files, and return a dictionary that maps each RecordKey -> List of RecordIdentifier should_update_records Given a list of RecordIdentifier , it query the database return a list of RecordIdentifier that requires update process_item (from Builder ) Given a single RecordIdentifier , return the data that it refers to and add meta data update_targets updates the database given a list of data","title":"Core Concepts"},{"location":"concepts/#concepts","text":"","title":"Concepts"},{"location":"concepts/#msonable","text":"One challenge in building complex data-transformation codes is keeping track of all the settings necessary to make some output database. One bad solution is to hard-code these settings, but then any modification is difficult to keep track of. Maggma solves this by putting the configuration with the pipeline definition in JSON or YAML files. This is done using the MSONable pattern, which requires that any Maggma object (the databases and transformation steps) can convert itself to a python dictionary with it's configuration parameters in a process called serialization. These dictionaries can then be converted back to the origianl Maggma object without having to know what class it belonged. MSONable does this by injecting in @class and @module keys that tell it where to find the original python code for that Maggma object.","title":"MSONable"},{"location":"concepts/#store","text":"Another challenge is dealing with all the different types of databases out there. Maggma was originally built off MongoDB, so it's interface looks a lot like PyMongo . Still, there are a number of usefull new object databases that can be used to store large quantities of data you don't need to search in such as Amazon S3 and Google Cloud. It would be nice to have a single interface to all of these so you could write your datapipeline only once. Stores are databases containing organized document-based data. They represent either a data source or a data sink. They are modeled around the MongoDB collection although they can represent more complex data sources that auto-alias keys without the user knowing, or even providing concatenation or joining of Stores. Stores implement methods to connect , query , find distinct values, groupby fields, update documents, and remove documents. Stores also implement a number of critical fields for Maggma that help in efficient document processing: the key and the last_updated_field . key is the field that is used to uniquely index the underlying data source. last_updated_field is the timestamp of when that document was last modified.","title":"Store"},{"location":"concepts/#builder","text":"Builders represent a data processing step. Builders break down each transformation into 3 phases: get_items , process_item , and update_targets : get_items : Retrieve items from the source Store(s) for processing by the next phase process_item : Manipulate the input item and create an output document that is sent to the next phase for storage. update_target : Add the processed item to the target Store(s). Both get_items and update_targets can perform IO (input/output) to the data stores. process_item is expected to not perform any IO so that it can be parallelized by Maggma. Builders can be chained together into an array and then saved as a JSON file to be run on a production system.","title":"Builder"},{"location":"concepts/#drone","text":"Drone is a standardized class to synchronize local files and data in your database. It breaks down the process in 4 steps: get_items Given a folder path to a data folder, read all the files, and return a dictionary that maps each RecordKey -> List of RecordIdentifier should_update_records Given a list of RecordIdentifier , it query the database return a list of RecordIdentifier that requires update process_item (from Builder ) Given a single RecordIdentifier , return the data that it refers to and add meta data update_targets updates the database given a list of data","title":"Drone"},{"location":"getting_started/advanced_builder/","text":"Advanced Builder Concepts \u00b6 There are a number of features in maggma designed to assist with advanced features: Logging \u00b6 maggma builders have a python logger object that is already setup to output to the correct level. You can directly use it to output info , debug , and error messages. def get_items ( self ) -> Iterable : ... self . logger . info ( f \"Got { len ( to_process_ids ) } to process\" ) ... Querying for Updated Documents \u00b6 One of the most important features in a builder is incremental building which allows the builder to just process new documents. One of the parameters for a maggma store is the last_updated_field and the last_updated_type which tell maggma how to deal with dates in the source and target documents. This allows us to get the id of any documents that are newer in the target than the newest document in the source: new_ids = self . target . newer_in ( self . source ) Speeding up Data Transfers \u00b6 Since maggma is designed around Mongo style data sources and sinks, building indexes or in-memory copies of fields you want to search on is critical to get the fastest possible data input/output (IO). Since this is very builder and document style dependent, maggma provides a direct interface to ensure_indexes on a Store. A common paradigm is to do this in the beginning of get_items : def ensure_indexes ( self ): self . source . ensure_index ( \"some_search_fields\" ) self . target . ensure_index ( self . target . key ) def get_items ( self ) -> Iterable : self . ensure_indexes () ... Built in Templates for Advanced Builders \u00b6 maggma implements templates for builders that have many of these advanced features listed above: MapBuilder Creates one-to-one document mapping of items in the source Store to the transformed documents in the target Store. GroupBuilder Creates many-to-one document mapping of items in the source Store to transformed documents in the traget Store","title":"Advanced Builders"},{"location":"getting_started/advanced_builder/#advanced-builder-concepts","text":"There are a number of features in maggma designed to assist with advanced features:","title":"Advanced Builder Concepts"},{"location":"getting_started/advanced_builder/#logging","text":"maggma builders have a python logger object that is already setup to output to the correct level. You can directly use it to output info , debug , and error messages. def get_items ( self ) -> Iterable : ... self . logger . info ( f \"Got { len ( to_process_ids ) } to process\" ) ...","title":"Logging"},{"location":"getting_started/advanced_builder/#querying-for-updated-documents","text":"One of the most important features in a builder is incremental building which allows the builder to just process new documents. One of the parameters for a maggma store is the last_updated_field and the last_updated_type which tell maggma how to deal with dates in the source and target documents. This allows us to get the id of any documents that are newer in the target than the newest document in the source: new_ids = self . target . newer_in ( self . source )","title":"Querying for Updated Documents"},{"location":"getting_started/advanced_builder/#speeding-up-data-transfers","text":"Since maggma is designed around Mongo style data sources and sinks, building indexes or in-memory copies of fields you want to search on is critical to get the fastest possible data input/output (IO). Since this is very builder and document style dependent, maggma provides a direct interface to ensure_indexes on a Store. A common paradigm is to do this in the beginning of get_items : def ensure_indexes ( self ): self . source . ensure_index ( \"some_search_fields\" ) self . target . ensure_index ( self . target . key ) def get_items ( self ) -> Iterable : self . ensure_indexes () ...","title":"Speeding up Data Transfers"},{"location":"getting_started/advanced_builder/#built-in-templates-for-advanced-builders","text":"maggma implements templates for builders that have many of these advanced features listed above: MapBuilder Creates one-to-one document mapping of items in the source Store to the transformed documents in the target Store. GroupBuilder Creates many-to-one document mapping of items in the source Store to transformed documents in the traget Store","title":"Built in Templates for Advanced Builders"},{"location":"getting_started/advanced_stores/","text":"Configurations and Usage of Advanced store 's \u00b6 S3Store \u00b6 Configuration \u00b6 The S3Store interfaces with S3 object storage via boto3 . For this to work properly, you have to set your basic configuration in ~/.aws/config [default] source_profile = default Then, you have to set up your credentials in ~/.aws/credentials [default] aws_access_key_id = YOUR_KEY aws_secret_access_key = YOUR_SECRET For more information on the configuration please see the following documentation . Note that while these configurations are in the ~/.aws folder, they are shared by other similar services like the self-hosted minio service. Basic Usage \u00b6 MongoDB is not designed to handle large object storage. As such, we created an abstract object that combines the large object storage capabilities of Amazon S3 and the easy, python-friendly query language of MongoDB. These S3Store s all include an index store that only stores specific queryable data and the object key for retrieving the data from an S3 bucket using the key attribute (called 'fs_id' by default). An entry of in the index may look something like this: { fs_id : \"5fc6b87e99071dfdf04ca871\" task_id : \"mp-12345\" } Please note that since we are giving users the ability to reconstruct the index store using the object metadata, the object size in the index is limited by the metadata and not MongoDB. Different S3 services might have different rules, but the limit is typically smaller: 8 KB for aws The S3Store should be constructed as follows: from maggma.stores import MongograntStore , S3Store index = MongograntStore ( \"ro:mongodb03/js_cathodes\" , \"atomate_aeccar0_fs_index\" , key = \"fs_id\" ) s3store = S3Store ( index = index , bucket = \"<<BUCKET_NAME>>\" , s3_profile = \"<<S3_PROFILE_NAME>>\" , compress = True , endpoint_url = \"<<S3_URL>>\" , sub_dir = \"atomate_aeccar0_fs\" , s3_workers = 4 ) The subdir field creates subdirectories in the bucket to help the user organize their data. Parallelism \u00b6 Once you start working with large quantities of data, the speed at which you process this data will often be limited by database I/O. For the most time-consuming upload part of the process, we have implemented thread-level parallelism in the update member function. The update function received an entire chunk of processed data as defined by chunk_size , however since Store.update is typically called in the update_targets part of a builder, where builder execution is not longer multi-threaded. As such, we multithread the execution inside of update using s3_workers threads to perform the database write operation. As a general rule of thumb, if you notice that your update step is taking too long, you should change the s3_worker field which is optimized differently based on server-side resources.","title":"Configurations and Usage of Advanced `store`'s"},{"location":"getting_started/advanced_stores/#configurations-and-usage-of-advanced-stores","text":"","title":"Configurations and Usage of Advanced store's"},{"location":"getting_started/advanced_stores/#s3store","text":"","title":"S3Store"},{"location":"getting_started/advanced_stores/#configuration","text":"The S3Store interfaces with S3 object storage via boto3 . For this to work properly, you have to set your basic configuration in ~/.aws/config [default] source_profile = default Then, you have to set up your credentials in ~/.aws/credentials [default] aws_access_key_id = YOUR_KEY aws_secret_access_key = YOUR_SECRET For more information on the configuration please see the following documentation . Note that while these configurations are in the ~/.aws folder, they are shared by other similar services like the self-hosted minio service.","title":"Configuration"},{"location":"getting_started/advanced_stores/#basic-usage","text":"MongoDB is not designed to handle large object storage. As such, we created an abstract object that combines the large object storage capabilities of Amazon S3 and the easy, python-friendly query language of MongoDB. These S3Store s all include an index store that only stores specific queryable data and the object key for retrieving the data from an S3 bucket using the key attribute (called 'fs_id' by default). An entry of in the index may look something like this: { fs_id : \"5fc6b87e99071dfdf04ca871\" task_id : \"mp-12345\" } Please note that since we are giving users the ability to reconstruct the index store using the object metadata, the object size in the index is limited by the metadata and not MongoDB. Different S3 services might have different rules, but the limit is typically smaller: 8 KB for aws The S3Store should be constructed as follows: from maggma.stores import MongograntStore , S3Store index = MongograntStore ( \"ro:mongodb03/js_cathodes\" , \"atomate_aeccar0_fs_index\" , key = \"fs_id\" ) s3store = S3Store ( index = index , bucket = \"<<BUCKET_NAME>>\" , s3_profile = \"<<S3_PROFILE_NAME>>\" , compress = True , endpoint_url = \"<<S3_URL>>\" , sub_dir = \"atomate_aeccar0_fs\" , s3_workers = 4 ) The subdir field creates subdirectories in the bucket to help the user organize their data.","title":"Basic Usage"},{"location":"getting_started/advanced_stores/#parallelism","text":"Once you start working with large quantities of data, the speed at which you process this data will often be limited by database I/O. For the most time-consuming upload part of the process, we have implemented thread-level parallelism in the update member function. The update function received an entire chunk of processed data as defined by chunk_size , however since Store.update is typically called in the update_targets part of a builder, where builder execution is not longer multi-threaded. As such, we multithread the execution inside of update using s3_workers threads to perform the database write operation. As a general rule of thumb, if you notice that your update step is taking too long, you should change the s3_worker field which is optimized differently based on server-side resources.","title":"Parallelism"},{"location":"getting_started/group_builder/","text":"Group Builder \u00b6 Another advanced template in maggma is the GroupBuilder , which groups documents together before applying your function on the group of items. Just like MapBuilder , GroupBuilder also handles incremental building, keeping track of errors, getting only the data you need, and managing timeouts. GroupBuilder won't delete orphaned documents since that reverse relationshop isn't valid. Let's create a simple ResupplyBuilder , which will look at the inventory of items and determine what items need resupply. The source document will look something like this: { \"name\" : \"Banana\" , \"type\" : \"fruit\" , \"quantity\" : 20 , \"minimum\" : 10 , \"last_updated\" : \"2019-11-3T19:09:45\" } Our builder should give us documents that look like this: { \"names\" : [ \"Grapes\" , \"Apples\" , \"Bananas\" ], \"type\" : \"fruit\" , \"resupply\" : { \"Apples\" : 10 , \"Bananes\" : 0 , \"Grapes\" : 5 }, \"last_updated\" : \"2019-11-3T19:09:45\" } To begin, we define our GroupBuilder : from maggma.builders import GroupBuilder from maggma.core import Store class ResupplyBuilder ( GroupBuilder ): \"\"\" Simple builder that determines which items to resupply \"\"\" def __init__ ( inventory : Store , resupply : Store , resupply_percent : int = 100 , ** kwargs ): \"\"\" Arguments: inventory: current inventory information resupply: target resupply information resupply_percent: the percent of the minimum to include in the resupply \"\"\" self . inventory = inventory self . resupply = resupply self . resupply_percent = resupply_percent self . kwargs = kwargs super () . __init__ ( source = inventory , target = resupply , grouping_properties = [ \"type\" ], ** kwargs ) Note that unlike the previous MapBuilder example, we didn't call the source and target stores as such. Providing more usefull names is a good idea in writing builders to make it clearer what the underlying data should look like. GroupBuilder inherits from MapBuilder so it has the same configurational parameters. projection: list of the fields you want to project. This can reduce the data transfer load if you only need certain fields or sub-documents from the source documents timeout: optional timeout on the process function store_process_timeout: adds the process time into the target document for profiling retry_failed: retries running the process function on previously failed documents One parameter that doens't work in GroupBuilder is delete_orphans , since the Many-to-One relationshop makes determining orphaned documents very difficult. Finally let's get to the hard part which is running our function. We do this by defining unary_function def unary_function ( self , items : List [ Dict ]) -> Dict : resupply = {} for item in items : if item [ \"quantity\" ] > item [ \"minimum\" ]: resupply [ item [ \"name\" ]] = int ( item [ \"minimum\" ] * self . resupply_percent ) else : resupply [ item [ \"name\" ]] = 0 return { \"resupply\" : resupply } Just as in MapBuilder , we're not returning all the extra information typically kept in the originally item. Normally, we would have to write code that copies over the source key and convert it to the target key . Same goes for the last_updated_field . GroupBuilder takes care of this, while also recording errors, processing time, and the Builder version. GroupBuilder also keeps a plural version of the source.key field, so in this example, all the name values wil be put together and kept in names","title":"Working with GroupBuilder"},{"location":"getting_started/group_builder/#group-builder","text":"Another advanced template in maggma is the GroupBuilder , which groups documents together before applying your function on the group of items. Just like MapBuilder , GroupBuilder also handles incremental building, keeping track of errors, getting only the data you need, and managing timeouts. GroupBuilder won't delete orphaned documents since that reverse relationshop isn't valid. Let's create a simple ResupplyBuilder , which will look at the inventory of items and determine what items need resupply. The source document will look something like this: { \"name\" : \"Banana\" , \"type\" : \"fruit\" , \"quantity\" : 20 , \"minimum\" : 10 , \"last_updated\" : \"2019-11-3T19:09:45\" } Our builder should give us documents that look like this: { \"names\" : [ \"Grapes\" , \"Apples\" , \"Bananas\" ], \"type\" : \"fruit\" , \"resupply\" : { \"Apples\" : 10 , \"Bananes\" : 0 , \"Grapes\" : 5 }, \"last_updated\" : \"2019-11-3T19:09:45\" } To begin, we define our GroupBuilder : from maggma.builders import GroupBuilder from maggma.core import Store class ResupplyBuilder ( GroupBuilder ): \"\"\" Simple builder that determines which items to resupply \"\"\" def __init__ ( inventory : Store , resupply : Store , resupply_percent : int = 100 , ** kwargs ): \"\"\" Arguments: inventory: current inventory information resupply: target resupply information resupply_percent: the percent of the minimum to include in the resupply \"\"\" self . inventory = inventory self . resupply = resupply self . resupply_percent = resupply_percent self . kwargs = kwargs super () . __init__ ( source = inventory , target = resupply , grouping_properties = [ \"type\" ], ** kwargs ) Note that unlike the previous MapBuilder example, we didn't call the source and target stores as such. Providing more usefull names is a good idea in writing builders to make it clearer what the underlying data should look like. GroupBuilder inherits from MapBuilder so it has the same configurational parameters. projection: list of the fields you want to project. This can reduce the data transfer load if you only need certain fields or sub-documents from the source documents timeout: optional timeout on the process function store_process_timeout: adds the process time into the target document for profiling retry_failed: retries running the process function on previously failed documents One parameter that doens't work in GroupBuilder is delete_orphans , since the Many-to-One relationshop makes determining orphaned documents very difficult. Finally let's get to the hard part which is running our function. We do this by defining unary_function def unary_function ( self , items : List [ Dict ]) -> Dict : resupply = {} for item in items : if item [ \"quantity\" ] > item [ \"minimum\" ]: resupply [ item [ \"name\" ]] = int ( item [ \"minimum\" ] * self . resupply_percent ) else : resupply [ item [ \"name\" ]] = 0 return { \"resupply\" : resupply } Just as in MapBuilder , we're not returning all the extra information typically kept in the originally item. Normally, we would have to write code that copies over the source key and convert it to the target key . Same goes for the last_updated_field . GroupBuilder takes care of this, while also recording errors, processing time, and the Builder version. GroupBuilder also keeps a plural version of the source.key field, so in this example, all the name values wil be put together and kept in names","title":"Group Builder"},{"location":"getting_started/map_builder/","text":"Map Builder \u00b6 maggma has a built in builder called the MapBuilder which handles a number of tedious tasks in writing a builder. This class is designed to be used similar to a map operator in any other framework in even the map function in python. MapBuilder will take each document in the source store, apply the function you give it, and then store that in the target store. It handles incremental building, keeping track of errors, getting only the data you need, managing timeouts, and deleting orphaned documents through configurational options. Let's create the same MultiplierBuilder we wrote earlier using MapBuilder : from maggma.builders import MapBuilder from maggma.core import Store class MultiplyBuilder ( MapBuilder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" Just like before we define a new class, but this time it should inherit from MapBuilder . def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs kwargs = { k , v in kwargs . items () if k not in [ \"projection\" , \"delete_orphans\" , \"timeout\" , \"store_process_time\" , \"retry_failed\" ]} super () . __init__ ( source = source , target = target , projection = [ \"a\" ], delete_orphans = False , timeout = 10 , store_process_time = True , retry_failed = True , ** kwargs ) MapBuilder has a number of configurational options that you can hardcode as above or expose as properties for the user through **kwargs: projection: list of the fields you want to project. This can reduce the data transfer load if you only need certain fields or sub-documents from the source documents delete_orphans: this will delete documents in the target which don't have a corresponding document in the source timeout: optional timeout on the process function store_process_timeout: adds the process time into the target document for profiling retry_failed: retries running the process function on previously failed documents Finally let's get to the hard part which is running our function. We do this by defining unary_function def unary_function ( self , item ): return { \"a\" : item [ \"a\" ] * self . multiplier } Note that we're not returning all the extra information typically kept in the originally item. Normally, we would have to write code that copies over the source key and convert it to the target key . Same goes for the last_updated_field . MapBuilder takes care of this, while also recording errors, processing time, and the Builder version.","title":"Working with MapBuilder"},{"location":"getting_started/map_builder/#map-builder","text":"maggma has a built in builder called the MapBuilder which handles a number of tedious tasks in writing a builder. This class is designed to be used similar to a map operator in any other framework in even the map function in python. MapBuilder will take each document in the source store, apply the function you give it, and then store that in the target store. It handles incremental building, keeping track of errors, getting only the data you need, managing timeouts, and deleting orphaned documents through configurational options. Let's create the same MultiplierBuilder we wrote earlier using MapBuilder : from maggma.builders import MapBuilder from maggma.core import Store class MultiplyBuilder ( MapBuilder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" Just like before we define a new class, but this time it should inherit from MapBuilder . def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs kwargs = { k , v in kwargs . items () if k not in [ \"projection\" , \"delete_orphans\" , \"timeout\" , \"store_process_time\" , \"retry_failed\" ]} super () . __init__ ( source = source , target = target , projection = [ \"a\" ], delete_orphans = False , timeout = 10 , store_process_time = True , retry_failed = True , ** kwargs ) MapBuilder has a number of configurational options that you can hardcode as above or expose as properties for the user through **kwargs: projection: list of the fields you want to project. This can reduce the data transfer load if you only need certain fields or sub-documents from the source documents delete_orphans: this will delete documents in the target which don't have a corresponding document in the source timeout: optional timeout on the process function store_process_timeout: adds the process time into the target document for profiling retry_failed: retries running the process function on previously failed documents Finally let's get to the hard part which is running our function. We do this by defining unary_function def unary_function ( self , item ): return { \"a\" : item [ \"a\" ] * self . multiplier } Note that we're not returning all the extra information typically kept in the originally item. Normally, we would have to write code that copies over the source key and convert it to the target key . Same goes for the last_updated_field . MapBuilder takes care of this, while also recording errors, processing time, and the Builder version.","title":"Map Builder"},{"location":"getting_started/running_builders/","text":"Running Builders \u00b6 maggma is designed to run build-pipelines in a production environment. Builders can be run directly in a python environment, but this gives you none of the performance features such as multiprocessing. The base Builder class implements a simple run method that can be used to run that builder: class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" ... my_builder = MultiplyBuilder ( source_store , target_store , multiplier = 3 ) my_builder . run () A better way to run this builder would be to use the mrun command line tool. Since evrything in maggma is MSONable, we can use monty to dump the builders into a JSON file: from monty.serialization import dumpfn dumpfn ( my_builder , \"my_builder.json\" ) Then we can run the builder using mrun : mrun my_builder.json mrun has a number of usefull options: mrun --help Usage: mrun [ OPTIONS ] [ BUILDERS ] ... Options: -v, --verbose Controls logging level per number of v ' s -n, --num-workers INTEGER RANGE Number of worker processes. Defaults to single processing --help Show this message and exit. We can use the -n option to control how many workers run process_items in parallel. Similarly, -v controls the logging verbosity from just WARNINGs to INFO to DEBUG output. The result will be something that looks like this: 2020 -01-08 14 :33:17,187 - Builder - INFO - Starting Builder Builder 2020 -01-08 14 :33:17,217 - Builder - INFO - Processing 100 items Get: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 15366 .00it/s ] 2020 -01-08 14 :33:17,235 - MultiProcessor - INFO - Processing batch of 1000 items Update Targets: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 584 .51it/s ] Process Items: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 567 .39it/s ] There are progress bars for each of the three steps, which lets you understand what the slowest step is and the overall progress of the system. Running Distributed \u00b6 maggma can distribute work across multiple computers. There are two steps to this: Run a mrun manager by providing it with a --url to listen for workers on and --num-chunks ( -N ) which tells mrun how many sub-pieces to break up the work into. You can can run fewer workers then chunks. This will cause mrun to call the builder's prechunk to get the distribution of work and run distributd work on all workers Run mrun workers b y providing it with a --url to listen for a manager and --num-workers ( -n ) to tell it how many processes to run in this worker. The url argument takes a fully qualified url including protocol. tcp is recommended: Example: tcp://127.0.0.1:8080 Running Scripts \u00b6 mrun has the ability to run Builders defined in python scripts or in jupyter-notebooks. The only requirements are: The builder file has to be in a sub-directory from where mrun is called. The builders you want to run are in a variable called __builder__ or __builders__ mrun will run the whole python/jupyter file, grab the builders in these variables and adds these builders to the builder queue. Assuming you have a builder in a python file: my_builder.py class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" ... __builder__ = MultiplyBuilder ( source_store , target_store , multiplier = 3 ) You can use mrun to run this builder and parallelize for you: mrun -n 2 -v my_builder.py Running multiple builders \u00b6 mrun can run multiple builders. You can have multiple builders in a single file: json , python , or jupyter-notebook . Or you can chain multiple files in the order you want to run them: mrun -n 32 -vv my_first_builder.json builder_2_and_3.py last_builder.ipynb mrun will then execute the builders in these files in order. Reporting Build State \u00b6 mrun has the ability to report the status of the build pipeline to a user-provided Store . To do this, you first have to save the Store as a JSON or YAML file. Then you can use the -r option to give this to mrun . It will then periodicially add documents to the Store for one of 3 different events: BUILD_STARTED - This event tells us that a new builder started, the names of the sources and targets as well as the total number of items the builder expects to process UPDATE - This event tells us that a batch of items was processed and is going to update_targets . The number of items is stored in items . BUILD_ENDED - This event tells us the build process finished this specific builder. It also indicates the total number of errors and warnings that were caught during the process. These event docs also contain the builder , a build_id which is unique for each time a builder is run and anonymous but unique ID for the machine the builder was run on.","title":"Running a Builder Pipeline"},{"location":"getting_started/running_builders/#running-builders","text":"maggma is designed to run build-pipelines in a production environment. Builders can be run directly in a python environment, but this gives you none of the performance features such as multiprocessing. The base Builder class implements a simple run method that can be used to run that builder: class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" ... my_builder = MultiplyBuilder ( source_store , target_store , multiplier = 3 ) my_builder . run () A better way to run this builder would be to use the mrun command line tool. Since evrything in maggma is MSONable, we can use monty to dump the builders into a JSON file: from monty.serialization import dumpfn dumpfn ( my_builder , \"my_builder.json\" ) Then we can run the builder using mrun : mrun my_builder.json mrun has a number of usefull options: mrun --help Usage: mrun [ OPTIONS ] [ BUILDERS ] ... Options: -v, --verbose Controls logging level per number of v ' s -n, --num-workers INTEGER RANGE Number of worker processes. Defaults to single processing --help Show this message and exit. We can use the -n option to control how many workers run process_items in parallel. Similarly, -v controls the logging verbosity from just WARNINGs to INFO to DEBUG output. The result will be something that looks like this: 2020 -01-08 14 :33:17,187 - Builder - INFO - Starting Builder Builder 2020 -01-08 14 :33:17,217 - Builder - INFO - Processing 100 items Get: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 15366 .00it/s ] 2020 -01-08 14 :33:17,235 - MultiProcessor - INFO - Processing batch of 1000 items Update Targets: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 584 .51it/s ] Process Items: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 567 .39it/s ] There are progress bars for each of the three steps, which lets you understand what the slowest step is and the overall progress of the system.","title":"Running Builders"},{"location":"getting_started/running_builders/#running-distributed","text":"maggma can distribute work across multiple computers. There are two steps to this: Run a mrun manager by providing it with a --url to listen for workers on and --num-chunks ( -N ) which tells mrun how many sub-pieces to break up the work into. You can can run fewer workers then chunks. This will cause mrun to call the builder's prechunk to get the distribution of work and run distributd work on all workers Run mrun workers b y providing it with a --url to listen for a manager and --num-workers ( -n ) to tell it how many processes to run in this worker. The url argument takes a fully qualified url including protocol. tcp is recommended: Example: tcp://127.0.0.1:8080","title":"Running Distributed"},{"location":"getting_started/running_builders/#running-scripts","text":"mrun has the ability to run Builders defined in python scripts or in jupyter-notebooks. The only requirements are: The builder file has to be in a sub-directory from where mrun is called. The builders you want to run are in a variable called __builder__ or __builders__ mrun will run the whole python/jupyter file, grab the builders in these variables and adds these builders to the builder queue. Assuming you have a builder in a python file: my_builder.py class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" ... __builder__ = MultiplyBuilder ( source_store , target_store , multiplier = 3 ) You can use mrun to run this builder and parallelize for you: mrun -n 2 -v my_builder.py","title":"Running Scripts"},{"location":"getting_started/running_builders/#running-multiple-builders","text":"mrun can run multiple builders. You can have multiple builders in a single file: json , python , or jupyter-notebook . Or you can chain multiple files in the order you want to run them: mrun -n 32 -vv my_first_builder.json builder_2_and_3.py last_builder.ipynb mrun will then execute the builders in these files in order.","title":"Running multiple builders"},{"location":"getting_started/running_builders/#reporting-build-state","text":"mrun has the ability to report the status of the build pipeline to a user-provided Store . To do this, you first have to save the Store as a JSON or YAML file. Then you can use the -r option to give this to mrun . It will then periodicially add documents to the Store for one of 3 different events: BUILD_STARTED - This event tells us that a new builder started, the names of the sources and targets as well as the total number of items the builder expects to process UPDATE - This event tells us that a batch of items was processed and is going to update_targets . The number of items is stored in items . BUILD_ENDED - This event tells us the build process finished this specific builder. It also indicates the total number of errors and warnings that were caught during the process. These event docs also contain the builder , a build_id which is unique for each time a builder is run and anonymous but unique ID for the machine the builder was run on.","title":"Reporting Build State"},{"location":"getting_started/simple_builder/","text":"Writing a Builder \u00b6 Builder Architecture \u00b6 A Builder is a class that inherits from maggma.core.Builder and implement 3 methods: get_items : This method should return some iterable of items to run through process_items process_item : This method should take a single item, process it, and return the processed item update_targets : This method should take a list of processed items and update the target stores. To make this less abstract, we will write a builder that multiplies the \"a\" sub-document by a pre-configured multiplier . Let's assume we have some source collection in MongoDB with documents that look like this: { \"id\" : 1 , \"a\" : 3 , \"last_updated\" : \"2019-11-3\" } Class definition and __init__ \u00b6 A simple class definition for a Maggma-based builder looks like this: from maggma.core import Builder from maggma.core import Store class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" The __init__ for a builder can have any set of parameters. Generally, you want a source Store and a target Store along with any parameters that configure the builder. Due to the MSONable pattern, any parameters to __init__ have to be stored as attributes. A simple __init__ would look like this: def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) Python type annotations provide a really nice way of documenting the types we expect and being able to later type check using mypy . We defined the type for source and target as Store since we only care that implements that pattern. How exactly these Store s operate doesn't concern us here. Note that the __init__ arguments: source , target , multiplier , and kwargs get saved as attributess: self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs Finally, we want to call the base Builder 's __init__ to tell it our sources and targets for this builder. In addition, we pass along any extra parameters that might configured the base builder class. super () . __init__ ( sources = source , targets = target , ** kwargs ) Calling the parent class __init__ is a good practice as sub-classing builders is a good way to encapsulate complex logic. get_items \u00b6 get_items is conceptually a simple method to implement, but in practice can easily be more code than the rest of the builder. All of the logic for getting data from the sources has to happen here, which requires some planning. get_items should also sort all of the data into induvidual items to process. This simple builder has a very easy get_items : def get_items ( self ) -> Iterator : \"\"\" Gets induvidual documents to multiply \"\"\" return self . source . query () Here, get items just returns the results of query() from the store. It could also have been written as a generator: def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" for doc in self . source . query (): yield doc We could have also returned a list of items: def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" docs = list ( self . source . query ()) One advantage of using the generator approach is it is less memory intensive than the approach where a list of items returned. For large datasets, returning a list of all items for processing may be prohibitive due to memory constraints. process_item \u00b6 process_item just has to do the parallelizable work on each item. Since the item is whatever comes out of get_items , you know exactly what it should be. It may be a single document, a list of documents, a mapping, a set, etc. Our simple process item just has to multiply one field by self.mulitplier : def process_items ( self , item : Dict ) -> Dict : \"\"\" Multiplies the \"a\" sub-document by self.multiplier \"\"\" new_item = dict ( ** item ) new_item [ \"a\" ] *= self . multiplier return new_item update_targets \u00b6 Finally, we have to put the processed item in to the target store: def update_targets ( self , items : List [ Dict ]): \"\"\" Adds the processed items into the target store \"\"\" self . target . update ( items ) Note Note that whatever process_items returns, update_targets takes a List of these: For instance, if process_items returns str , then update_targets would look like: def update_target ( self , items : List [ str ]): Putting it all together we get: from typing import Dict , Iterable , List from maggma.core import Builder from maggma.core import Store class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" docs = list ( self . source . query ()) def process_items ( self , item : Dict ) -> Dict : \"\"\" Multiplies the \"a\" sub-document by self.multiplier \"\"\" new_item = dict ( ** item ) new_item [ \"a\" ] *= self . multiplier return new_item def update_targets ( self , items : List [ Dict ]): \"\"\" Adds the processed items into the target store \"\"\" self . target . update ( items ) Distributed Processing \u00b6 maggma can distribute a builder across multiple computers. The Builder must have a prechunk method defined. prechunk should do a subset of get_items to figure out what needs to be processed and then return dictionaries that modify the Builder in-place to only work on each subset. For example, if in the above example we'd first have to update the builder to be able to work on a subset of keys. One pattern is to define a generic query argument for the builder and use that in get items: def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , query : Optional [ Dict ] = None , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . query = query self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" query = self . query or {} docs = list ( self . source . query ( criteria = query )) Then we can define a prechunk method that modifies the Builder dict in place to operate on just a subset of the keys: from maggma.utils import grouper def prechunk ( self , number_splits : int ) -> Iterable [ Dict ]: keys = self . source . distinct ( self . source . key ) for split in grouper ( keys , N ): yield { \"query\" : { self . source . key : { \"$in\" : list ( split )}} } When distributed processing runs, it will modify the Builder dictionary in place by the prechunk dictionary. In this case, each builder distribute to a worker will get a modified query parameter that only runs on a subset of all posible keys.","title":"Writing a Builder"},{"location":"getting_started/simple_builder/#writing-a-builder","text":"","title":"Writing a Builder"},{"location":"getting_started/simple_builder/#builder-architecture","text":"A Builder is a class that inherits from maggma.core.Builder and implement 3 methods: get_items : This method should return some iterable of items to run through process_items process_item : This method should take a single item, process it, and return the processed item update_targets : This method should take a list of processed items and update the target stores. To make this less abstract, we will write a builder that multiplies the \"a\" sub-document by a pre-configured multiplier . Let's assume we have some source collection in MongoDB with documents that look like this: { \"id\" : 1 , \"a\" : 3 , \"last_updated\" : \"2019-11-3\" }","title":"Builder Architecture"},{"location":"getting_started/simple_builder/#class-definition-and-__init__","text":"A simple class definition for a Maggma-based builder looks like this: from maggma.core import Builder from maggma.core import Store class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" The __init__ for a builder can have any set of parameters. Generally, you want a source Store and a target Store along with any parameters that configure the builder. Due to the MSONable pattern, any parameters to __init__ have to be stored as attributes. A simple __init__ would look like this: def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) Python type annotations provide a really nice way of documenting the types we expect and being able to later type check using mypy . We defined the type for source and target as Store since we only care that implements that pattern. How exactly these Store s operate doesn't concern us here. Note that the __init__ arguments: source , target , multiplier , and kwargs get saved as attributess: self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs Finally, we want to call the base Builder 's __init__ to tell it our sources and targets for this builder. In addition, we pass along any extra parameters that might configured the base builder class. super () . __init__ ( sources = source , targets = target , ** kwargs ) Calling the parent class __init__ is a good practice as sub-classing builders is a good way to encapsulate complex logic.","title":"Class definition and __init__"},{"location":"getting_started/simple_builder/#get_items","text":"get_items is conceptually a simple method to implement, but in practice can easily be more code than the rest of the builder. All of the logic for getting data from the sources has to happen here, which requires some planning. get_items should also sort all of the data into induvidual items to process. This simple builder has a very easy get_items : def get_items ( self ) -> Iterator : \"\"\" Gets induvidual documents to multiply \"\"\" return self . source . query () Here, get items just returns the results of query() from the store. It could also have been written as a generator: def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" for doc in self . source . query (): yield doc We could have also returned a list of items: def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" docs = list ( self . source . query ()) One advantage of using the generator approach is it is less memory intensive than the approach where a list of items returned. For large datasets, returning a list of all items for processing may be prohibitive due to memory constraints.","title":"get_items"},{"location":"getting_started/simple_builder/#process_item","text":"process_item just has to do the parallelizable work on each item. Since the item is whatever comes out of get_items , you know exactly what it should be. It may be a single document, a list of documents, a mapping, a set, etc. Our simple process item just has to multiply one field by self.mulitplier : def process_items ( self , item : Dict ) -> Dict : \"\"\" Multiplies the \"a\" sub-document by self.multiplier \"\"\" new_item = dict ( ** item ) new_item [ \"a\" ] *= self . multiplier return new_item","title":"process_item"},{"location":"getting_started/simple_builder/#update_targets","text":"Finally, we have to put the processed item in to the target store: def update_targets ( self , items : List [ Dict ]): \"\"\" Adds the processed items into the target store \"\"\" self . target . update ( items ) Note Note that whatever process_items returns, update_targets takes a List of these: For instance, if process_items returns str , then update_targets would look like: def update_target ( self , items : List [ str ]): Putting it all together we get: from typing import Dict , Iterable , List from maggma.core import Builder from maggma.core import Store class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" docs = list ( self . source . query ()) def process_items ( self , item : Dict ) -> Dict : \"\"\" Multiplies the \"a\" sub-document by self.multiplier \"\"\" new_item = dict ( ** item ) new_item [ \"a\" ] *= self . multiplier return new_item def update_targets ( self , items : List [ Dict ]): \"\"\" Adds the processed items into the target store \"\"\" self . target . update ( items )","title":"update_targets"},{"location":"getting_started/simple_builder/#distributed-processing","text":"maggma can distribute a builder across multiple computers. The Builder must have a prechunk method defined. prechunk should do a subset of get_items to figure out what needs to be processed and then return dictionaries that modify the Builder in-place to only work on each subset. For example, if in the above example we'd first have to update the builder to be able to work on a subset of keys. One pattern is to define a generic query argument for the builder and use that in get items: def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , query : Optional [ Dict ] = None , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . query = query self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" query = self . query or {} docs = list ( self . source . query ( criteria = query )) Then we can define a prechunk method that modifies the Builder dict in place to operate on just a subset of the keys: from maggma.utils import grouper def prechunk ( self , number_splits : int ) -> Iterable [ Dict ]: keys = self . source . distinct ( self . source . key ) for split in grouper ( keys , N ): yield { \"query\" : { self . source . key : { \"$in\" : list ( split )}} } When distributed processing runs, it will modify the Builder dictionary in place by the prechunk dictionary. In this case, each builder distribute to a worker will get a modified query parameter that only runs on a subset of all posible keys.","title":"Distributed Processing"},{"location":"getting_started/simple_drone/","text":"Simple Drone \u00b6 Let's implement a Simple Drone example. The simple drone will sync database with a local file structure like below. You may find sample files here - data - citation-1.bibtex - citation-2.bibtex - citation-3.bibtex - citation-4.bibtex - citation-5.bibtex - citation-6.bibtex - text-1.txt - text-2.txt - text-3.txt - text-4.txt - text-5.txt Notice that the pattern here for computing the key is the number between - and its file extension. So we can go ahead and define the compute_record_identifier_key function that does exactly that def compute_record_identifier_key ( self , doc : Document ) -> str : prefix , postfix = doc . name . split ( sep = \"-\" , maxsplit = 1 ) ID , ftype = postfix . split ( sep = \".\" , maxsplit = 1 ) return ID Notice that these files are all in one single directory, we can simply read all these files and generate a list of Document . A Document represent a FILE, a file that contains data, not a directory. def generate_documents ( self , folder_path : Path ) -> List [ Document ] : files_paths = [ folder_path / f for f in os.listdir(folder_path.as_posix()) ] return [ Document(path=fp, name=fp.name) for fp in files_paths ] Now we need to organize these documents, or aka to build an association. So let's define a helper function called organize_documents def organize_documents ( self , documents : List [ Document ] ) -> Dict [ str, List[Document ] ]: log : Dict = dict () for doc in documents : key = self . compute_record_identifier_key ( doc ) log [ key ] = log . get ( key , [] ) + [ doc ] return log We also want to have a way to compute RecordIdentifier when given a list of documents, so we overwrite the compute_record_identifier Please note that RecordIdentifier comes with a state_hash field. This field is used to compare against the state_hash in the database so that we can efficiently know which file has changed without compare byte by byte. RecordIdentifier comes with a default method of computing state_hash using md5sum. You may modify it or simply use it by calling recordIdentifier.compute_state_hash() def compute_record_identifier ( self , record_key : str , doc_list : List [ Document ] ) -> RecordIdentifier : \"\"\" Compute meta data for this list of documents, and generate a RecordIdentifier object :param record_key: record keys that indicate a record :param doc_list: document on disk that this record include :return: RecordIdentifier that represent this doc_list \"\"\" recordIdentifier = RecordIdentifier ( last_updated = datetime . now (), documents = doc_list , record_key = record_key ) recordIdentifier . state_hash = recordIdentifier . compute_state_hash () return recordIdentifier At this point, we have all the necessary components to overwrite the read function from the base Drone class. We basically generate a list of documents, organize them, and then generate a list of RecordIdentifier def read ( self , path : Path ) -> List [ RecordIdentifier ] : documents : List [ Document ] = self . generate_documents ( folder_path = path ) log = self . organize_documents ( documents = documents ) record_identifiers = [ self.compute_record_identifier(record_key, doc_list) for record_key, doc_list in log.items() ] return record_identifiers Lastly, if there's a file that needs to be updated, we want to extract the data and append some meta data. In our case, this is very simple. def compute_data ( self , recordI D : RecordI dentifier ) -> Dict : record = dict () for document in recordI D . documents : if \"citations\" in document . name : with open ( document . path . as_posix (), \"r\" ) as file : s = file . read () record[ \"citations\" ] = s if \"text\" in document . name : with open ( document . path . as_posix (), \"r\" ) as file : s = file . read () record[ \"text\" ] = s return record Now, you have a working SimpleBibDrone! You can use it like this: mongo_store = MongoStore( database=\"drone_test\", collection_name=\"drone_test\", key=\"record_key\" ) simple_path = Path.cwd() / \"data\" simple_bib_drone = SimpleBibDrone(store=mongo_store, path=simple_path) simple_bib_drone.run() For complete code, please visit here","title":"Writing a Drone"},{"location":"getting_started/simple_drone/#simple-drone","text":"Let's implement a Simple Drone example. The simple drone will sync database with a local file structure like below. You may find sample files here - data - citation-1.bibtex - citation-2.bibtex - citation-3.bibtex - citation-4.bibtex - citation-5.bibtex - citation-6.bibtex - text-1.txt - text-2.txt - text-3.txt - text-4.txt - text-5.txt Notice that the pattern here for computing the key is the number between - and its file extension. So we can go ahead and define the compute_record_identifier_key function that does exactly that def compute_record_identifier_key ( self , doc : Document ) -> str : prefix , postfix = doc . name . split ( sep = \"-\" , maxsplit = 1 ) ID , ftype = postfix . split ( sep = \".\" , maxsplit = 1 ) return ID Notice that these files are all in one single directory, we can simply read all these files and generate a list of Document . A Document represent a FILE, a file that contains data, not a directory. def generate_documents ( self , folder_path : Path ) -> List [ Document ] : files_paths = [ folder_path / f for f in os.listdir(folder_path.as_posix()) ] return [ Document(path=fp, name=fp.name) for fp in files_paths ] Now we need to organize these documents, or aka to build an association. So let's define a helper function called organize_documents def organize_documents ( self , documents : List [ Document ] ) -> Dict [ str, List[Document ] ]: log : Dict = dict () for doc in documents : key = self . compute_record_identifier_key ( doc ) log [ key ] = log . get ( key , [] ) + [ doc ] return log We also want to have a way to compute RecordIdentifier when given a list of documents, so we overwrite the compute_record_identifier Please note that RecordIdentifier comes with a state_hash field. This field is used to compare against the state_hash in the database so that we can efficiently know which file has changed without compare byte by byte. RecordIdentifier comes with a default method of computing state_hash using md5sum. You may modify it or simply use it by calling recordIdentifier.compute_state_hash() def compute_record_identifier ( self , record_key : str , doc_list : List [ Document ] ) -> RecordIdentifier : \"\"\" Compute meta data for this list of documents, and generate a RecordIdentifier object :param record_key: record keys that indicate a record :param doc_list: document on disk that this record include :return: RecordIdentifier that represent this doc_list \"\"\" recordIdentifier = RecordIdentifier ( last_updated = datetime . now (), documents = doc_list , record_key = record_key ) recordIdentifier . state_hash = recordIdentifier . compute_state_hash () return recordIdentifier At this point, we have all the necessary components to overwrite the read function from the base Drone class. We basically generate a list of documents, organize them, and then generate a list of RecordIdentifier def read ( self , path : Path ) -> List [ RecordIdentifier ] : documents : List [ Document ] = self . generate_documents ( folder_path = path ) log = self . organize_documents ( documents = documents ) record_identifiers = [ self.compute_record_identifier(record_key, doc_list) for record_key, doc_list in log.items() ] return record_identifiers Lastly, if there's a file that needs to be updated, we want to extract the data and append some meta data. In our case, this is very simple. def compute_data ( self , recordI D : RecordI dentifier ) -> Dict : record = dict () for document in recordI D . documents : if \"citations\" in document . name : with open ( document . path . as_posix (), \"r\" ) as file : s = file . read () record[ \"citations\" ] = s if \"text\" in document . name : with open ( document . path . as_posix (), \"r\" ) as file : s = file . read () record[ \"text\" ] = s return record Now, you have a working SimpleBibDrone! You can use it like this: mongo_store = MongoStore( database=\"drone_test\", collection_name=\"drone_test\", key=\"record_key\" ) simple_path = Path.cwd() / \"data\" simple_bib_drone = SimpleBibDrone(store=mongo_store, path=simple_path) simple_bib_drone.run() For complete code, please visit here","title":"Simple Drone"},{"location":"getting_started/stores/","text":"Using Store \u00b6 A Store is just a wrapper to access data from somewhere. That somewhere is typically a MongoDB collection, but it could also be GridFS which lets you keep large binary objects. maggma makes GridFS and MongoDB collections feel the same. Beyond that it adds in something that looks like GridFS but is actually using AWS S3 as the storage space. Finally, Store can actually perform logic, concatenating two or more Stores together to make them look like one data source for instance. This means you only have to write a Builder for one scenario of how to transform data and the choice of Store lets you control where the data comes from and goes. List of Stores \u00b6 Current working and tested Stores include: MongoStore: interfaces to a MongoDB Collection MongoURIStore: MongoDB Introduced advanced URIs including their special \"mongodb+srv://\" which uses a combination of SRV and TXT DNS records to fully setup the client. This store is to safely handle these kinds of URIs. MemoryStore: just a Store that exists temporarily in memory JSONStore: builds a MemoryStore and then populates it with the contents of the given JSON files GridFSStore: interfaces to GridFS collection in MongoDB MongograntStore: uses Mongogrant to get credentials for MongoDB database VaulStore: uses Vault to get credentials for a MongoDB database AliasingStore: aliases keys from the underlying store to new names SandboxStore: provides permission control to documents via a _sbxn sandbox key S3Store: provides an interface to an S3 Bucket either on AWS or self-hosted solutions ( additional documentation ) JointStore: joins several MongoDB collections together, merging documents with the same key , so they look like one collection ConcatStore: concatenates several MongoDB collections in series so they look like one collection The Store interface \u00b6 Initializing a Store \u00b6 All Store s have a few basic arguments that are critical for basic usage. Every Store has two attributes that the user should customize based on the data contained in that store: key and last_updated_field . The key defines how the Store tells documents apart. Typically this is _id in MongoDB, but you could use your own field (be sure all values under the key field can be used to uniquely identify documents). last_updated_field tells Store how to order the documents by a date, which is typically in the datetime format, but can also be an ISO 8601-format (ex: 2009-05-28T16:15:00 ) Store s can also take a Validator object to make sure the data going into obeys some schema. Using a Store \u00b6 You must connect to a store by running store.connect() before querying and updating the store. If you are operating on the stores inside of another code it is recommended to use the built-in context manager: with MongoStore ( ... ) as store : store . query () Stores provide a number of basic methods that make easy to use: query: Standard mongo style find method that lets you search the store. query_one: Same as above but limits returned results to just the first document that matches your query. update: Update the documents into the collection. This will override documents if the key field matches. ensure_index: This creates an index for the underlying data-source for fast querying. distinct: Gets distinct values of a field. groupby: Similar to query but performs a grouping operation and returns sets of documents. remove_docs: Removes documents from the underlying data source. last_updated: Finds the most recently updated last_updated_field value and returns that. Useful for knowing how old a data-source is. newer_in: Finds all documents that are newer in the target collection and returns their key s. This is a very useful way of performing incremental processing.","title":"Using Stores"},{"location":"getting_started/stores/#using-store","text":"A Store is just a wrapper to access data from somewhere. That somewhere is typically a MongoDB collection, but it could also be GridFS which lets you keep large binary objects. maggma makes GridFS and MongoDB collections feel the same. Beyond that it adds in something that looks like GridFS but is actually using AWS S3 as the storage space. Finally, Store can actually perform logic, concatenating two or more Stores together to make them look like one data source for instance. This means you only have to write a Builder for one scenario of how to transform data and the choice of Store lets you control where the data comes from and goes.","title":"Using Store"},{"location":"getting_started/stores/#list-of-stores","text":"Current working and tested Stores include: MongoStore: interfaces to a MongoDB Collection MongoURIStore: MongoDB Introduced advanced URIs including their special \"mongodb+srv://\" which uses a combination of SRV and TXT DNS records to fully setup the client. This store is to safely handle these kinds of URIs. MemoryStore: just a Store that exists temporarily in memory JSONStore: builds a MemoryStore and then populates it with the contents of the given JSON files GridFSStore: interfaces to GridFS collection in MongoDB MongograntStore: uses Mongogrant to get credentials for MongoDB database VaulStore: uses Vault to get credentials for a MongoDB database AliasingStore: aliases keys from the underlying store to new names SandboxStore: provides permission control to documents via a _sbxn sandbox key S3Store: provides an interface to an S3 Bucket either on AWS or self-hosted solutions ( additional documentation ) JointStore: joins several MongoDB collections together, merging documents with the same key , so they look like one collection ConcatStore: concatenates several MongoDB collections in series so they look like one collection","title":"List of Stores"},{"location":"getting_started/stores/#the-store-interface","text":"","title":"The Store interface"},{"location":"getting_started/stores/#initializing-a-store","text":"All Store s have a few basic arguments that are critical for basic usage. Every Store has two attributes that the user should customize based on the data contained in that store: key and last_updated_field . The key defines how the Store tells documents apart. Typically this is _id in MongoDB, but you could use your own field (be sure all values under the key field can be used to uniquely identify documents). last_updated_field tells Store how to order the documents by a date, which is typically in the datetime format, but can also be an ISO 8601-format (ex: 2009-05-28T16:15:00 ) Store s can also take a Validator object to make sure the data going into obeys some schema.","title":"Initializing a Store"},{"location":"getting_started/stores/#using-a-store","text":"You must connect to a store by running store.connect() before querying and updating the store. If you are operating on the stores inside of another code it is recommended to use the built-in context manager: with MongoStore ( ... ) as store : store . query () Stores provide a number of basic methods that make easy to use: query: Standard mongo style find method that lets you search the store. query_one: Same as above but limits returned results to just the first document that matches your query. update: Update the documents into the collection. This will override documents if the key field matches. ensure_index: This creates an index for the underlying data-source for fast querying. distinct: Gets distinct values of a field. groupby: Similar to query but performs a grouping operation and returns sets of documents. remove_docs: Removes documents from the underlying data source. last_updated: Finds the most recently updated last_updated_field value and returns that. Useful for knowing how old a data-source is. newer_in: Finds all documents that are newer in the target collection and returns their key s. This is a very useful way of performing incremental processing.","title":"Using a Store"},{"location":"reference/builders/","text":"One-to-One Map Builder and a simple CopyBuilder implementation CopyBuilder ( MapBuilder ) \u00b6 Sync a source store with a target store. Source code in maggma/builders/map_builder.py class CopyBuilder ( MapBuilder ): \"\"\"Sync a source store with a target store.\"\"\" def unary_function ( self , item ): \"\"\" Identity function for copy builder map operation \"\"\" if \"_id\" in item : del item [ \"_id\" ] return item unary_function ( self , item ) \u00b6 Identity function for copy builder map operation Source code in maggma/builders/map_builder.py def unary_function ( self , item ): \"\"\" Identity function for copy builder map operation \"\"\" if \"_id\" in item : del item [ \"_id\" ] return item MapBuilder ( Builder ) \u00b6 Apply a unary function to yield a target document for each source document. Supports incremental building, where a source document gets built only if it has newer (by last_updated_field) data than the corresponding (by key) target document. Source code in maggma/builders/map_builder.py class MapBuilder ( Builder , metaclass = ABCMeta ): \"\"\" Apply a unary function to yield a target document for each source document. Supports incremental building, where a source document gets built only if it has newer (by last_updated_field) data than the corresponding (by key) target document. \"\"\" def __init__ ( self , source : Store , target : Store , query : Optional [ Dict ] = None , projection : Optional [ List ] = None , delete_orphans : bool = False , timeout : int = 0 , store_process_time : bool = True , retry_failed : bool = False , ** kwargs , ): \"\"\" Apply a unary function to each source document. Args: source: source store target: target store query: optional query to filter source store projection: list of keys to project from the source for processing. Limits data transfer to improve efficiency. delete_orphans: Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. timeout: maximum running time per item in seconds store_process_time: If True, add \"_process_time\" key to document for profiling purposes retry_failed: If True, will retry building documents that previously failed \"\"\" self . source = source self . target = target self . query = query self . projection = projection self . delete_orphans = delete_orphans self . kwargs = kwargs self . timeout = timeout self . store_process_time = store_process_time self . retry_failed = retry_failed super () . __init__ ( sources = [ source ], targets = [ target ], ** kwargs ) def ensure_indexes ( self ): \"\"\" Ensures indicies on critical fields for MapBuilder \"\"\" index_checks = [ self . source . ensure_index ( self . source . key ), self . source . ensure_index ( self . source . last_updated_field ), self . target . ensure_index ( self . target . key ), self . target . ensure_index ( self . target . last_updated_field ), self . target . ensure_index ( \"state\" ), ] if not all ( index_checks ): self . logger . warning ( \"Missing one or more important indices on stores. \" \"Performance for large stores may be severely degraded. \" \"Ensure indices on target.key and \" \"[(store.last_updated_field, -1), (store.key, 1)] \" \"for each of source and target.\" ) def prechunk ( self , number_splits : int ) -> Iterator [ Dict ]: \"\"\" Generic prechunk for map builder to perform domain-decompostion by the key field \"\"\" self . ensure_indexes () keys = self . target . newer_in ( self . source , criteria = self . query , exhaustive = True ) N = ceil ( len ( keys ) / number_splits ) for split in grouper ( keys , N ): yield { \"query\" : { self . source . key : { \"$in\" : list ( split )}}} def get_items ( self ): \"\"\" Generic get items for Map Builder designed to perform incremental building \"\"\" self . logger . info ( \"Starting {} Builder\" . format ( self . __class__ . __name__ )) self . ensure_indexes () keys = self . target . newer_in ( self . source , criteria = self . query , exhaustive = True ) if self . retry_failed : if isinstance ( self . query , ( dict )): failed_query = { \"$and\" : [ self . query , { \"state\" : \"failed\" }]} else : failed_query = { \"state\" : \"failed\" } failed_keys = self . target . distinct ( self . target . key , criteria = failed_query ) keys = list ( set ( keys + failed_keys )) self . logger . info ( \"Processing {} items\" . format ( len ( keys ))) if self . projection : projection = list ( set ( self . projection + [ self . source . key , self . source . last_updated_field ]) ) else : projection = None self . total = len ( keys ) for chunked_keys in grouper ( keys , self . chunk_size ): chunked_keys = list ( chunked_keys ) for doc in list ( self . source . query ( criteria = { self . source . key : { \"$in\" : chunked_keys }}, properties = projection , ) ): yield doc def process_item ( self , item : Dict ): \"\"\" Generic process items to process a dictionary using a map function \"\"\" self . logger . debug ( \"Processing: {} \" . format ( item [ self . source . key ])) time_start = time () try : with Timeout ( seconds = self . timeout ): processed = dict ( self . unary_function ( item )) processed . update ({ \"state\" : \"successful\" }) for k in [ self . source . key , self . source . last_updated_field ]: if k in processed : del processed [ k ] except Exception as e : self . logger . error ( traceback . format_exc ()) processed = { \"error\" : str ( e ), \"state\" : \"failed\" } time_end = time () key , last_updated_field = self . source . key , self . source . last_updated_field out = { self . target . key : item [ key ], self . target . last_updated_field : self . source . _lu_func [ 0 ]( item . get ( last_updated_field , datetime . utcnow ()) ), } if self . store_process_time : out [ \"_process_time\" ] = time_end - time_start out . update ( processed ) return out def update_targets ( self , items : List [ Dict ]): \"\"\" Generic update targets for Map Builder \"\"\" target = self . target for item in items : item [ \"_bt\" ] = datetime . utcnow () if \"_id\" in item : del item [ \"_id\" ] if len ( items ) > 0 : target . update ( items ) def finalize ( self ): \"\"\" Finalize MapBuilder operations including removing orphaned documents \"\"\" if self . delete_orphans : source_keyvals = set ( self . source . distinct ( self . source . key )) target_keyvals = set ( self . target . distinct ( self . target . key )) to_delete = list ( target_keyvals - source_keyvals ) if len ( to_delete ): self . logger . info ( \"Finalize: Deleting {} orphans.\" . format ( len ( to_delete )) ) self . target . remove_docs ({ self . target . key : { \"$in\" : to_delete }}) super () . finalize () @abstractmethod def unary_function ( self , item ): \"\"\" ufn: Unary function to process item You do not need to provide values for source.key and source.last_updated_field in the output. Any uncaught exceptions will be caught by process_item and logged to the \"error\" field in the target document. \"\"\" pass __init__ ( self , source , target , query = None , projection = None , delete_orphans = False , timeout = 0 , store_process_time = True , retry_failed = False , ** kwargs ) special \u00b6 Apply a unary function to each source document. Parameters: Name Type Description Default source Store source store required target Store target store required query Optional[Dict] optional query to filter source store None projection Optional[List] list of keys to project from the source for processing. Limits data transfer to improve efficiency. None delete_orphans bool Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. False timeout int maximum running time per item in seconds 0 store_process_time bool If True, add \"_process_time\" key to document for profiling purposes True retry_failed bool If True, will retry building documents that previously failed False Source code in maggma/builders/map_builder.py def __init__ ( self , source : Store , target : Store , query : Optional [ Dict ] = None , projection : Optional [ List ] = None , delete_orphans : bool = False , timeout : int = 0 , store_process_time : bool = True , retry_failed : bool = False , ** kwargs , ): \"\"\" Apply a unary function to each source document. Args: source: source store target: target store query: optional query to filter source store projection: list of keys to project from the source for processing. Limits data transfer to improve efficiency. delete_orphans: Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. timeout: maximum running time per item in seconds store_process_time: If True, add \"_process_time\" key to document for profiling purposes retry_failed: If True, will retry building documents that previously failed \"\"\" self . source = source self . target = target self . query = query self . projection = projection self . delete_orphans = delete_orphans self . kwargs = kwargs self . timeout = timeout self . store_process_time = store_process_time self . retry_failed = retry_failed super () . __init__ ( sources = [ source ], targets = [ target ], ** kwargs ) ensure_indexes ( self ) \u00b6 Ensures indicies on critical fields for MapBuilder Source code in maggma/builders/map_builder.py def ensure_indexes ( self ): \"\"\" Ensures indicies on critical fields for MapBuilder \"\"\" index_checks = [ self . source . ensure_index ( self . source . key ), self . source . ensure_index ( self . source . last_updated_field ), self . target . ensure_index ( self . target . key ), self . target . ensure_index ( self . target . last_updated_field ), self . target . ensure_index ( \"state\" ), ] if not all ( index_checks ): self . logger . warning ( \"Missing one or more important indices on stores. \" \"Performance for large stores may be severely degraded. \" \"Ensure indices on target.key and \" \"[(store.last_updated_field, -1), (store.key, 1)] \" \"for each of source and target.\" ) finalize ( self ) \u00b6 Finalize MapBuilder operations including removing orphaned documents Source code in maggma/builders/map_builder.py def finalize ( self ): \"\"\" Finalize MapBuilder operations including removing orphaned documents \"\"\" if self . delete_orphans : source_keyvals = set ( self . source . distinct ( self . source . key )) target_keyvals = set ( self . target . distinct ( self . target . key )) to_delete = list ( target_keyvals - source_keyvals ) if len ( to_delete ): self . logger . info ( \"Finalize: Deleting {} orphans.\" . format ( len ( to_delete )) ) self . target . remove_docs ({ self . target . key : { \"$in\" : to_delete }}) super () . finalize () get_items ( self ) \u00b6 Generic get items for Map Builder designed to perform incremental building Source code in maggma/builders/map_builder.py def get_items ( self ): \"\"\" Generic get items for Map Builder designed to perform incremental building \"\"\" self . logger . info ( \"Starting {} Builder\" . format ( self . __class__ . __name__ )) self . ensure_indexes () keys = self . target . newer_in ( self . source , criteria = self . query , exhaustive = True ) if self . retry_failed : if isinstance ( self . query , ( dict )): failed_query = { \"$and\" : [ self . query , { \"state\" : \"failed\" }]} else : failed_query = { \"state\" : \"failed\" } failed_keys = self . target . distinct ( self . target . key , criteria = failed_query ) keys = list ( set ( keys + failed_keys )) self . logger . info ( \"Processing {} items\" . format ( len ( keys ))) if self . projection : projection = list ( set ( self . projection + [ self . source . key , self . source . last_updated_field ]) ) else : projection = None self . total = len ( keys ) for chunked_keys in grouper ( keys , self . chunk_size ): chunked_keys = list ( chunked_keys ) for doc in list ( self . source . query ( criteria = { self . source . key : { \"$in\" : chunked_keys }}, properties = projection , ) ): yield doc prechunk ( self , number_splits ) \u00b6 Generic prechunk for map builder to perform domain-decompostion by the key field Source code in maggma/builders/map_builder.py def prechunk ( self , number_splits : int ) -> Iterator [ Dict ]: \"\"\" Generic prechunk for map builder to perform domain-decompostion by the key field \"\"\" self . ensure_indexes () keys = self . target . newer_in ( self . source , criteria = self . query , exhaustive = True ) N = ceil ( len ( keys ) / number_splits ) for split in grouper ( keys , N ): yield { \"query\" : { self . source . key : { \"$in\" : list ( split )}}} process_item ( self , item ) \u00b6 Generic process items to process a dictionary using a map function Source code in maggma/builders/map_builder.py def process_item ( self , item : Dict ): \"\"\" Generic process items to process a dictionary using a map function \"\"\" self . logger . debug ( \"Processing: {} \" . format ( item [ self . source . key ])) time_start = time () try : with Timeout ( seconds = self . timeout ): processed = dict ( self . unary_function ( item )) processed . update ({ \"state\" : \"successful\" }) for k in [ self . source . key , self . source . last_updated_field ]: if k in processed : del processed [ k ] except Exception as e : self . logger . error ( traceback . format_exc ()) processed = { \"error\" : str ( e ), \"state\" : \"failed\" } time_end = time () key , last_updated_field = self . source . key , self . source . last_updated_field out = { self . target . key : item [ key ], self . target . last_updated_field : self . source . _lu_func [ 0 ]( item . get ( last_updated_field , datetime . utcnow ()) ), } if self . store_process_time : out [ \"_process_time\" ] = time_end - time_start out . update ( processed ) return out unary_function ( self , item ) \u00b6 Unary function to process item You do not need to provide values for source.key and source.last_updated_field in the output. Any uncaught exceptions will be caught by process_item and logged to the \"error\" field in the target document. Source code in maggma/builders/map_builder.py @abstractmethod def unary_function ( self , item ): \"\"\" ufn: Unary function to process item You do not need to provide values for source.key and source.last_updated_field in the output. Any uncaught exceptions will be caught by process_item and logged to the \"error\" field in the target document. \"\"\" pass update_targets ( self , items ) \u00b6 Generic update targets for Map Builder Source code in maggma/builders/map_builder.py def update_targets ( self , items : List [ Dict ]): \"\"\" Generic update targets for Map Builder \"\"\" target = self . target for item in items : item [ \"_bt\" ] = datetime . utcnow () if \"_id\" in item : del item [ \"_id\" ] if len ( items ) > 0 : target . update ( items ) Many-to-Many GroupBuilder GroupBuilder ( Builder ) \u00b6 Group source docs and produces merged documents for each group Supports incremental building, where a source group gets (re)built only if it has a newer (by last_updated_field) doc than the corresponding (by key) target doc. This is a Many-to-One or Many-to-Many Builder. As a result, this builder can't determine when a source document is orphaned. Source code in maggma/builders/group_builder.py class GroupBuilder ( Builder , metaclass = ABCMeta ): \"\"\" Group source docs and produces merged documents for each group Supports incremental building, where a source group gets (re)built only if it has a newer (by last_updated_field) doc than the corresponding (by key) target doc. This is a Many-to-One or Many-to-Many Builder. As a result, this builder can't determine when a source document is orphaned. \"\"\" def __init__ ( self , source : Store , target : Store , grouping_keys : List [ str ], query : Optional [ Dict ] = None , projection : Optional [ List ] = None , timeout : int = 0 , store_process_time : bool = True , retry_failed : bool = False , ** kwargs , ): \"\"\" Args: source: source store target: target store query: optional query to filter source store projection: list of keys to project from the source for processing. Limits data transfer to improve efficiency. delete_orphans: Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. timeout: maximum running time per item in seconds store_process_time: If True, add \"_process_time\" key to document for profiling purposes retry_failed: If True, will retry building documents that previously failed \"\"\" self . source = source self . target = target self . grouping_keys = grouping_keys self . query = query self . projection = projection self . kwargs = kwargs self . timeout = timeout self . store_process_time = store_process_time self . retry_failed = retry_failed self . _target_keys_field = f \" { self . source . key } s\" super () . __init__ ( sources = [ source ], targets = [ target ], ** kwargs ) def ensure_indexes ( self ): \"\"\" Ensures indicies on critical fields for GroupBuilder which include the plural version of the target's key field \"\"\" index_checks = [ self . source . ensure_index ( self . source . key ), self . source . ensure_index ( self . source . last_updated_field ), self . target . ensure_index ( self . target . key ), self . target . ensure_index ( self . target . last_updated_field ), self . target . ensure_index ( \"state\" ), self . target . ensure_index ( self . _target_keys_field ), ] if not all ( index_checks ): self . logger . warning ( \"Missing one or more important indices on stores. \" \"Performance for large stores may be severely degraded. \" \"Ensure indices on target.key and \" \"[(store.last_updated_field, -1), (store.key, 1)] \" \"for each of source and target.\" ) def prechunk ( self , number_splits : int ) -> Iterator [ Dict ]: \"\"\" Generic prechunk for group builder to perform domain-decompostion by the grouping keys \"\"\" self . ensure_indexes () keys = self . get_ids_to_process () groups = self . get_groups_from_keys ( keys ) N = ceil ( len ( groups ) / number_splits ) for split in grouper ( keys , N ): yield { \"query\" : dict ( zip ( self . grouping_keys , split ))} def get_items ( self ): self . logger . info ( \"Starting {} Builder\" . format ( self . __class__ . __name__ )) self . ensure_indexes () keys = self . get_ids_to_process () groups = self . get_groups_from_keys ( keys ) if self . projection : projection = list ( set ( self . projection + [ self . source . key , self . source . last_updated_field ]) ) else : projection = None self . total = len ( groups ) for group in groups : docs = list ( self . source . query ( criteria = dict ( zip ( self . grouping_keys , group )), properties = projection ) ) yield docs def process_item ( self , item : List [ Dict ]) -> Dict [ Tuple , Dict ]: # type: ignore keys = list ( d [ self . source . key ] for d in item ) self . logger . debug ( \"Processing: {} \" . format ( keys )) time_start = time () try : with Timeout ( seconds = self . timeout ): processed = self . unary_function ( item ) processed . update ({ \"state\" : \"successful\" }) except Exception as e : self . logger . error ( traceback . format_exc ()) processed = { \"error\" : str ( e ), \"state\" : \"failed\" } time_end = time () last_updated = [ self . source . _lu_func [ 0 ]( d [ self . source . last_updated_field ]) for d in item ] update_doc = { self . target . key : keys [ 0 ], f \" { self . source . key } s\" : keys , self . target . last_updated_field : max ( last_updated ), \"_bt\" : datetime . utcnow (), } processed . update ({ k : v for k , v in update_doc . items () if k not in processed }) if self . store_process_time : processed [ \"_process_time\" ] = time_end - time_start return processed def update_targets ( self , items : List [ Dict ]): \"\"\" Generic update targets for Group Builder \"\"\" target = self . target for item in items : if \"_id\" in item : del item [ \"_id\" ] if len ( items ) > 0 : target . update ( items ) @abstractmethod def unary_function ( self , items : List [ Dict ]) -> Dict : \"\"\" Processing function for GroupBuilder Arguments: items: list of of documents with matching grouping keys Returns: Dictionary mapping: tuple of source document keys that are in the grouped document to the grouped and processed document \"\"\" def get_ids_to_process ( self ) -> Iterable : \"\"\" Gets the IDs that need to be processed \"\"\" query = self . query or {} distinct_from_target = list ( self . target . distinct ( self . _target_keys_field , criteria = query ) ) processed_ids = [] # Not always gauranteed that MongoDB will unpack the list so we # have to make sure we do that for d in distinct_from_target : if isinstance ( d , list ): processed_ids . extend ( d ) else : processed_ids . append ( d ) all_ids = set ( self . source . distinct ( self . source . key , criteria = query )) self . logger . debug ( f \"Found { len ( all_ids ) } total docs in source\" ) if self . retry_failed : failed_keys = self . target . distinct ( self . _target_keys_field , criteria = { \"state\" : \"failed\" , ** query } ) unprocessed_ids = all_ids - ( set ( processed_ids ) - set ( failed_keys )) self . logger . debug ( f \"Found { len ( failed_keys ) } failed IDs in target\" ) else : unprocessed_ids = all_ids - set ( processed_ids ) self . logger . info ( f \"Found { len ( unprocessed_ids ) } IDs to process\" ) new_ids = set ( self . source . newer_in ( self . target , criteria = query , exhaustive = False ) ) self . logger . info ( f \"Found { len ( new_ids ) } updated IDs to process\" ) return list ( new_ids | unprocessed_ids ) def get_groups_from_keys ( self , keys ) -> Set [ Tuple ]: \"\"\" Get the groups by grouping_keys for these documents \"\"\" grouping_keys = self . grouping_keys groups : Set [ Tuple ] = set () for chunked_keys in grouper ( keys , self . chunk_size ): docs = list ( self . source . query ( criteria = { self . source . key : { \"$in\" : chunked_keys }}, properties = grouping_keys , ) ) sub_groups = set ( tuple ( get ( d , prop , None ) for prop in grouping_keys ) for d in docs ) self . logger . debug ( f \"Found { len ( sub_groups ) } subgroups to process\" ) groups |= sub_groups self . logger . info ( f \"Found { len ( groups ) } groups to process\" ) return groups __init__ ( self , source , target , grouping_keys , query = None , projection = None , timeout = 0 , store_process_time = True , retry_failed = False , ** kwargs ) special \u00b6 Parameters: Name Type Description Default source Store source store required target Store target store required query Optional[Dict] optional query to filter source store None projection Optional[List] list of keys to project from the source for processing. Limits data transfer to improve efficiency. None delete_orphans Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. required timeout int maximum running time per item in seconds 0 store_process_time bool If True, add \"_process_time\" key to document for profiling purposes True retry_failed bool If True, will retry building documents that previously failed False Source code in maggma/builders/group_builder.py def __init__ ( self , source : Store , target : Store , grouping_keys : List [ str ], query : Optional [ Dict ] = None , projection : Optional [ List ] = None , timeout : int = 0 , store_process_time : bool = True , retry_failed : bool = False , ** kwargs , ): \"\"\" Args: source: source store target: target store query: optional query to filter source store projection: list of keys to project from the source for processing. Limits data transfer to improve efficiency. delete_orphans: Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. timeout: maximum running time per item in seconds store_process_time: If True, add \"_process_time\" key to document for profiling purposes retry_failed: If True, will retry building documents that previously failed \"\"\" self . source = source self . target = target self . grouping_keys = grouping_keys self . query = query self . projection = projection self . kwargs = kwargs self . timeout = timeout self . store_process_time = store_process_time self . retry_failed = retry_failed self . _target_keys_field = f \" { self . source . key } s\" super () . __init__ ( sources = [ source ], targets = [ target ], ** kwargs ) ensure_indexes ( self ) \u00b6 Ensures indicies on critical fields for GroupBuilder which include the plural version of the target's key field Source code in maggma/builders/group_builder.py def ensure_indexes ( self ): \"\"\" Ensures indicies on critical fields for GroupBuilder which include the plural version of the target's key field \"\"\" index_checks = [ self . source . ensure_index ( self . source . key ), self . source . ensure_index ( self . source . last_updated_field ), self . target . ensure_index ( self . target . key ), self . target . ensure_index ( self . target . last_updated_field ), self . target . ensure_index ( \"state\" ), self . target . ensure_index ( self . _target_keys_field ), ] if not all ( index_checks ): self . logger . warning ( \"Missing one or more important indices on stores. \" \"Performance for large stores may be severely degraded. \" \"Ensure indices on target.key and \" \"[(store.last_updated_field, -1), (store.key, 1)] \" \"for each of source and target.\" ) get_groups_from_keys ( self , keys ) \u00b6 Get the groups by grouping_keys for these documents Source code in maggma/builders/group_builder.py def get_groups_from_keys ( self , keys ) -> Set [ Tuple ]: \"\"\" Get the groups by grouping_keys for these documents \"\"\" grouping_keys = self . grouping_keys groups : Set [ Tuple ] = set () for chunked_keys in grouper ( keys , self . chunk_size ): docs = list ( self . source . query ( criteria = { self . source . key : { \"$in\" : chunked_keys }}, properties = grouping_keys , ) ) sub_groups = set ( tuple ( get ( d , prop , None ) for prop in grouping_keys ) for d in docs ) self . logger . debug ( f \"Found { len ( sub_groups ) } subgroups to process\" ) groups |= sub_groups self . logger . info ( f \"Found { len ( groups ) } groups to process\" ) return groups get_ids_to_process ( self ) \u00b6 Gets the IDs that need to be processed Source code in maggma/builders/group_builder.py def get_ids_to_process ( self ) -> Iterable : \"\"\" Gets the IDs that need to be processed \"\"\" query = self . query or {} distinct_from_target = list ( self . target . distinct ( self . _target_keys_field , criteria = query ) ) processed_ids = [] # Not always gauranteed that MongoDB will unpack the list so we # have to make sure we do that for d in distinct_from_target : if isinstance ( d , list ): processed_ids . extend ( d ) else : processed_ids . append ( d ) all_ids = set ( self . source . distinct ( self . source . key , criteria = query )) self . logger . debug ( f \"Found { len ( all_ids ) } total docs in source\" ) if self . retry_failed : failed_keys = self . target . distinct ( self . _target_keys_field , criteria = { \"state\" : \"failed\" , ** query } ) unprocessed_ids = all_ids - ( set ( processed_ids ) - set ( failed_keys )) self . logger . debug ( f \"Found { len ( failed_keys ) } failed IDs in target\" ) else : unprocessed_ids = all_ids - set ( processed_ids ) self . logger . info ( f \"Found { len ( unprocessed_ids ) } IDs to process\" ) new_ids = set ( self . source . newer_in ( self . target , criteria = query , exhaustive = False ) ) self . logger . info ( f \"Found { len ( new_ids ) } updated IDs to process\" ) return list ( new_ids | unprocessed_ids ) get_items ( self ) \u00b6 Returns all the items to process. Returns: Type Description generator or list of items to process Source code in maggma/builders/group_builder.py def get_items ( self ): self . logger . info ( \"Starting {} Builder\" . format ( self . __class__ . __name__ )) self . ensure_indexes () keys = self . get_ids_to_process () groups = self . get_groups_from_keys ( keys ) if self . projection : projection = list ( set ( self . projection + [ self . source . key , self . source . last_updated_field ]) ) else : projection = None self . total = len ( groups ) for group in groups : docs = list ( self . source . query ( criteria = dict ( zip ( self . grouping_keys , group )), properties = projection ) ) yield docs prechunk ( self , number_splits ) \u00b6 Generic prechunk for group builder to perform domain-decompostion by the grouping keys Source code in maggma/builders/group_builder.py def prechunk ( self , number_splits : int ) -> Iterator [ Dict ]: \"\"\" Generic prechunk for group builder to perform domain-decompostion by the grouping keys \"\"\" self . ensure_indexes () keys = self . get_ids_to_process () groups = self . get_groups_from_keys ( keys ) N = ceil ( len ( groups ) / number_splits ) for split in grouper ( keys , N ): yield { \"query\" : dict ( zip ( self . grouping_keys , split ))} process_item ( self , item ) \u00b6 Process an item. There should be no database operations in this method. Default behavior is to return the item. Parameters: Name Type Description Default item List[Dict] required Returns: Type Description item an item to update Source code in maggma/builders/group_builder.py def process_item ( self , item : List [ Dict ]) -> Dict [ Tuple , Dict ]: # type: ignore keys = list ( d [ self . source . key ] for d in item ) self . logger . debug ( \"Processing: {} \" . format ( keys )) time_start = time () try : with Timeout ( seconds = self . timeout ): processed = self . unary_function ( item ) processed . update ({ \"state\" : \"successful\" }) except Exception as e : self . logger . error ( traceback . format_exc ()) processed = { \"error\" : str ( e ), \"state\" : \"failed\" } time_end = time () last_updated = [ self . source . _lu_func [ 0 ]( d [ self . source . last_updated_field ]) for d in item ] update_doc = { self . target . key : keys [ 0 ], f \" { self . source . key } s\" : keys , self . target . last_updated_field : max ( last_updated ), \"_bt\" : datetime . utcnow (), } processed . update ({ k : v for k , v in update_doc . items () if k not in processed }) if self . store_process_time : processed [ \"_process_time\" ] = time_end - time_start return processed unary_function ( self , items ) \u00b6 Processing function for GroupBuilder Parameters: Name Type Description Default items List[Dict] list of of documents with matching grouping keys required Returns: Type Description Dictionary mapping tuple of source document keys that are in the grouped document to the grouped and processed document Source code in maggma/builders/group_builder.py @abstractmethod def unary_function ( self , items : List [ Dict ]) -> Dict : \"\"\" Processing function for GroupBuilder Arguments: items: list of of documents with matching grouping keys Returns: Dictionary mapping: tuple of source document keys that are in the grouped document to the grouped and processed document \"\"\" update_targets ( self , items ) \u00b6 Generic update targets for Group Builder Source code in maggma/builders/group_builder.py def update_targets ( self , items : List [ Dict ]): \"\"\" Generic update targets for Group Builder \"\"\" target = self . target for item in items : if \"_id\" in item : del item [ \"_id\" ] if len ( items ) > 0 : target . update ( items )","title":"Builders"},{"location":"reference/builders/#maggma.builders.map_builder.CopyBuilder","text":"Sync a source store with a target store. Source code in maggma/builders/map_builder.py class CopyBuilder ( MapBuilder ): \"\"\"Sync a source store with a target store.\"\"\" def unary_function ( self , item ): \"\"\" Identity function for copy builder map operation \"\"\" if \"_id\" in item : del item [ \"_id\" ] return item","title":"CopyBuilder"},{"location":"reference/builders/#maggma.builders.map_builder.CopyBuilder.unary_function","text":"Identity function for copy builder map operation Source code in maggma/builders/map_builder.py def unary_function ( self , item ): \"\"\" Identity function for copy builder map operation \"\"\" if \"_id\" in item : del item [ \"_id\" ] return item","title":"unary_function()"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder","text":"Apply a unary function to yield a target document for each source document. Supports incremental building, where a source document gets built only if it has newer (by last_updated_field) data than the corresponding (by key) target document. Source code in maggma/builders/map_builder.py class MapBuilder ( Builder , metaclass = ABCMeta ): \"\"\" Apply a unary function to yield a target document for each source document. Supports incremental building, where a source document gets built only if it has newer (by last_updated_field) data than the corresponding (by key) target document. \"\"\" def __init__ ( self , source : Store , target : Store , query : Optional [ Dict ] = None , projection : Optional [ List ] = None , delete_orphans : bool = False , timeout : int = 0 , store_process_time : bool = True , retry_failed : bool = False , ** kwargs , ): \"\"\" Apply a unary function to each source document. Args: source: source store target: target store query: optional query to filter source store projection: list of keys to project from the source for processing. Limits data transfer to improve efficiency. delete_orphans: Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. timeout: maximum running time per item in seconds store_process_time: If True, add \"_process_time\" key to document for profiling purposes retry_failed: If True, will retry building documents that previously failed \"\"\" self . source = source self . target = target self . query = query self . projection = projection self . delete_orphans = delete_orphans self . kwargs = kwargs self . timeout = timeout self . store_process_time = store_process_time self . retry_failed = retry_failed super () . __init__ ( sources = [ source ], targets = [ target ], ** kwargs ) def ensure_indexes ( self ): \"\"\" Ensures indicies on critical fields for MapBuilder \"\"\" index_checks = [ self . source . ensure_index ( self . source . key ), self . source . ensure_index ( self . source . last_updated_field ), self . target . ensure_index ( self . target . key ), self . target . ensure_index ( self . target . last_updated_field ), self . target . ensure_index ( \"state\" ), ] if not all ( index_checks ): self . logger . warning ( \"Missing one or more important indices on stores. \" \"Performance for large stores may be severely degraded. \" \"Ensure indices on target.key and \" \"[(store.last_updated_field, -1), (store.key, 1)] \" \"for each of source and target.\" ) def prechunk ( self , number_splits : int ) -> Iterator [ Dict ]: \"\"\" Generic prechunk for map builder to perform domain-decompostion by the key field \"\"\" self . ensure_indexes () keys = self . target . newer_in ( self . source , criteria = self . query , exhaustive = True ) N = ceil ( len ( keys ) / number_splits ) for split in grouper ( keys , N ): yield { \"query\" : { self . source . key : { \"$in\" : list ( split )}}} def get_items ( self ): \"\"\" Generic get items for Map Builder designed to perform incremental building \"\"\" self . logger . info ( \"Starting {} Builder\" . format ( self . __class__ . __name__ )) self . ensure_indexes () keys = self . target . newer_in ( self . source , criteria = self . query , exhaustive = True ) if self . retry_failed : if isinstance ( self . query , ( dict )): failed_query = { \"$and\" : [ self . query , { \"state\" : \"failed\" }]} else : failed_query = { \"state\" : \"failed\" } failed_keys = self . target . distinct ( self . target . key , criteria = failed_query ) keys = list ( set ( keys + failed_keys )) self . logger . info ( \"Processing {} items\" . format ( len ( keys ))) if self . projection : projection = list ( set ( self . projection + [ self . source . key , self . source . last_updated_field ]) ) else : projection = None self . total = len ( keys ) for chunked_keys in grouper ( keys , self . chunk_size ): chunked_keys = list ( chunked_keys ) for doc in list ( self . source . query ( criteria = { self . source . key : { \"$in\" : chunked_keys }}, properties = projection , ) ): yield doc def process_item ( self , item : Dict ): \"\"\" Generic process items to process a dictionary using a map function \"\"\" self . logger . debug ( \"Processing: {} \" . format ( item [ self . source . key ])) time_start = time () try : with Timeout ( seconds = self . timeout ): processed = dict ( self . unary_function ( item )) processed . update ({ \"state\" : \"successful\" }) for k in [ self . source . key , self . source . last_updated_field ]: if k in processed : del processed [ k ] except Exception as e : self . logger . error ( traceback . format_exc ()) processed = { \"error\" : str ( e ), \"state\" : \"failed\" } time_end = time () key , last_updated_field = self . source . key , self . source . last_updated_field out = { self . target . key : item [ key ], self . target . last_updated_field : self . source . _lu_func [ 0 ]( item . get ( last_updated_field , datetime . utcnow ()) ), } if self . store_process_time : out [ \"_process_time\" ] = time_end - time_start out . update ( processed ) return out def update_targets ( self , items : List [ Dict ]): \"\"\" Generic update targets for Map Builder \"\"\" target = self . target for item in items : item [ \"_bt\" ] = datetime . utcnow () if \"_id\" in item : del item [ \"_id\" ] if len ( items ) > 0 : target . update ( items ) def finalize ( self ): \"\"\" Finalize MapBuilder operations including removing orphaned documents \"\"\" if self . delete_orphans : source_keyvals = set ( self . source . distinct ( self . source . key )) target_keyvals = set ( self . target . distinct ( self . target . key )) to_delete = list ( target_keyvals - source_keyvals ) if len ( to_delete ): self . logger . info ( \"Finalize: Deleting {} orphans.\" . format ( len ( to_delete )) ) self . target . remove_docs ({ self . target . key : { \"$in\" : to_delete }}) super () . finalize () @abstractmethod def unary_function ( self , item ): \"\"\" ufn: Unary function to process item You do not need to provide values for source.key and source.last_updated_field in the output. Any uncaught exceptions will be caught by process_item and logged to the \"error\" field in the target document. \"\"\" pass","title":"MapBuilder"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.__init__","text":"Apply a unary function to each source document. Parameters: Name Type Description Default source Store source store required target Store target store required query Optional[Dict] optional query to filter source store None projection Optional[List] list of keys to project from the source for processing. Limits data transfer to improve efficiency. None delete_orphans bool Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. False timeout int maximum running time per item in seconds 0 store_process_time bool If True, add \"_process_time\" key to document for profiling purposes True retry_failed bool If True, will retry building documents that previously failed False Source code in maggma/builders/map_builder.py def __init__ ( self , source : Store , target : Store , query : Optional [ Dict ] = None , projection : Optional [ List ] = None , delete_orphans : bool = False , timeout : int = 0 , store_process_time : bool = True , retry_failed : bool = False , ** kwargs , ): \"\"\" Apply a unary function to each source document. Args: source: source store target: target store query: optional query to filter source store projection: list of keys to project from the source for processing. Limits data transfer to improve efficiency. delete_orphans: Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. timeout: maximum running time per item in seconds store_process_time: If True, add \"_process_time\" key to document for profiling purposes retry_failed: If True, will retry building documents that previously failed \"\"\" self . source = source self . target = target self . query = query self . projection = projection self . delete_orphans = delete_orphans self . kwargs = kwargs self . timeout = timeout self . store_process_time = store_process_time self . retry_failed = retry_failed super () . __init__ ( sources = [ source ], targets = [ target ], ** kwargs )","title":"__init__()"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.ensure_indexes","text":"Ensures indicies on critical fields for MapBuilder Source code in maggma/builders/map_builder.py def ensure_indexes ( self ): \"\"\" Ensures indicies on critical fields for MapBuilder \"\"\" index_checks = [ self . source . ensure_index ( self . source . key ), self . source . ensure_index ( self . source . last_updated_field ), self . target . ensure_index ( self . target . key ), self . target . ensure_index ( self . target . last_updated_field ), self . target . ensure_index ( \"state\" ), ] if not all ( index_checks ): self . logger . warning ( \"Missing one or more important indices on stores. \" \"Performance for large stores may be severely degraded. \" \"Ensure indices on target.key and \" \"[(store.last_updated_field, -1), (store.key, 1)] \" \"for each of source and target.\" )","title":"ensure_indexes()"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.finalize","text":"Finalize MapBuilder operations including removing orphaned documents Source code in maggma/builders/map_builder.py def finalize ( self ): \"\"\" Finalize MapBuilder operations including removing orphaned documents \"\"\" if self . delete_orphans : source_keyvals = set ( self . source . distinct ( self . source . key )) target_keyvals = set ( self . target . distinct ( self . target . key )) to_delete = list ( target_keyvals - source_keyvals ) if len ( to_delete ): self . logger . info ( \"Finalize: Deleting {} orphans.\" . format ( len ( to_delete )) ) self . target . remove_docs ({ self . target . key : { \"$in\" : to_delete }}) super () . finalize ()","title":"finalize()"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.get_items","text":"Generic get items for Map Builder designed to perform incremental building Source code in maggma/builders/map_builder.py def get_items ( self ): \"\"\" Generic get items for Map Builder designed to perform incremental building \"\"\" self . logger . info ( \"Starting {} Builder\" . format ( self . __class__ . __name__ )) self . ensure_indexes () keys = self . target . newer_in ( self . source , criteria = self . query , exhaustive = True ) if self . retry_failed : if isinstance ( self . query , ( dict )): failed_query = { \"$and\" : [ self . query , { \"state\" : \"failed\" }]} else : failed_query = { \"state\" : \"failed\" } failed_keys = self . target . distinct ( self . target . key , criteria = failed_query ) keys = list ( set ( keys + failed_keys )) self . logger . info ( \"Processing {} items\" . format ( len ( keys ))) if self . projection : projection = list ( set ( self . projection + [ self . source . key , self . source . last_updated_field ]) ) else : projection = None self . total = len ( keys ) for chunked_keys in grouper ( keys , self . chunk_size ): chunked_keys = list ( chunked_keys ) for doc in list ( self . source . query ( criteria = { self . source . key : { \"$in\" : chunked_keys }}, properties = projection , ) ): yield doc","title":"get_items()"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.prechunk","text":"Generic prechunk for map builder to perform domain-decompostion by the key field Source code in maggma/builders/map_builder.py def prechunk ( self , number_splits : int ) -> Iterator [ Dict ]: \"\"\" Generic prechunk for map builder to perform domain-decompostion by the key field \"\"\" self . ensure_indexes () keys = self . target . newer_in ( self . source , criteria = self . query , exhaustive = True ) N = ceil ( len ( keys ) / number_splits ) for split in grouper ( keys , N ): yield { \"query\" : { self . source . key : { \"$in\" : list ( split )}}}","title":"prechunk()"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.process_item","text":"Generic process items to process a dictionary using a map function Source code in maggma/builders/map_builder.py def process_item ( self , item : Dict ): \"\"\" Generic process items to process a dictionary using a map function \"\"\" self . logger . debug ( \"Processing: {} \" . format ( item [ self . source . key ])) time_start = time () try : with Timeout ( seconds = self . timeout ): processed = dict ( self . unary_function ( item )) processed . update ({ \"state\" : \"successful\" }) for k in [ self . source . key , self . source . last_updated_field ]: if k in processed : del processed [ k ] except Exception as e : self . logger . error ( traceback . format_exc ()) processed = { \"error\" : str ( e ), \"state\" : \"failed\" } time_end = time () key , last_updated_field = self . source . key , self . source . last_updated_field out = { self . target . key : item [ key ], self . target . last_updated_field : self . source . _lu_func [ 0 ]( item . get ( last_updated_field , datetime . utcnow ()) ), } if self . store_process_time : out [ \"_process_time\" ] = time_end - time_start out . update ( processed ) return out","title":"process_item()"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.unary_function","text":"Unary function to process item You do not need to provide values for source.key and source.last_updated_field in the output. Any uncaught exceptions will be caught by process_item and logged to the \"error\" field in the target document. Source code in maggma/builders/map_builder.py @abstractmethod def unary_function ( self , item ): \"\"\" ufn: Unary function to process item You do not need to provide values for source.key and source.last_updated_field in the output. Any uncaught exceptions will be caught by process_item and logged to the \"error\" field in the target document. \"\"\" pass","title":"unary_function()"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.update_targets","text":"Generic update targets for Map Builder Source code in maggma/builders/map_builder.py def update_targets ( self , items : List [ Dict ]): \"\"\" Generic update targets for Map Builder \"\"\" target = self . target for item in items : item [ \"_bt\" ] = datetime . utcnow () if \"_id\" in item : del item [ \"_id\" ] if len ( items ) > 0 : target . update ( items ) Many-to-Many GroupBuilder","title":"update_targets()"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder","text":"Group source docs and produces merged documents for each group Supports incremental building, where a source group gets (re)built only if it has a newer (by last_updated_field) doc than the corresponding (by key) target doc. This is a Many-to-One or Many-to-Many Builder. As a result, this builder can't determine when a source document is orphaned. Source code in maggma/builders/group_builder.py class GroupBuilder ( Builder , metaclass = ABCMeta ): \"\"\" Group source docs and produces merged documents for each group Supports incremental building, where a source group gets (re)built only if it has a newer (by last_updated_field) doc than the corresponding (by key) target doc. This is a Many-to-One or Many-to-Many Builder. As a result, this builder can't determine when a source document is orphaned. \"\"\" def __init__ ( self , source : Store , target : Store , grouping_keys : List [ str ], query : Optional [ Dict ] = None , projection : Optional [ List ] = None , timeout : int = 0 , store_process_time : bool = True , retry_failed : bool = False , ** kwargs , ): \"\"\" Args: source: source store target: target store query: optional query to filter source store projection: list of keys to project from the source for processing. Limits data transfer to improve efficiency. delete_orphans: Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. timeout: maximum running time per item in seconds store_process_time: If True, add \"_process_time\" key to document for profiling purposes retry_failed: If True, will retry building documents that previously failed \"\"\" self . source = source self . target = target self . grouping_keys = grouping_keys self . query = query self . projection = projection self . kwargs = kwargs self . timeout = timeout self . store_process_time = store_process_time self . retry_failed = retry_failed self . _target_keys_field = f \" { self . source . key } s\" super () . __init__ ( sources = [ source ], targets = [ target ], ** kwargs ) def ensure_indexes ( self ): \"\"\" Ensures indicies on critical fields for GroupBuilder which include the plural version of the target's key field \"\"\" index_checks = [ self . source . ensure_index ( self . source . key ), self . source . ensure_index ( self . source . last_updated_field ), self . target . ensure_index ( self . target . key ), self . target . ensure_index ( self . target . last_updated_field ), self . target . ensure_index ( \"state\" ), self . target . ensure_index ( self . _target_keys_field ), ] if not all ( index_checks ): self . logger . warning ( \"Missing one or more important indices on stores. \" \"Performance for large stores may be severely degraded. \" \"Ensure indices on target.key and \" \"[(store.last_updated_field, -1), (store.key, 1)] \" \"for each of source and target.\" ) def prechunk ( self , number_splits : int ) -> Iterator [ Dict ]: \"\"\" Generic prechunk for group builder to perform domain-decompostion by the grouping keys \"\"\" self . ensure_indexes () keys = self . get_ids_to_process () groups = self . get_groups_from_keys ( keys ) N = ceil ( len ( groups ) / number_splits ) for split in grouper ( keys , N ): yield { \"query\" : dict ( zip ( self . grouping_keys , split ))} def get_items ( self ): self . logger . info ( \"Starting {} Builder\" . format ( self . __class__ . __name__ )) self . ensure_indexes () keys = self . get_ids_to_process () groups = self . get_groups_from_keys ( keys ) if self . projection : projection = list ( set ( self . projection + [ self . source . key , self . source . last_updated_field ]) ) else : projection = None self . total = len ( groups ) for group in groups : docs = list ( self . source . query ( criteria = dict ( zip ( self . grouping_keys , group )), properties = projection ) ) yield docs def process_item ( self , item : List [ Dict ]) -> Dict [ Tuple , Dict ]: # type: ignore keys = list ( d [ self . source . key ] for d in item ) self . logger . debug ( \"Processing: {} \" . format ( keys )) time_start = time () try : with Timeout ( seconds = self . timeout ): processed = self . unary_function ( item ) processed . update ({ \"state\" : \"successful\" }) except Exception as e : self . logger . error ( traceback . format_exc ()) processed = { \"error\" : str ( e ), \"state\" : \"failed\" } time_end = time () last_updated = [ self . source . _lu_func [ 0 ]( d [ self . source . last_updated_field ]) for d in item ] update_doc = { self . target . key : keys [ 0 ], f \" { self . source . key } s\" : keys , self . target . last_updated_field : max ( last_updated ), \"_bt\" : datetime . utcnow (), } processed . update ({ k : v for k , v in update_doc . items () if k not in processed }) if self . store_process_time : processed [ \"_process_time\" ] = time_end - time_start return processed def update_targets ( self , items : List [ Dict ]): \"\"\" Generic update targets for Group Builder \"\"\" target = self . target for item in items : if \"_id\" in item : del item [ \"_id\" ] if len ( items ) > 0 : target . update ( items ) @abstractmethod def unary_function ( self , items : List [ Dict ]) -> Dict : \"\"\" Processing function for GroupBuilder Arguments: items: list of of documents with matching grouping keys Returns: Dictionary mapping: tuple of source document keys that are in the grouped document to the grouped and processed document \"\"\" def get_ids_to_process ( self ) -> Iterable : \"\"\" Gets the IDs that need to be processed \"\"\" query = self . query or {} distinct_from_target = list ( self . target . distinct ( self . _target_keys_field , criteria = query ) ) processed_ids = [] # Not always gauranteed that MongoDB will unpack the list so we # have to make sure we do that for d in distinct_from_target : if isinstance ( d , list ): processed_ids . extend ( d ) else : processed_ids . append ( d ) all_ids = set ( self . source . distinct ( self . source . key , criteria = query )) self . logger . debug ( f \"Found { len ( all_ids ) } total docs in source\" ) if self . retry_failed : failed_keys = self . target . distinct ( self . _target_keys_field , criteria = { \"state\" : \"failed\" , ** query } ) unprocessed_ids = all_ids - ( set ( processed_ids ) - set ( failed_keys )) self . logger . debug ( f \"Found { len ( failed_keys ) } failed IDs in target\" ) else : unprocessed_ids = all_ids - set ( processed_ids ) self . logger . info ( f \"Found { len ( unprocessed_ids ) } IDs to process\" ) new_ids = set ( self . source . newer_in ( self . target , criteria = query , exhaustive = False ) ) self . logger . info ( f \"Found { len ( new_ids ) } updated IDs to process\" ) return list ( new_ids | unprocessed_ids ) def get_groups_from_keys ( self , keys ) -> Set [ Tuple ]: \"\"\" Get the groups by grouping_keys for these documents \"\"\" grouping_keys = self . grouping_keys groups : Set [ Tuple ] = set () for chunked_keys in grouper ( keys , self . chunk_size ): docs = list ( self . source . query ( criteria = { self . source . key : { \"$in\" : chunked_keys }}, properties = grouping_keys , ) ) sub_groups = set ( tuple ( get ( d , prop , None ) for prop in grouping_keys ) for d in docs ) self . logger . debug ( f \"Found { len ( sub_groups ) } subgroups to process\" ) groups |= sub_groups self . logger . info ( f \"Found { len ( groups ) } groups to process\" ) return groups","title":"GroupBuilder"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.__init__","text":"Parameters: Name Type Description Default source Store source store required target Store target store required query Optional[Dict] optional query to filter source store None projection Optional[List] list of keys to project from the source for processing. Limits data transfer to improve efficiency. None delete_orphans Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. required timeout int maximum running time per item in seconds 0 store_process_time bool If True, add \"_process_time\" key to document for profiling purposes True retry_failed bool If True, will retry building documents that previously failed False Source code in maggma/builders/group_builder.py def __init__ ( self , source : Store , target : Store , grouping_keys : List [ str ], query : Optional [ Dict ] = None , projection : Optional [ List ] = None , timeout : int = 0 , store_process_time : bool = True , retry_failed : bool = False , ** kwargs , ): \"\"\" Args: source: source store target: target store query: optional query to filter source store projection: list of keys to project from the source for processing. Limits data transfer to improve efficiency. delete_orphans: Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. timeout: maximum running time per item in seconds store_process_time: If True, add \"_process_time\" key to document for profiling purposes retry_failed: If True, will retry building documents that previously failed \"\"\" self . source = source self . target = target self . grouping_keys = grouping_keys self . query = query self . projection = projection self . kwargs = kwargs self . timeout = timeout self . store_process_time = store_process_time self . retry_failed = retry_failed self . _target_keys_field = f \" { self . source . key } s\" super () . __init__ ( sources = [ source ], targets = [ target ], ** kwargs )","title":"__init__()"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.ensure_indexes","text":"Ensures indicies on critical fields for GroupBuilder which include the plural version of the target's key field Source code in maggma/builders/group_builder.py def ensure_indexes ( self ): \"\"\" Ensures indicies on critical fields for GroupBuilder which include the plural version of the target's key field \"\"\" index_checks = [ self . source . ensure_index ( self . source . key ), self . source . ensure_index ( self . source . last_updated_field ), self . target . ensure_index ( self . target . key ), self . target . ensure_index ( self . target . last_updated_field ), self . target . ensure_index ( \"state\" ), self . target . ensure_index ( self . _target_keys_field ), ] if not all ( index_checks ): self . logger . warning ( \"Missing one or more important indices on stores. \" \"Performance for large stores may be severely degraded. \" \"Ensure indices on target.key and \" \"[(store.last_updated_field, -1), (store.key, 1)] \" \"for each of source and target.\" )","title":"ensure_indexes()"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.get_groups_from_keys","text":"Get the groups by grouping_keys for these documents Source code in maggma/builders/group_builder.py def get_groups_from_keys ( self , keys ) -> Set [ Tuple ]: \"\"\" Get the groups by grouping_keys for these documents \"\"\" grouping_keys = self . grouping_keys groups : Set [ Tuple ] = set () for chunked_keys in grouper ( keys , self . chunk_size ): docs = list ( self . source . query ( criteria = { self . source . key : { \"$in\" : chunked_keys }}, properties = grouping_keys , ) ) sub_groups = set ( tuple ( get ( d , prop , None ) for prop in grouping_keys ) for d in docs ) self . logger . debug ( f \"Found { len ( sub_groups ) } subgroups to process\" ) groups |= sub_groups self . logger . info ( f \"Found { len ( groups ) } groups to process\" ) return groups","title":"get_groups_from_keys()"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.get_ids_to_process","text":"Gets the IDs that need to be processed Source code in maggma/builders/group_builder.py def get_ids_to_process ( self ) -> Iterable : \"\"\" Gets the IDs that need to be processed \"\"\" query = self . query or {} distinct_from_target = list ( self . target . distinct ( self . _target_keys_field , criteria = query ) ) processed_ids = [] # Not always gauranteed that MongoDB will unpack the list so we # have to make sure we do that for d in distinct_from_target : if isinstance ( d , list ): processed_ids . extend ( d ) else : processed_ids . append ( d ) all_ids = set ( self . source . distinct ( self . source . key , criteria = query )) self . logger . debug ( f \"Found { len ( all_ids ) } total docs in source\" ) if self . retry_failed : failed_keys = self . target . distinct ( self . _target_keys_field , criteria = { \"state\" : \"failed\" , ** query } ) unprocessed_ids = all_ids - ( set ( processed_ids ) - set ( failed_keys )) self . logger . debug ( f \"Found { len ( failed_keys ) } failed IDs in target\" ) else : unprocessed_ids = all_ids - set ( processed_ids ) self . logger . info ( f \"Found { len ( unprocessed_ids ) } IDs to process\" ) new_ids = set ( self . source . newer_in ( self . target , criteria = query , exhaustive = False ) ) self . logger . info ( f \"Found { len ( new_ids ) } updated IDs to process\" ) return list ( new_ids | unprocessed_ids )","title":"get_ids_to_process()"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.get_items","text":"Returns all the items to process. Returns: Type Description generator or list of items to process Source code in maggma/builders/group_builder.py def get_items ( self ): self . logger . info ( \"Starting {} Builder\" . format ( self . __class__ . __name__ )) self . ensure_indexes () keys = self . get_ids_to_process () groups = self . get_groups_from_keys ( keys ) if self . projection : projection = list ( set ( self . projection + [ self . source . key , self . source . last_updated_field ]) ) else : projection = None self . total = len ( groups ) for group in groups : docs = list ( self . source . query ( criteria = dict ( zip ( self . grouping_keys , group )), properties = projection ) ) yield docs","title":"get_items()"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.prechunk","text":"Generic prechunk for group builder to perform domain-decompostion by the grouping keys Source code in maggma/builders/group_builder.py def prechunk ( self , number_splits : int ) -> Iterator [ Dict ]: \"\"\" Generic prechunk for group builder to perform domain-decompostion by the grouping keys \"\"\" self . ensure_indexes () keys = self . get_ids_to_process () groups = self . get_groups_from_keys ( keys ) N = ceil ( len ( groups ) / number_splits ) for split in grouper ( keys , N ): yield { \"query\" : dict ( zip ( self . grouping_keys , split ))}","title":"prechunk()"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.process_item","text":"Process an item. There should be no database operations in this method. Default behavior is to return the item. Parameters: Name Type Description Default item List[Dict] required Returns: Type Description item an item to update Source code in maggma/builders/group_builder.py def process_item ( self , item : List [ Dict ]) -> Dict [ Tuple , Dict ]: # type: ignore keys = list ( d [ self . source . key ] for d in item ) self . logger . debug ( \"Processing: {} \" . format ( keys )) time_start = time () try : with Timeout ( seconds = self . timeout ): processed = self . unary_function ( item ) processed . update ({ \"state\" : \"successful\" }) except Exception as e : self . logger . error ( traceback . format_exc ()) processed = { \"error\" : str ( e ), \"state\" : \"failed\" } time_end = time () last_updated = [ self . source . _lu_func [ 0 ]( d [ self . source . last_updated_field ]) for d in item ] update_doc = { self . target . key : keys [ 0 ], f \" { self . source . key } s\" : keys , self . target . last_updated_field : max ( last_updated ), \"_bt\" : datetime . utcnow (), } processed . update ({ k : v for k , v in update_doc . items () if k not in processed }) if self . store_process_time : processed [ \"_process_time\" ] = time_end - time_start return processed","title":"process_item()"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.unary_function","text":"Processing function for GroupBuilder Parameters: Name Type Description Default items List[Dict] list of of documents with matching grouping keys required Returns: Type Description Dictionary mapping tuple of source document keys that are in the grouped document to the grouped and processed document Source code in maggma/builders/group_builder.py @abstractmethod def unary_function ( self , items : List [ Dict ]) -> Dict : \"\"\" Processing function for GroupBuilder Arguments: items: list of of documents with matching grouping keys Returns: Dictionary mapping: tuple of source document keys that are in the grouped document to the grouped and processed document \"\"\"","title":"unary_function()"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.update_targets","text":"Generic update targets for Group Builder Source code in maggma/builders/group_builder.py def update_targets ( self , items : List [ Dict ]): \"\"\" Generic update targets for Group Builder \"\"\" target = self . target for item in items : if \"_id\" in item : del item [ \"_id\" ] if len ( items ) > 0 : target . update ( items )","title":"update_targets()"},{"location":"reference/core_builder/","text":"Module containing the core builder definition Builder ( MSONable ) \u00b6 Base Builder class At minimum this class should implement: get_items - Get items from the sources update_targets - Updates the sources with results Multiprocessing and MPI processing can be used if all the data processing is limited to process_items Source code in maggma/core/builder.py class Builder ( MSONable , metaclass = ABCMeta ): \"\"\" Base Builder class At minimum this class should implement: get_items - Get items from the sources update_targets - Updates the sources with results Multiprocessing and MPI processing can be used if all the data processing is limited to process_items \"\"\" def __init__ ( self , sources : Union [ List [ Store ], Store ], targets : Union [ List [ Store ], Store ], chunk_size : int = 1000 , ): \"\"\" Initialize the builder the framework. Arguments: sources: source Store(s) targets: target Store(s) chunk_size: chunk size for processing \"\"\" self . sources = sources if isinstance ( sources , list ) else [ sources ] self . targets = targets if isinstance ( targets , list ) else [ targets ] self . chunk_size = chunk_size self . total = None # type: Optional[int] self . logger = logging . getLogger ( type ( self ) . __name__ ) self . logger . addHandler ( logging . NullHandler ()) def connect ( self ): \"\"\" Connect to the builder sources and targets. \"\"\" for s in self . sources + self . targets : s . connect () def prechunk ( self , number_splits : int ) -> Iterable [ Dict ]: \"\"\" Part of a domain-decomposition paradigm to allow the builder to operate on multiple nodes by divinding up the IO as well as the compute This function should return an iterator of dictionaries that can be distributed to multiple instances of the builder to get/process/udpate on Arguments: number_splits: The number of groups to split the documents to work on \"\"\" self . logger . info ( f \" { self . __class__ . __name__ } doesn't have distributed processing capabillities.\" \" Instead this builder will run on just one worker for all processing\" ) raise NotImplementedError ( f \" { self . __class__ . __name__ } doesn't have distributed processing capabillities.\" \" Instead this builder will run on just one worker for all processing\" ) @abstractmethod def get_items ( self ) -> Iterable : \"\"\" Returns all the items to process. Returns: generator or list of items to process \"\"\" pass def process_item ( self , item : Any ) -> Any : \"\"\" Process an item. There should be no database operations in this method. Default behavior is to return the item. Arguments: item: Returns: item: an item to update \"\"\" return item @abstractmethod def update_targets ( self , items : List ): \"\"\" Takes a list of items from process item and updates the targets with them. Can also perform other book keeping in the process such as storing gridfs oids, etc. Arguments: items: Returns: \"\"\" pass def finalize ( self ): \"\"\" Perform any final clean up. \"\"\" # Close any Mongo connections. for store in self . sources + self . targets : try : store . close () except AttributeError : continue def run ( self , log_level = logging . DEBUG ): \"\"\" Run the builder serially This is only intended for diagnostic purposes \"\"\" # Set up logging root = logging . getLogger () root . setLevel ( log_level ) ch = TqdmLoggingHandler () formatter = logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s - %(message)s \" ) ch . setFormatter ( formatter ) root . addHandler ( ch ) self . connect () cursor = self . get_items () for chunk in grouper ( tqdm ( cursor ), self . chunk_size ): self . logger . info ( \"Processing batch of {} items\" . format ( self . chunk_size )) processed_chunk = [ self . process_item ( item ) for item in chunk ] processed_items = [ item for item in processed_chunk if item is not None ] self . update_targets ( processed_items ) self . finalize () def __getstate__ ( self ): return self . as_dict () def __setstate__ ( self , d ): d = { k : v for k , v in d . items () if not k . startswith ( \"@\" )} d = MontyDecoder () . process_decoded ( d ) self . __init__ ( ** d ) __init__ ( self , sources , targets , chunk_size = 1000 ) special \u00b6 Initialize the builder the framework. Parameters: Name Type Description Default sources Union[List[maggma.core.store.Store], maggma.core.store.Store] source Store(s) required targets Union[List[maggma.core.store.Store], maggma.core.store.Store] target Store(s) required chunk_size int chunk size for processing 1000 Source code in maggma/core/builder.py def __init__ ( self , sources : Union [ List [ Store ], Store ], targets : Union [ List [ Store ], Store ], chunk_size : int = 1000 , ): \"\"\" Initialize the builder the framework. Arguments: sources: source Store(s) targets: target Store(s) chunk_size: chunk size for processing \"\"\" self . sources = sources if isinstance ( sources , list ) else [ sources ] self . targets = targets if isinstance ( targets , list ) else [ targets ] self . chunk_size = chunk_size self . total = None # type: Optional[int] self . logger = logging . getLogger ( type ( self ) . __name__ ) self . logger . addHandler ( logging . NullHandler ()) connect ( self ) \u00b6 Connect to the builder sources and targets. Source code in maggma/core/builder.py def connect ( self ): \"\"\" Connect to the builder sources and targets. \"\"\" for s in self . sources + self . targets : s . connect () finalize ( self ) \u00b6 Perform any final clean up. Source code in maggma/core/builder.py def finalize ( self ): \"\"\" Perform any final clean up. \"\"\" # Close any Mongo connections. for store in self . sources + self . targets : try : store . close () except AttributeError : continue get_items ( self ) \u00b6 Returns all the items to process. Returns: Type Description Iterable generator or list of items to process Source code in maggma/core/builder.py @abstractmethod def get_items ( self ) -> Iterable : \"\"\" Returns all the items to process. Returns: generator or list of items to process \"\"\" pass prechunk ( self , number_splits ) \u00b6 Part of a domain-decomposition paradigm to allow the builder to operate on multiple nodes by divinding up the IO as well as the compute This function should return an iterator of dictionaries that can be distributed to multiple instances of the builder to get/process/udpate on Parameters: Name Type Description Default number_splits int The number of groups to split the documents to work on required Source code in maggma/core/builder.py def prechunk ( self , number_splits : int ) -> Iterable [ Dict ]: \"\"\" Part of a domain-decomposition paradigm to allow the builder to operate on multiple nodes by divinding up the IO as well as the compute This function should return an iterator of dictionaries that can be distributed to multiple instances of the builder to get/process/udpate on Arguments: number_splits: The number of groups to split the documents to work on \"\"\" self . logger . info ( f \" { self . __class__ . __name__ } doesn't have distributed processing capabillities.\" \" Instead this builder will run on just one worker for all processing\" ) raise NotImplementedError ( f \" { self . __class__ . __name__ } doesn't have distributed processing capabillities.\" \" Instead this builder will run on just one worker for all processing\" ) process_item ( self , item ) \u00b6 Process an item. There should be no database operations in this method. Default behavior is to return the item. Parameters: Name Type Description Default item Any required Returns: Type Description item an item to update Source code in maggma/core/builder.py def process_item ( self , item : Any ) -> Any : \"\"\" Process an item. There should be no database operations in this method. Default behavior is to return the item. Arguments: item: Returns: item: an item to update \"\"\" return item run ( self , log_level = 10 ) \u00b6 Run the builder serially This is only intended for diagnostic purposes Source code in maggma/core/builder.py def run ( self , log_level = logging . DEBUG ): \"\"\" Run the builder serially This is only intended for diagnostic purposes \"\"\" # Set up logging root = logging . getLogger () root . setLevel ( log_level ) ch = TqdmLoggingHandler () formatter = logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s - %(message)s \" ) ch . setFormatter ( formatter ) root . addHandler ( ch ) self . connect () cursor = self . get_items () for chunk in grouper ( tqdm ( cursor ), self . chunk_size ): self . logger . info ( \"Processing batch of {} items\" . format ( self . chunk_size )) processed_chunk = [ self . process_item ( item ) for item in chunk ] processed_items = [ item for item in processed_chunk if item is not None ] self . update_targets ( processed_items ) self . finalize () update_targets ( self , items ) \u00b6 Takes a list of items from process item and updates the targets with them. Can also perform other book keeping in the process such as storing gridfs oids, etc. Parameters: Name Type Description Default items List required Source code in maggma/core/builder.py @abstractmethod def update_targets ( self , items : List ): \"\"\" Takes a list of items from process item and updates the targets with them. Can also perform other book keeping in the process such as storing gridfs oids, etc. Arguments: items: Returns: \"\"\" pass","title":"Builder"},{"location":"reference/core_builder/#maggma.core.builder.Builder","text":"Base Builder class At minimum this class should implement: get_items - Get items from the sources update_targets - Updates the sources with results Multiprocessing and MPI processing can be used if all the data processing is limited to process_items Source code in maggma/core/builder.py class Builder ( MSONable , metaclass = ABCMeta ): \"\"\" Base Builder class At minimum this class should implement: get_items - Get items from the sources update_targets - Updates the sources with results Multiprocessing and MPI processing can be used if all the data processing is limited to process_items \"\"\" def __init__ ( self , sources : Union [ List [ Store ], Store ], targets : Union [ List [ Store ], Store ], chunk_size : int = 1000 , ): \"\"\" Initialize the builder the framework. Arguments: sources: source Store(s) targets: target Store(s) chunk_size: chunk size for processing \"\"\" self . sources = sources if isinstance ( sources , list ) else [ sources ] self . targets = targets if isinstance ( targets , list ) else [ targets ] self . chunk_size = chunk_size self . total = None # type: Optional[int] self . logger = logging . getLogger ( type ( self ) . __name__ ) self . logger . addHandler ( logging . NullHandler ()) def connect ( self ): \"\"\" Connect to the builder sources and targets. \"\"\" for s in self . sources + self . targets : s . connect () def prechunk ( self , number_splits : int ) -> Iterable [ Dict ]: \"\"\" Part of a domain-decomposition paradigm to allow the builder to operate on multiple nodes by divinding up the IO as well as the compute This function should return an iterator of dictionaries that can be distributed to multiple instances of the builder to get/process/udpate on Arguments: number_splits: The number of groups to split the documents to work on \"\"\" self . logger . info ( f \" { self . __class__ . __name__ } doesn't have distributed processing capabillities.\" \" Instead this builder will run on just one worker for all processing\" ) raise NotImplementedError ( f \" { self . __class__ . __name__ } doesn't have distributed processing capabillities.\" \" Instead this builder will run on just one worker for all processing\" ) @abstractmethod def get_items ( self ) -> Iterable : \"\"\" Returns all the items to process. Returns: generator or list of items to process \"\"\" pass def process_item ( self , item : Any ) -> Any : \"\"\" Process an item. There should be no database operations in this method. Default behavior is to return the item. Arguments: item: Returns: item: an item to update \"\"\" return item @abstractmethod def update_targets ( self , items : List ): \"\"\" Takes a list of items from process item and updates the targets with them. Can also perform other book keeping in the process such as storing gridfs oids, etc. Arguments: items: Returns: \"\"\" pass def finalize ( self ): \"\"\" Perform any final clean up. \"\"\" # Close any Mongo connections. for store in self . sources + self . targets : try : store . close () except AttributeError : continue def run ( self , log_level = logging . DEBUG ): \"\"\" Run the builder serially This is only intended for diagnostic purposes \"\"\" # Set up logging root = logging . getLogger () root . setLevel ( log_level ) ch = TqdmLoggingHandler () formatter = logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s - %(message)s \" ) ch . setFormatter ( formatter ) root . addHandler ( ch ) self . connect () cursor = self . get_items () for chunk in grouper ( tqdm ( cursor ), self . chunk_size ): self . logger . info ( \"Processing batch of {} items\" . format ( self . chunk_size )) processed_chunk = [ self . process_item ( item ) for item in chunk ] processed_items = [ item for item in processed_chunk if item is not None ] self . update_targets ( processed_items ) self . finalize () def __getstate__ ( self ): return self . as_dict () def __setstate__ ( self , d ): d = { k : v for k , v in d . items () if not k . startswith ( \"@\" )} d = MontyDecoder () . process_decoded ( d ) self . __init__ ( ** d )","title":"Builder"},{"location":"reference/core_builder/#maggma.core.builder.Builder.__init__","text":"Initialize the builder the framework. Parameters: Name Type Description Default sources Union[List[maggma.core.store.Store], maggma.core.store.Store] source Store(s) required targets Union[List[maggma.core.store.Store], maggma.core.store.Store] target Store(s) required chunk_size int chunk size for processing 1000 Source code in maggma/core/builder.py def __init__ ( self , sources : Union [ List [ Store ], Store ], targets : Union [ List [ Store ], Store ], chunk_size : int = 1000 , ): \"\"\" Initialize the builder the framework. Arguments: sources: source Store(s) targets: target Store(s) chunk_size: chunk size for processing \"\"\" self . sources = sources if isinstance ( sources , list ) else [ sources ] self . targets = targets if isinstance ( targets , list ) else [ targets ] self . chunk_size = chunk_size self . total = None # type: Optional[int] self . logger = logging . getLogger ( type ( self ) . __name__ ) self . logger . addHandler ( logging . NullHandler ())","title":"__init__()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.connect","text":"Connect to the builder sources and targets. Source code in maggma/core/builder.py def connect ( self ): \"\"\" Connect to the builder sources and targets. \"\"\" for s in self . sources + self . targets : s . connect ()","title":"connect()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.finalize","text":"Perform any final clean up. Source code in maggma/core/builder.py def finalize ( self ): \"\"\" Perform any final clean up. \"\"\" # Close any Mongo connections. for store in self . sources + self . targets : try : store . close () except AttributeError : continue","title":"finalize()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.get_items","text":"Returns all the items to process. Returns: Type Description Iterable generator or list of items to process Source code in maggma/core/builder.py @abstractmethod def get_items ( self ) -> Iterable : \"\"\" Returns all the items to process. Returns: generator or list of items to process \"\"\" pass","title":"get_items()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.prechunk","text":"Part of a domain-decomposition paradigm to allow the builder to operate on multiple nodes by divinding up the IO as well as the compute This function should return an iterator of dictionaries that can be distributed to multiple instances of the builder to get/process/udpate on Parameters: Name Type Description Default number_splits int The number of groups to split the documents to work on required Source code in maggma/core/builder.py def prechunk ( self , number_splits : int ) -> Iterable [ Dict ]: \"\"\" Part of a domain-decomposition paradigm to allow the builder to operate on multiple nodes by divinding up the IO as well as the compute This function should return an iterator of dictionaries that can be distributed to multiple instances of the builder to get/process/udpate on Arguments: number_splits: The number of groups to split the documents to work on \"\"\" self . logger . info ( f \" { self . __class__ . __name__ } doesn't have distributed processing capabillities.\" \" Instead this builder will run on just one worker for all processing\" ) raise NotImplementedError ( f \" { self . __class__ . __name__ } doesn't have distributed processing capabillities.\" \" Instead this builder will run on just one worker for all processing\" )","title":"prechunk()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.process_item","text":"Process an item. There should be no database operations in this method. Default behavior is to return the item. Parameters: Name Type Description Default item Any required Returns: Type Description item an item to update Source code in maggma/core/builder.py def process_item ( self , item : Any ) -> Any : \"\"\" Process an item. There should be no database operations in this method. Default behavior is to return the item. Arguments: item: Returns: item: an item to update \"\"\" return item","title":"process_item()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.run","text":"Run the builder serially This is only intended for diagnostic purposes Source code in maggma/core/builder.py def run ( self , log_level = logging . DEBUG ): \"\"\" Run the builder serially This is only intended for diagnostic purposes \"\"\" # Set up logging root = logging . getLogger () root . setLevel ( log_level ) ch = TqdmLoggingHandler () formatter = logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s - %(message)s \" ) ch . setFormatter ( formatter ) root . addHandler ( ch ) self . connect () cursor = self . get_items () for chunk in grouper ( tqdm ( cursor ), self . chunk_size ): self . logger . info ( \"Processing batch of {} items\" . format ( self . chunk_size )) processed_chunk = [ self . process_item ( item ) for item in chunk ] processed_items = [ item for item in processed_chunk if item is not None ] self . update_targets ( processed_items ) self . finalize ()","title":"run()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.update_targets","text":"Takes a list of items from process item and updates the targets with them. Can also perform other book keeping in the process such as storing gridfs oids, etc. Parameters: Name Type Description Default items List required Source code in maggma/core/builder.py @abstractmethod def update_targets ( self , items : List ): \"\"\" Takes a list of items from process item and updates the targets with them. Can also perform other book keeping in the process such as storing gridfs oids, etc. Arguments: items: Returns: \"\"\" pass","title":"update_targets()"},{"location":"reference/core_drone/","text":"Document ( BaseModel ) pydantic-model \u00b6 Represent a file Source code in maggma/core/drone.py class Document ( BaseModel ): \"\"\" Represent a file \"\"\" path : PosixPath = Field ( ... , title = \"Path of this file\" ) name : str = Field ( ... , title = \"File name\" ) Drone ( Builder ) \u00b6 An abstract drone that handles operations with database, using Builder to support multi-threading User have to implement all abstract methods to specify the data that they want to use to interact with this class Note: All embarrassingly parallel function should be in process_items Note: The steps of a Drone can be divided into roughly 5 steps For sample usage, please see /docs/getting_started/core_drone.md and example implementation is available in tests/builders/test_simple_bibdrone.py Source code in maggma/core/drone.py class Drone ( Builder ): \"\"\" An abstract drone that handles operations with database, using Builder to support multi-threading User have to implement all abstract methods to specify the data that they want to use to interact with this class Note: All embarrassingly parallel function should be in process_items Note: The steps of a Drone can be divided into roughly 5 steps For sample usage, please see /docs/getting_started/core_drone.md and example implementation is available in tests/builders/test_simple_bibdrone.py \"\"\" def __init__ ( self , store , path : Path ): self . store = store self . path = path super () . __init__ ( sources = [], targets = store ) @abstractmethod def compute_record_identifier_key ( self , doc : Document ) -> str : \"\"\" Compute the RecordIdentifier key that this document correspond to Args: doc: document which the record identifier key will be inferred from Returns: RecordIdentifiierKey \"\"\" raise NotImplementedError @abstractmethod def read ( self , path : Path ) -> List [ RecordIdentifier ]: \"\"\" Given a folder path to a data folder, read all the files, and return a dictionary that maps each RecordKey -> [RecordIdentifier] ** Note: require user to implement the function computeRecordIdentifierKey Args: path: Path object that indicate a path to a data folder Returns: List of Record Identifiers \"\"\" pass @abstractmethod def compute_data ( self , recordID : RecordIdentifier ) -> Dict : \"\"\" User can specify what raw data they want to save from the Documents that this recordID refers to Args: recordID: recordID that needs to be re-saved Returns: Dictionary of NAME_OF_DATA -> DATA ex: for a recordID refering to 1, { \"citation\": cite.bibtex , \"text\": data.txt } \"\"\" pass def should_update_records ( self , record_identifiers : List [ RecordIdentifier ] ) -> List [ RecordIdentifier ]: \"\"\" Batch query database by computing all the keys and send them at once Args: record_identifiers: all the record_identifiers that need to fetch from database Returns: List of recordIdentifiers representing data that needs to be updated \"\"\" cursor = self . store . query ( criteria = { \"record_key\" : { \"$in\" : [ r . record_key for r in record_identifiers ]} }, properties = [ \"record_key\" , \"state_hash\" , \"last_updated\" ], ) not_exists = object () db_record_log = { doc [ \"record_key\" ]: doc [ \"state_hash\" ] for doc in cursor } to_update_list = [ recordID . state_hash != db_record_log . get ( recordID . record_key , not_exists ) for recordID in record_identifiers ] return [ recordID for recordID , to_update in zip ( record_identifiers , to_update_list ) if to_update ] def assimilate ( self , path : Path ) -> List [ RecordIdentifier ]: \"\"\" Function mainly for debugging purpose. It will 1. read file in the path specified 2. convert them into recordIdentifier 3. return the converted recordIdentifiers Args: path: path in which files are read Returns: list of record Identifiers \"\"\" record_identifiers : List [ RecordIdentifier ] = self . read ( path = path ) return record_identifiers def get_items ( self ) -> Iterable : \"\"\" Read from the path that was given, compare against database files to find recordIDs that needs to be updated Returns: RecordIdentifiers that needs to be updated \"\"\" self . logger . debug ( \"Starting get_items in {} Builder\" . format ( self . __class__ . __name__ ) ) record_identifiers : List [ RecordIdentifier ] = self . read ( path = self . path ) records_to_update = self . should_update_records ( record_identifiers ) return records_to_update def update_targets ( self , items : List ): \"\"\" Receive a list of items to update, update the items Assume that each item are in the correct format Args: items: List of items to update Returns: None \"\"\" if len ( items ) > 0 : self . logger . debug ( \"Updating {} items\" . format ( len ( items ))) self . store . update ( items ) else : self . logger . debug ( \"There are no items to update\" ) def process_item ( self , item : RecordIdentifier ) -> Dict : # type: ignore \"\"\" compute the item to update Args: item: item to update Returns: result from expanding the item \"\"\" return { ** self . compute_data ( recordID = item ), ** item . dict ()} assimilate ( self , path ) \u00b6 Function mainly for debugging purpose. It will 1. read file in the path specified 2. convert them into recordIdentifier 3. return the converted recordIdentifiers Parameters: Name Type Description Default path Path path in which files are read required Returns: Type Description List[maggma.core.drone.RecordIdentifier] list of record Identifiers Source code in maggma/core/drone.py def assimilate ( self , path : Path ) -> List [ RecordIdentifier ]: \"\"\" Function mainly for debugging purpose. It will 1. read file in the path specified 2. convert them into recordIdentifier 3. return the converted recordIdentifiers Args: path: path in which files are read Returns: list of record Identifiers \"\"\" record_identifiers : List [ RecordIdentifier ] = self . read ( path = path ) return record_identifiers compute_data ( self , recordID ) \u00b6 User can specify what raw data they want to save from the Documents that this recordID refers to Parameters: Name Type Description Default recordID RecordIdentifier recordID that needs to be re-saved required Returns: Type Description Dictionary of NAME_OF_DATA -> DATA ex for a recordID refering to 1, { \"citation\": cite.bibtex , \"text\": data.txt } Source code in maggma/core/drone.py @abstractmethod def compute_data ( self , recordID : RecordIdentifier ) -> Dict : \"\"\" User can specify what raw data they want to save from the Documents that this recordID refers to Args: recordID: recordID that needs to be re-saved Returns: Dictionary of NAME_OF_DATA -> DATA ex: for a recordID refering to 1, { \"citation\": cite.bibtex , \"text\": data.txt } \"\"\" pass compute_record_identifier_key ( self , doc ) \u00b6 Compute the RecordIdentifier key that this document correspond to Parameters: Name Type Description Default doc Document document which the record identifier key will be inferred from required Returns: Type Description str RecordIdentifiierKey Source code in maggma/core/drone.py @abstractmethod def compute_record_identifier_key ( self , doc : Document ) -> str : \"\"\" Compute the RecordIdentifier key that this document correspond to Args: doc: document which the record identifier key will be inferred from Returns: RecordIdentifiierKey \"\"\" raise NotImplementedError get_items ( self ) \u00b6 Read from the path that was given, compare against database files to find recordIDs that needs to be updated Returns: Type Description Iterable RecordIdentifiers that needs to be updated Source code in maggma/core/drone.py def get_items ( self ) -> Iterable : \"\"\" Read from the path that was given, compare against database files to find recordIDs that needs to be updated Returns: RecordIdentifiers that needs to be updated \"\"\" self . logger . debug ( \"Starting get_items in {} Builder\" . format ( self . __class__ . __name__ ) ) record_identifiers : List [ RecordIdentifier ] = self . read ( path = self . path ) records_to_update = self . should_update_records ( record_identifiers ) return records_to_update process_item ( self , item ) \u00b6 compute the item to update Parameters: Name Type Description Default item RecordIdentifier item to update required Returns: Type Description Dict result from expanding the item Source code in maggma/core/drone.py def process_item ( self , item : RecordIdentifier ) -> Dict : # type: ignore \"\"\" compute the item to update Args: item: item to update Returns: result from expanding the item \"\"\" return { ** self . compute_data ( recordID = item ), ** item . dict ()} read ( self , path ) \u00b6 Given a folder path to a data folder, read all the files, and return a dictionary that maps each RecordKey -> [RecordIdentifier] ** Note: require user to implement the function computeRecordIdentifierKey Parameters: Name Type Description Default path Path Path object that indicate a path to a data folder required Returns: Type Description List[maggma.core.drone.RecordIdentifier] List of Record Identifiers Source code in maggma/core/drone.py @abstractmethod def read ( self , path : Path ) -> List [ RecordIdentifier ]: \"\"\" Given a folder path to a data folder, read all the files, and return a dictionary that maps each RecordKey -> [RecordIdentifier] ** Note: require user to implement the function computeRecordIdentifierKey Args: path: Path object that indicate a path to a data folder Returns: List of Record Identifiers \"\"\" pass should_update_records ( self , record_identifiers ) \u00b6 Batch query database by computing all the keys and send them at once Parameters: Name Type Description Default record_identifiers List[maggma.core.drone.RecordIdentifier] all the record_identifiers that need to fetch from database required Returns: Type Description List[maggma.core.drone.RecordIdentifier] List of recordIdentifiers representing data that needs to be updated Source code in maggma/core/drone.py def should_update_records ( self , record_identifiers : List [ RecordIdentifier ] ) -> List [ RecordIdentifier ]: \"\"\" Batch query database by computing all the keys and send them at once Args: record_identifiers: all the record_identifiers that need to fetch from database Returns: List of recordIdentifiers representing data that needs to be updated \"\"\" cursor = self . store . query ( criteria = { \"record_key\" : { \"$in\" : [ r . record_key for r in record_identifiers ]} }, properties = [ \"record_key\" , \"state_hash\" , \"last_updated\" ], ) not_exists = object () db_record_log = { doc [ \"record_key\" ]: doc [ \"state_hash\" ] for doc in cursor } to_update_list = [ recordID . state_hash != db_record_log . get ( recordID . record_key , not_exists ) for recordID in record_identifiers ] return [ recordID for recordID , to_update in zip ( record_identifiers , to_update_list ) if to_update ] update_targets ( self , items ) \u00b6 Receive a list of items to update, update the items Assume that each item are in the correct format Parameters: Name Type Description Default items List List of items to update required Returns: Type Description None Source code in maggma/core/drone.py def update_targets ( self , items : List ): \"\"\" Receive a list of items to update, update the items Assume that each item are in the correct format Args: items: List of items to update Returns: None \"\"\" if len ( items ) > 0 : self . logger . debug ( \"Updating {} items\" . format ( len ( items ))) self . store . update ( items ) else : self . logger . debug ( \"There are no items to update\" ) RecordIdentifier ( BaseModel ) pydantic-model \u00b6 The meta data for a record Source code in maggma/core/drone.py class RecordIdentifier ( BaseModel ): \"\"\" The meta data for a record \"\"\" last_updated : datetime = Field ( ... , title = \"The time in which this record is last updated\" ) documents : List [ Document ] = Field ( [], title = \"List of documents this RecordIdentifier indicate\" ) record_key : str = Field ( ... , title = \"Hash that uniquely define this record, can be inferred from each document inside\" , ) state_hash : Optional [ str ] = Field ( None , title = \"Hash of the state of the documents in this Record\" ) @property def parent_directory ( self ) -> Path : \"\"\" root most directory that documnents in this record share :return: \"\"\" paths = [ doc . path . as_posix () for doc in self . documents ] parent_path = Path ( os . path . commonprefix ( paths )) if not parent_path . is_dir (): return parent_path . parent return parent_path def compute_state_hash ( self ) -> str : \"\"\" compute the hash of the state of the documents in this record :param doc_list: list of documents :return: hash of the list of documents passed in \"\"\" digest = hashlib . md5 () for doc in self . documents : digest . update ( doc . name . encode ()) with open ( doc . path . as_posix (), \"rb\" ) as file : buf = file . read () digest . update ( buf ) return str ( digest . hexdigest ()) parent_directory : Path property readonly \u00b6 root most directory that documnents in this record share :return: compute_state_hash ( self ) \u00b6 compute the hash of the state of the documents in this record :param doc_list: list of documents :return: hash of the list of documents passed in Source code in maggma/core/drone.py def compute_state_hash ( self ) -> str : \"\"\" compute the hash of the state of the documents in this record :param doc_list: list of documents :return: hash of the list of documents passed in \"\"\" digest = hashlib . md5 () for doc in self . documents : digest . update ( doc . name . encode ()) with open ( doc . path . as_posix (), \"rb\" ) as file : buf = file . read () digest . update ( buf ) return str ( digest . hexdigest ())","title":"Drone"},{"location":"reference/core_drone/#maggma.core.drone.Document","text":"Represent a file Source code in maggma/core/drone.py class Document ( BaseModel ): \"\"\" Represent a file \"\"\" path : PosixPath = Field ( ... , title = \"Path of this file\" ) name : str = Field ( ... , title = \"File name\" )","title":"Document"},{"location":"reference/core_drone/#maggma.core.drone.Drone","text":"An abstract drone that handles operations with database, using Builder to support multi-threading User have to implement all abstract methods to specify the data that they want to use to interact with this class Note: All embarrassingly parallel function should be in process_items Note: The steps of a Drone can be divided into roughly 5 steps For sample usage, please see /docs/getting_started/core_drone.md and example implementation is available in tests/builders/test_simple_bibdrone.py Source code in maggma/core/drone.py class Drone ( Builder ): \"\"\" An abstract drone that handles operations with database, using Builder to support multi-threading User have to implement all abstract methods to specify the data that they want to use to interact with this class Note: All embarrassingly parallel function should be in process_items Note: The steps of a Drone can be divided into roughly 5 steps For sample usage, please see /docs/getting_started/core_drone.md and example implementation is available in tests/builders/test_simple_bibdrone.py \"\"\" def __init__ ( self , store , path : Path ): self . store = store self . path = path super () . __init__ ( sources = [], targets = store ) @abstractmethod def compute_record_identifier_key ( self , doc : Document ) -> str : \"\"\" Compute the RecordIdentifier key that this document correspond to Args: doc: document which the record identifier key will be inferred from Returns: RecordIdentifiierKey \"\"\" raise NotImplementedError @abstractmethod def read ( self , path : Path ) -> List [ RecordIdentifier ]: \"\"\" Given a folder path to a data folder, read all the files, and return a dictionary that maps each RecordKey -> [RecordIdentifier] ** Note: require user to implement the function computeRecordIdentifierKey Args: path: Path object that indicate a path to a data folder Returns: List of Record Identifiers \"\"\" pass @abstractmethod def compute_data ( self , recordID : RecordIdentifier ) -> Dict : \"\"\" User can specify what raw data they want to save from the Documents that this recordID refers to Args: recordID: recordID that needs to be re-saved Returns: Dictionary of NAME_OF_DATA -> DATA ex: for a recordID refering to 1, { \"citation\": cite.bibtex , \"text\": data.txt } \"\"\" pass def should_update_records ( self , record_identifiers : List [ RecordIdentifier ] ) -> List [ RecordIdentifier ]: \"\"\" Batch query database by computing all the keys and send them at once Args: record_identifiers: all the record_identifiers that need to fetch from database Returns: List of recordIdentifiers representing data that needs to be updated \"\"\" cursor = self . store . query ( criteria = { \"record_key\" : { \"$in\" : [ r . record_key for r in record_identifiers ]} }, properties = [ \"record_key\" , \"state_hash\" , \"last_updated\" ], ) not_exists = object () db_record_log = { doc [ \"record_key\" ]: doc [ \"state_hash\" ] for doc in cursor } to_update_list = [ recordID . state_hash != db_record_log . get ( recordID . record_key , not_exists ) for recordID in record_identifiers ] return [ recordID for recordID , to_update in zip ( record_identifiers , to_update_list ) if to_update ] def assimilate ( self , path : Path ) -> List [ RecordIdentifier ]: \"\"\" Function mainly for debugging purpose. It will 1. read file in the path specified 2. convert them into recordIdentifier 3. return the converted recordIdentifiers Args: path: path in which files are read Returns: list of record Identifiers \"\"\" record_identifiers : List [ RecordIdentifier ] = self . read ( path = path ) return record_identifiers def get_items ( self ) -> Iterable : \"\"\" Read from the path that was given, compare against database files to find recordIDs that needs to be updated Returns: RecordIdentifiers that needs to be updated \"\"\" self . logger . debug ( \"Starting get_items in {} Builder\" . format ( self . __class__ . __name__ ) ) record_identifiers : List [ RecordIdentifier ] = self . read ( path = self . path ) records_to_update = self . should_update_records ( record_identifiers ) return records_to_update def update_targets ( self , items : List ): \"\"\" Receive a list of items to update, update the items Assume that each item are in the correct format Args: items: List of items to update Returns: None \"\"\" if len ( items ) > 0 : self . logger . debug ( \"Updating {} items\" . format ( len ( items ))) self . store . update ( items ) else : self . logger . debug ( \"There are no items to update\" ) def process_item ( self , item : RecordIdentifier ) -> Dict : # type: ignore \"\"\" compute the item to update Args: item: item to update Returns: result from expanding the item \"\"\" return { ** self . compute_data ( recordID = item ), ** item . dict ()}","title":"Drone"},{"location":"reference/core_drone/#maggma.core.drone.Drone.assimilate","text":"Function mainly for debugging purpose. It will 1. read file in the path specified 2. convert them into recordIdentifier 3. return the converted recordIdentifiers Parameters: Name Type Description Default path Path path in which files are read required Returns: Type Description List[maggma.core.drone.RecordIdentifier] list of record Identifiers Source code in maggma/core/drone.py def assimilate ( self , path : Path ) -> List [ RecordIdentifier ]: \"\"\" Function mainly for debugging purpose. It will 1. read file in the path specified 2. convert them into recordIdentifier 3. return the converted recordIdentifiers Args: path: path in which files are read Returns: list of record Identifiers \"\"\" record_identifiers : List [ RecordIdentifier ] = self . read ( path = path ) return record_identifiers","title":"assimilate()"},{"location":"reference/core_drone/#maggma.core.drone.Drone.compute_data","text":"User can specify what raw data they want to save from the Documents that this recordID refers to Parameters: Name Type Description Default recordID RecordIdentifier recordID that needs to be re-saved required Returns: Type Description Dictionary of NAME_OF_DATA -> DATA ex for a recordID refering to 1, { \"citation\": cite.bibtex , \"text\": data.txt } Source code in maggma/core/drone.py @abstractmethod def compute_data ( self , recordID : RecordIdentifier ) -> Dict : \"\"\" User can specify what raw data they want to save from the Documents that this recordID refers to Args: recordID: recordID that needs to be re-saved Returns: Dictionary of NAME_OF_DATA -> DATA ex: for a recordID refering to 1, { \"citation\": cite.bibtex , \"text\": data.txt } \"\"\" pass","title":"compute_data()"},{"location":"reference/core_drone/#maggma.core.drone.Drone.compute_record_identifier_key","text":"Compute the RecordIdentifier key that this document correspond to Parameters: Name Type Description Default doc Document document which the record identifier key will be inferred from required Returns: Type Description str RecordIdentifiierKey Source code in maggma/core/drone.py @abstractmethod def compute_record_identifier_key ( self , doc : Document ) -> str : \"\"\" Compute the RecordIdentifier key that this document correspond to Args: doc: document which the record identifier key will be inferred from Returns: RecordIdentifiierKey \"\"\" raise NotImplementedError","title":"compute_record_identifier_key()"},{"location":"reference/core_drone/#maggma.core.drone.Drone.get_items","text":"Read from the path that was given, compare against database files to find recordIDs that needs to be updated Returns: Type Description Iterable RecordIdentifiers that needs to be updated Source code in maggma/core/drone.py def get_items ( self ) -> Iterable : \"\"\" Read from the path that was given, compare against database files to find recordIDs that needs to be updated Returns: RecordIdentifiers that needs to be updated \"\"\" self . logger . debug ( \"Starting get_items in {} Builder\" . format ( self . __class__ . __name__ ) ) record_identifiers : List [ RecordIdentifier ] = self . read ( path = self . path ) records_to_update = self . should_update_records ( record_identifiers ) return records_to_update","title":"get_items()"},{"location":"reference/core_drone/#maggma.core.drone.Drone.process_item","text":"compute the item to update Parameters: Name Type Description Default item RecordIdentifier item to update required Returns: Type Description Dict result from expanding the item Source code in maggma/core/drone.py def process_item ( self , item : RecordIdentifier ) -> Dict : # type: ignore \"\"\" compute the item to update Args: item: item to update Returns: result from expanding the item \"\"\" return { ** self . compute_data ( recordID = item ), ** item . dict ()}","title":"process_item()"},{"location":"reference/core_drone/#maggma.core.drone.Drone.read","text":"Given a folder path to a data folder, read all the files, and return a dictionary that maps each RecordKey -> [RecordIdentifier] ** Note: require user to implement the function computeRecordIdentifierKey Parameters: Name Type Description Default path Path Path object that indicate a path to a data folder required Returns: Type Description List[maggma.core.drone.RecordIdentifier] List of Record Identifiers Source code in maggma/core/drone.py @abstractmethod def read ( self , path : Path ) -> List [ RecordIdentifier ]: \"\"\" Given a folder path to a data folder, read all the files, and return a dictionary that maps each RecordKey -> [RecordIdentifier] ** Note: require user to implement the function computeRecordIdentifierKey Args: path: Path object that indicate a path to a data folder Returns: List of Record Identifiers \"\"\" pass","title":"read()"},{"location":"reference/core_drone/#maggma.core.drone.Drone.should_update_records","text":"Batch query database by computing all the keys and send them at once Parameters: Name Type Description Default record_identifiers List[maggma.core.drone.RecordIdentifier] all the record_identifiers that need to fetch from database required Returns: Type Description List[maggma.core.drone.RecordIdentifier] List of recordIdentifiers representing data that needs to be updated Source code in maggma/core/drone.py def should_update_records ( self , record_identifiers : List [ RecordIdentifier ] ) -> List [ RecordIdentifier ]: \"\"\" Batch query database by computing all the keys and send them at once Args: record_identifiers: all the record_identifiers that need to fetch from database Returns: List of recordIdentifiers representing data that needs to be updated \"\"\" cursor = self . store . query ( criteria = { \"record_key\" : { \"$in\" : [ r . record_key for r in record_identifiers ]} }, properties = [ \"record_key\" , \"state_hash\" , \"last_updated\" ], ) not_exists = object () db_record_log = { doc [ \"record_key\" ]: doc [ \"state_hash\" ] for doc in cursor } to_update_list = [ recordID . state_hash != db_record_log . get ( recordID . record_key , not_exists ) for recordID in record_identifiers ] return [ recordID for recordID , to_update in zip ( record_identifiers , to_update_list ) if to_update ]","title":"should_update_records()"},{"location":"reference/core_drone/#maggma.core.drone.Drone.update_targets","text":"Receive a list of items to update, update the items Assume that each item are in the correct format Parameters: Name Type Description Default items List List of items to update required Returns: Type Description None Source code in maggma/core/drone.py def update_targets ( self , items : List ): \"\"\" Receive a list of items to update, update the items Assume that each item are in the correct format Args: items: List of items to update Returns: None \"\"\" if len ( items ) > 0 : self . logger . debug ( \"Updating {} items\" . format ( len ( items ))) self . store . update ( items ) else : self . logger . debug ( \"There are no items to update\" )","title":"update_targets()"},{"location":"reference/core_drone/#maggma.core.drone.RecordIdentifier","text":"The meta data for a record Source code in maggma/core/drone.py class RecordIdentifier ( BaseModel ): \"\"\" The meta data for a record \"\"\" last_updated : datetime = Field ( ... , title = \"The time in which this record is last updated\" ) documents : List [ Document ] = Field ( [], title = \"List of documents this RecordIdentifier indicate\" ) record_key : str = Field ( ... , title = \"Hash that uniquely define this record, can be inferred from each document inside\" , ) state_hash : Optional [ str ] = Field ( None , title = \"Hash of the state of the documents in this Record\" ) @property def parent_directory ( self ) -> Path : \"\"\" root most directory that documnents in this record share :return: \"\"\" paths = [ doc . path . as_posix () for doc in self . documents ] parent_path = Path ( os . path . commonprefix ( paths )) if not parent_path . is_dir (): return parent_path . parent return parent_path def compute_state_hash ( self ) -> str : \"\"\" compute the hash of the state of the documents in this record :param doc_list: list of documents :return: hash of the list of documents passed in \"\"\" digest = hashlib . md5 () for doc in self . documents : digest . update ( doc . name . encode ()) with open ( doc . path . as_posix (), \"rb\" ) as file : buf = file . read () digest . update ( buf ) return str ( digest . hexdigest ())","title":"RecordIdentifier"},{"location":"reference/core_drone/#maggma.core.drone.RecordIdentifier.parent_directory","text":"root most directory that documnents in this record share :return:","title":"parent_directory"},{"location":"reference/core_drone/#maggma.core.drone.RecordIdentifier.compute_state_hash","text":"compute the hash of the state of the documents in this record :param doc_list: list of documents :return: hash of the list of documents passed in Source code in maggma/core/drone.py def compute_state_hash ( self ) -> str : \"\"\" compute the hash of the state of the documents in this record :param doc_list: list of documents :return: hash of the list of documents passed in \"\"\" digest = hashlib . md5 () for doc in self . documents : digest . update ( doc . name . encode ()) with open ( doc . path . as_posix (), \"rb\" ) as file : buf = file . read () digest . update ( buf ) return str ( digest . hexdigest ())","title":"compute_state_hash()"},{"location":"reference/core_store/","text":"Module containing the core Store definition DateTimeFormat ( Enum ) \u00b6 Datetime format in store document Source code in maggma/core/store.py class DateTimeFormat ( Enum ): \"\"\"Datetime format in store document\"\"\" DateTime = \"datetime\" IsoFormat = \"isoformat\" Sort ( Enum ) \u00b6 Enumeration for sorting order Source code in maggma/core/store.py class Sort ( Enum ): \"\"\"Enumeration for sorting order\"\"\" Ascending = 1 Descending = - 1 Store ( MSONable ) \u00b6 Abstract class for a data Store Defines the interface for all data going in and out of a Builder Source code in maggma/core/store.py class Store ( MSONable , metaclass = ABCMeta ): \"\"\" Abstract class for a data Store Defines the interface for all data going in and out of a Builder \"\"\" def __init__ ( self , key : str = \"task_id\" , last_updated_field : str = \"last_updated\" , last_updated_type : DateTimeFormat = DateTimeFormat ( \"datetime\" ), validator : Optional [ Validator ] = None , ): \"\"\" Args: key: main key to index on last_updated_field: field for date/time stamping the data last_updated_type: the date/time format for the last_updated_field. Can be \"datetime\" or \"isoformat\" validator: Validator to validate documents going into the store \"\"\" self . key = key self . last_updated_field = last_updated_field self . last_updated_type = last_updated_type self . _lu_func = ( LU_KEY_ISOFORMAT if DateTimeFormat ( last_updated_type ) == DateTimeFormat . IsoFormat else ( identity , identity ) ) # type: Tuple[Callable, Callable] self . validator = validator self . logger = logging . getLogger ( type ( self ) . __name__ ) self . logger . addHandler ( logging . NullHandler ()) @abstractproperty def _collection ( self ): \"\"\" Returns a handle to the pymongo collection object \"\"\" @abstractproperty def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" @abstractmethod def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data Args: force_reset: whether to reset the connection or not \"\"\" @abstractmethod def close ( self ): \"\"\" Closes any connections \"\"\" @abstractmethod def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" @abstractmethod def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" @abstractmethod def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" @abstractmethod def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" @abstractmethod def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" @abstractmethod def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" def query_one ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , ): \"\"\" Queries the Store for a single document Args: criteria: PyMongo filter for documents to search properties: properties to return in the document sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. \"\"\" return next ( self . query ( criteria = criteria , properties = properties , sort = sort ), None ) def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria or {} results = [ key for key , _ in self . groupby ( field , properties = [ field ], criteria = criteria ) ] results = [ get ( r , field ) for r in results ] return results @property def last_updated ( self ) -> datetime : \"\"\" Provides the most recent last_updated date time stamp from the documents in this Store \"\"\" doc = next ( self . query ( properties = [ self . last_updated_field ], sort = { self . last_updated_field : - 1 }, limit = 1 , ), None , ) if doc and not has ( doc , self . last_updated_field ): raise StoreError ( f \"No field ' { self . last_updated_field } ' in store document. Please ensure Store.last_updated_field \" \"is a datetime field in your store that represents the time of \" \"last update to each document.\" ) elif not doc or get ( doc , self . last_updated_field ) is None : # Handle when collection has docs but `NoneType` last_updated_field. return datetime . min else : return self . _lu_func [ 0 ]( get ( doc , self . last_updated_field )) def newer_in ( self , target : \"Store\" , criteria : Optional [ Dict ] = None , exhaustive : bool = False ) -> List [ str ]: \"\"\" Returns the keys of documents that are newer in the target Store than this Store. Args: target: target Store to criteria: PyMongo filter for documents to search in exhaustive: triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in \"\"\" self . ensure_index ( self . key ) self . ensure_index ( self . last_updated_field ) if exhaustive : # Get our current last_updated dates for each key value props = { self . key : 1 , self . last_updated_field : 1 , \"_id\" : 0 } dates = { d [ self . key ]: self . _lu_func [ 0 ]( d . get ( self . last_updated_field , datetime . max ) ) for d in self . query ( properties = props ) } # Get the last_updated for the store we're comparing with props = { target . key : 1 , target . last_updated_field : 1 , \"_id\" : 0 } target_dates = { d [ target . key ]: target . _lu_func [ 0 ]( d . get ( target . last_updated_field , datetime . min ) ) for d in target . query ( criteria = criteria , properties = props ) } new_keys = set ( target_dates . keys ()) - set ( dates . keys ()) updated_keys = { key for key , date in dates . items () if target_dates . get ( key , datetime . min ) > date } return list ( new_keys | updated_keys ) else : criteria = { self . last_updated_field : { \"$gt\" : self . _lu_func [ 1 ]( self . last_updated )} } return target . distinct ( field = self . key , criteria = criteria ) @deprecated ( message = \"Please use Store.newer_in\" ) def lu_filter ( self , targets ): \"\"\"Creates a MongoDB filter for new documents. By \"new\", we mean documents in this Store that were last updated later than any document in targets. Args: targets (list): A list of Stores \"\"\" if isinstance ( targets , Store ): targets = [ targets ] lu_list = [ t . last_updated for t in targets ] return { self . last_updated_field : { \"$gt\" : self . _lu_func [ 1 ]( max ( lu_list ))}} @deprecated ( message = \"Use Store.newer_in\" ) def updated_keys ( self , target , criteria = None ): \"\"\" Returns keys for docs that are newer in the target store in comparison with this store when comparing the last updated field (last_updated_field) Args: target (Store): store to look for updated documents criteria (dict): mongo query to limit scope Returns: list of keys that have been updated in target store \"\"\" self . ensure_index ( self . key ) self . ensure_index ( self . last_updated_field ) return self . newer_in ( target , criteria = criteria ) def __ne__ ( self , other ): return not self == other def __getstate__ ( self ): return self . as_dict () def __setstate__ ( self , d ): d = { k : v for k , v in d . items () if not k . startswith ( \"@\" )} d = MontyDecoder () . process_decoded ( d ) self . __init__ ( ** d ) def __enter__ ( self ): self . connect () return self def __exit__ ( self , exception_type , exception_value , traceback ): self . close () last_updated : datetime property readonly \u00b6 Provides the most recent last_updated date time stamp from the documents in this Store name : str property readonly \u00b6 Return a string representing this data source __init__ ( self , key = 'task_id' , last_updated_field = 'last_updated' , last_updated_type =< DateTimeFormat . DateTime : 'datetime' > , validator = None ) special \u00b6 Parameters: Name Type Description Default key str main key to index on 'task_id' last_updated_field str field for date/time stamping the data 'last_updated' last_updated_type DateTimeFormat the date/time format for the last_updated_field. Can be \"datetime\" or \"isoformat\" <DateTimeFormat.DateTime: 'datetime'> validator Optional[maggma.core.validator.Validator] Validator to validate documents going into the store None Source code in maggma/core/store.py def __init__ ( self , key : str = \"task_id\" , last_updated_field : str = \"last_updated\" , last_updated_type : DateTimeFormat = DateTimeFormat ( \"datetime\" ), validator : Optional [ Validator ] = None , ): \"\"\" Args: key: main key to index on last_updated_field: field for date/time stamping the data last_updated_type: the date/time format for the last_updated_field. Can be \"datetime\" or \"isoformat\" validator: Validator to validate documents going into the store \"\"\" self . key = key self . last_updated_field = last_updated_field self . last_updated_type = last_updated_type self . _lu_func = ( LU_KEY_ISOFORMAT if DateTimeFormat ( last_updated_type ) == DateTimeFormat . IsoFormat else ( identity , identity ) ) # type: Tuple[Callable, Callable] self . validator = validator self . logger = logging . getLogger ( type ( self ) . __name__ ) self . logger . addHandler ( logging . NullHandler ()) close ( self ) \u00b6 Closes any connections Source code in maggma/core/store.py @abstractmethod def close ( self ): \"\"\" Closes any connections \"\"\" connect ( self , force_reset = False ) \u00b6 Connect to the source data Parameters: Name Type Description Default force_reset bool whether to reset the connection or not False Source code in maggma/core/store.py @abstractmethod def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data Args: force_reset: whether to reset the connection or not \"\"\" count ( self , criteria = None ) \u00b6 Counts the number of documents matching the query criteria Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None Source code in maggma/core/store.py @abstractmethod def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Get all distinct values for a field Parameters: Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None Source code in maggma/core/store.py def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria or {} results = [ key for key , _ in self . groupby ( field , properties = [ field ], criteria = criteria ) ] results = [ get ( r , field ) for r in results ] return results ensure_index ( self , key , unique = False ) \u00b6 Tries to create an index and return true if it suceeded Parameters: Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns: Type Description bool bool indicating if the index exists/was created Source code in maggma/core/store.py @abstractmethod def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Simple grouping function that will group documents by keys. Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) Source code in maggma/core/store.py @abstractmethod def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" newer_in ( self , target , criteria = None , exhaustive = False ) \u00b6 Returns the keys of documents that are newer in the target Store than this Store. Parameters: Name Type Description Default target Store target Store to required criteria Optional[Dict] PyMongo filter for documents to search in None exhaustive bool triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in False Source code in maggma/core/store.py def newer_in ( self , target : \"Store\" , criteria : Optional [ Dict ] = None , exhaustive : bool = False ) -> List [ str ]: \"\"\" Returns the keys of documents that are newer in the target Store than this Store. Args: target: target Store to criteria: PyMongo filter for documents to search in exhaustive: triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in \"\"\" self . ensure_index ( self . key ) self . ensure_index ( self . last_updated_field ) if exhaustive : # Get our current last_updated dates for each key value props = { self . key : 1 , self . last_updated_field : 1 , \"_id\" : 0 } dates = { d [ self . key ]: self . _lu_func [ 0 ]( d . get ( self . last_updated_field , datetime . max ) ) for d in self . query ( properties = props ) } # Get the last_updated for the store we're comparing with props = { target . key : 1 , target . last_updated_field : 1 , \"_id\" : 0 } target_dates = { d [ target . key ]: target . _lu_func [ 0 ]( d . get ( target . last_updated_field , datetime . min ) ) for d in target . query ( criteria = criteria , properties = props ) } new_keys = set ( target_dates . keys ()) - set ( dates . keys ()) updated_keys = { key for key , date in dates . items () if target_dates . get ( key , datetime . min ) > date } return list ( new_keys | updated_keys ) else : criteria = { self . last_updated_field : { \"$gt\" : self . _lu_func [ 1 ]( self . last_updated )} } return target . distinct ( field = self . key , criteria = criteria ) query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Queries the Store for a set of documents Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Source code in maggma/core/store.py @abstractmethod def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" query_one ( self , criteria = None , properties = None , sort = None ) \u00b6 Queries the Store for a single document Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search None properties Union[Dict, List] properties to return in the document None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None Source code in maggma/core/store.py def query_one ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , ): \"\"\" Queries the Store for a single document Args: criteria: PyMongo filter for documents to search properties: properties to return in the document sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. \"\"\" return next ( self . query ( criteria = criteria , properties = properties , sort = sort ), None ) remove_docs ( self , criteria ) \u00b6 Remove docs matching the query dictionary Parameters: Name Type Description Default criteria Dict query dictionary to match required Source code in maggma/core/store.py @abstractmethod def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" update ( self , docs , key = None ) \u00b6 Update documents into the Store Parameters: Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Source code in maggma/core/store.py @abstractmethod def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" StoreError ( Exception ) \u00b6 General Store-related error Source code in maggma/core/store.py class StoreError ( Exception ): \"\"\"General Store-related error\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( self , * args , ** kwargs )","title":"Store"},{"location":"reference/core_store/#maggma.core.store.DateTimeFormat","text":"Datetime format in store document Source code in maggma/core/store.py class DateTimeFormat ( Enum ): \"\"\"Datetime format in store document\"\"\" DateTime = \"datetime\" IsoFormat = \"isoformat\"","title":"DateTimeFormat"},{"location":"reference/core_store/#maggma.core.store.Sort","text":"Enumeration for sorting order Source code in maggma/core/store.py class Sort ( Enum ): \"\"\"Enumeration for sorting order\"\"\" Ascending = 1 Descending = - 1","title":"Sort"},{"location":"reference/core_store/#maggma.core.store.Store","text":"Abstract class for a data Store Defines the interface for all data going in and out of a Builder Source code in maggma/core/store.py class Store ( MSONable , metaclass = ABCMeta ): \"\"\" Abstract class for a data Store Defines the interface for all data going in and out of a Builder \"\"\" def __init__ ( self , key : str = \"task_id\" , last_updated_field : str = \"last_updated\" , last_updated_type : DateTimeFormat = DateTimeFormat ( \"datetime\" ), validator : Optional [ Validator ] = None , ): \"\"\" Args: key: main key to index on last_updated_field: field for date/time stamping the data last_updated_type: the date/time format for the last_updated_field. Can be \"datetime\" or \"isoformat\" validator: Validator to validate documents going into the store \"\"\" self . key = key self . last_updated_field = last_updated_field self . last_updated_type = last_updated_type self . _lu_func = ( LU_KEY_ISOFORMAT if DateTimeFormat ( last_updated_type ) == DateTimeFormat . IsoFormat else ( identity , identity ) ) # type: Tuple[Callable, Callable] self . validator = validator self . logger = logging . getLogger ( type ( self ) . __name__ ) self . logger . addHandler ( logging . NullHandler ()) @abstractproperty def _collection ( self ): \"\"\" Returns a handle to the pymongo collection object \"\"\" @abstractproperty def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" @abstractmethod def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data Args: force_reset: whether to reset the connection or not \"\"\" @abstractmethod def close ( self ): \"\"\" Closes any connections \"\"\" @abstractmethod def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" @abstractmethod def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" @abstractmethod def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" @abstractmethod def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" @abstractmethod def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" @abstractmethod def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" def query_one ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , ): \"\"\" Queries the Store for a single document Args: criteria: PyMongo filter for documents to search properties: properties to return in the document sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. \"\"\" return next ( self . query ( criteria = criteria , properties = properties , sort = sort ), None ) def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria or {} results = [ key for key , _ in self . groupby ( field , properties = [ field ], criteria = criteria ) ] results = [ get ( r , field ) for r in results ] return results @property def last_updated ( self ) -> datetime : \"\"\" Provides the most recent last_updated date time stamp from the documents in this Store \"\"\" doc = next ( self . query ( properties = [ self . last_updated_field ], sort = { self . last_updated_field : - 1 }, limit = 1 , ), None , ) if doc and not has ( doc , self . last_updated_field ): raise StoreError ( f \"No field ' { self . last_updated_field } ' in store document. Please ensure Store.last_updated_field \" \"is a datetime field in your store that represents the time of \" \"last update to each document.\" ) elif not doc or get ( doc , self . last_updated_field ) is None : # Handle when collection has docs but `NoneType` last_updated_field. return datetime . min else : return self . _lu_func [ 0 ]( get ( doc , self . last_updated_field )) def newer_in ( self , target : \"Store\" , criteria : Optional [ Dict ] = None , exhaustive : bool = False ) -> List [ str ]: \"\"\" Returns the keys of documents that are newer in the target Store than this Store. Args: target: target Store to criteria: PyMongo filter for documents to search in exhaustive: triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in \"\"\" self . ensure_index ( self . key ) self . ensure_index ( self . last_updated_field ) if exhaustive : # Get our current last_updated dates for each key value props = { self . key : 1 , self . last_updated_field : 1 , \"_id\" : 0 } dates = { d [ self . key ]: self . _lu_func [ 0 ]( d . get ( self . last_updated_field , datetime . max ) ) for d in self . query ( properties = props ) } # Get the last_updated for the store we're comparing with props = { target . key : 1 , target . last_updated_field : 1 , \"_id\" : 0 } target_dates = { d [ target . key ]: target . _lu_func [ 0 ]( d . get ( target . last_updated_field , datetime . min ) ) for d in target . query ( criteria = criteria , properties = props ) } new_keys = set ( target_dates . keys ()) - set ( dates . keys ()) updated_keys = { key for key , date in dates . items () if target_dates . get ( key , datetime . min ) > date } return list ( new_keys | updated_keys ) else : criteria = { self . last_updated_field : { \"$gt\" : self . _lu_func [ 1 ]( self . last_updated )} } return target . distinct ( field = self . key , criteria = criteria ) @deprecated ( message = \"Please use Store.newer_in\" ) def lu_filter ( self , targets ): \"\"\"Creates a MongoDB filter for new documents. By \"new\", we mean documents in this Store that were last updated later than any document in targets. Args: targets (list): A list of Stores \"\"\" if isinstance ( targets , Store ): targets = [ targets ] lu_list = [ t . last_updated for t in targets ] return { self . last_updated_field : { \"$gt\" : self . _lu_func [ 1 ]( max ( lu_list ))}} @deprecated ( message = \"Use Store.newer_in\" ) def updated_keys ( self , target , criteria = None ): \"\"\" Returns keys for docs that are newer in the target store in comparison with this store when comparing the last updated field (last_updated_field) Args: target (Store): store to look for updated documents criteria (dict): mongo query to limit scope Returns: list of keys that have been updated in target store \"\"\" self . ensure_index ( self . key ) self . ensure_index ( self . last_updated_field ) return self . newer_in ( target , criteria = criteria ) def __ne__ ( self , other ): return not self == other def __getstate__ ( self ): return self . as_dict () def __setstate__ ( self , d ): d = { k : v for k , v in d . items () if not k . startswith ( \"@\" )} d = MontyDecoder () . process_decoded ( d ) self . __init__ ( ** d ) def __enter__ ( self ): self . connect () return self def __exit__ ( self , exception_type , exception_value , traceback ): self . close ()","title":"Store"},{"location":"reference/core_store/#maggma.core.store.Store.last_updated","text":"Provides the most recent last_updated date time stamp from the documents in this Store","title":"last_updated"},{"location":"reference/core_store/#maggma.core.store.Store.name","text":"Return a string representing this data source","title":"name"},{"location":"reference/core_store/#maggma.core.store.Store.__init__","text":"Parameters: Name Type Description Default key str main key to index on 'task_id' last_updated_field str field for date/time stamping the data 'last_updated' last_updated_type DateTimeFormat the date/time format for the last_updated_field. Can be \"datetime\" or \"isoformat\" <DateTimeFormat.DateTime: 'datetime'> validator Optional[maggma.core.validator.Validator] Validator to validate documents going into the store None Source code in maggma/core/store.py def __init__ ( self , key : str = \"task_id\" , last_updated_field : str = \"last_updated\" , last_updated_type : DateTimeFormat = DateTimeFormat ( \"datetime\" ), validator : Optional [ Validator ] = None , ): \"\"\" Args: key: main key to index on last_updated_field: field for date/time stamping the data last_updated_type: the date/time format for the last_updated_field. Can be \"datetime\" or \"isoformat\" validator: Validator to validate documents going into the store \"\"\" self . key = key self . last_updated_field = last_updated_field self . last_updated_type = last_updated_type self . _lu_func = ( LU_KEY_ISOFORMAT if DateTimeFormat ( last_updated_type ) == DateTimeFormat . IsoFormat else ( identity , identity ) ) # type: Tuple[Callable, Callable] self . validator = validator self . logger = logging . getLogger ( type ( self ) . __name__ ) self . logger . addHandler ( logging . NullHandler ())","title":"__init__()"},{"location":"reference/core_store/#maggma.core.store.Store.close","text":"Closes any connections Source code in maggma/core/store.py @abstractmethod def close ( self ): \"\"\" Closes any connections \"\"\"","title":"close()"},{"location":"reference/core_store/#maggma.core.store.Store.connect","text":"Connect to the source data Parameters: Name Type Description Default force_reset bool whether to reset the connection or not False Source code in maggma/core/store.py @abstractmethod def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data Args: force_reset: whether to reset the connection or not \"\"\"","title":"connect()"},{"location":"reference/core_store/#maggma.core.store.Store.count","text":"Counts the number of documents matching the query criteria Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None Source code in maggma/core/store.py @abstractmethod def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\"","title":"count()"},{"location":"reference/core_store/#maggma.core.store.Store.distinct","text":"Get all distinct values for a field Parameters: Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None Source code in maggma/core/store.py def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria or {} results = [ key for key , _ in self . groupby ( field , properties = [ field ], criteria = criteria ) ] results = [ get ( r , field ) for r in results ] return results","title":"distinct()"},{"location":"reference/core_store/#maggma.core.store.Store.ensure_index","text":"Tries to create an index and return true if it suceeded Parameters: Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns: Type Description bool bool indicating if the index exists/was created Source code in maggma/core/store.py @abstractmethod def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\"","title":"ensure_index()"},{"location":"reference/core_store/#maggma.core.store.Store.groupby","text":"Simple grouping function that will group documents by keys. Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) Source code in maggma/core/store.py @abstractmethod def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\"","title":"groupby()"},{"location":"reference/core_store/#maggma.core.store.Store.newer_in","text":"Returns the keys of documents that are newer in the target Store than this Store. Parameters: Name Type Description Default target Store target Store to required criteria Optional[Dict] PyMongo filter for documents to search in None exhaustive bool triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in False Source code in maggma/core/store.py def newer_in ( self , target : \"Store\" , criteria : Optional [ Dict ] = None , exhaustive : bool = False ) -> List [ str ]: \"\"\" Returns the keys of documents that are newer in the target Store than this Store. Args: target: target Store to criteria: PyMongo filter for documents to search in exhaustive: triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in \"\"\" self . ensure_index ( self . key ) self . ensure_index ( self . last_updated_field ) if exhaustive : # Get our current last_updated dates for each key value props = { self . key : 1 , self . last_updated_field : 1 , \"_id\" : 0 } dates = { d [ self . key ]: self . _lu_func [ 0 ]( d . get ( self . last_updated_field , datetime . max ) ) for d in self . query ( properties = props ) } # Get the last_updated for the store we're comparing with props = { target . key : 1 , target . last_updated_field : 1 , \"_id\" : 0 } target_dates = { d [ target . key ]: target . _lu_func [ 0 ]( d . get ( target . last_updated_field , datetime . min ) ) for d in target . query ( criteria = criteria , properties = props ) } new_keys = set ( target_dates . keys ()) - set ( dates . keys ()) updated_keys = { key for key , date in dates . items () if target_dates . get ( key , datetime . min ) > date } return list ( new_keys | updated_keys ) else : criteria = { self . last_updated_field : { \"$gt\" : self . _lu_func [ 1 ]( self . last_updated )} } return target . distinct ( field = self . key , criteria = criteria )","title":"newer_in()"},{"location":"reference/core_store/#maggma.core.store.Store.query","text":"Queries the Store for a set of documents Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Source code in maggma/core/store.py @abstractmethod def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\"","title":"query()"},{"location":"reference/core_store/#maggma.core.store.Store.query_one","text":"Queries the Store for a single document Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search None properties Union[Dict, List] properties to return in the document None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None Source code in maggma/core/store.py def query_one ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , ): \"\"\" Queries the Store for a single document Args: criteria: PyMongo filter for documents to search properties: properties to return in the document sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. \"\"\" return next ( self . query ( criteria = criteria , properties = properties , sort = sort ), None )","title":"query_one()"},{"location":"reference/core_store/#maggma.core.store.Store.remove_docs","text":"Remove docs matching the query dictionary Parameters: Name Type Description Default criteria Dict query dictionary to match required Source code in maggma/core/store.py @abstractmethod def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\"","title":"remove_docs()"},{"location":"reference/core_store/#maggma.core.store.Store.update","text":"Update documents into the Store Parameters: Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Source code in maggma/core/store.py @abstractmethod def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\"","title":"update()"},{"location":"reference/core_store/#maggma.core.store.StoreError","text":"General Store-related error Source code in maggma/core/store.py class StoreError ( Exception ): \"\"\"General Store-related error\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( self , * args , ** kwargs )","title":"StoreError"},{"location":"reference/core_validator/","text":"Validator class for document-level validation on Stores. Attach an instance of a Validator subclass to a Store .schema variable to enable validation on that Store. Validator ( MSONable ) \u00b6 A generic class to perform document-level validation on Stores. Attach a Validator to a Store during initialization, any all documents added to the Store will call .validate_doc() before being added. Source code in maggma/core/validator.py class Validator ( MSONable , metaclass = ABCMeta ): \"\"\" A generic class to perform document-level validation on Stores. Attach a Validator to a Store during initialization, any all documents added to the Store will call .validate_doc() before being added. \"\"\" @abstractmethod def is_valid ( self , doc : Dict ) -> bool : \"\"\" Determines if the document is valid Args: doc: document to check \"\"\" @abstractmethod def validation_errors ( self , doc : Dict ) -> List [ str ]: \"\"\" If document is not valid, provides a list of strings to display for why validation has failed Returns empty list if the document is valid Args: doc: document to check \"\"\" is_valid ( self , doc ) \u00b6 Determines if the document is valid Parameters: Name Type Description Default doc Dict document to check required Source code in maggma/core/validator.py @abstractmethod def is_valid ( self , doc : Dict ) -> bool : \"\"\" Determines if the document is valid Args: doc: document to check \"\"\" validation_errors ( self , doc ) \u00b6 If document is not valid, provides a list of strings to display for why validation has failed Returns empty list if the document is valid Parameters: Name Type Description Default doc Dict document to check required Source code in maggma/core/validator.py @abstractmethod def validation_errors ( self , doc : Dict ) -> List [ str ]: \"\"\" If document is not valid, provides a list of strings to display for why validation has failed Returns empty list if the document is valid Args: doc: document to check \"\"\"","title":"Validator"},{"location":"reference/core_validator/#maggma.core.validator.Validator","text":"A generic class to perform document-level validation on Stores. Attach a Validator to a Store during initialization, any all documents added to the Store will call .validate_doc() before being added. Source code in maggma/core/validator.py class Validator ( MSONable , metaclass = ABCMeta ): \"\"\" A generic class to perform document-level validation on Stores. Attach a Validator to a Store during initialization, any all documents added to the Store will call .validate_doc() before being added. \"\"\" @abstractmethod def is_valid ( self , doc : Dict ) -> bool : \"\"\" Determines if the document is valid Args: doc: document to check \"\"\" @abstractmethod def validation_errors ( self , doc : Dict ) -> List [ str ]: \"\"\" If document is not valid, provides a list of strings to display for why validation has failed Returns empty list if the document is valid Args: doc: document to check \"\"\"","title":"Validator"},{"location":"reference/core_validator/#maggma.core.validator.Validator.is_valid","text":"Determines if the document is valid Parameters: Name Type Description Default doc Dict document to check required Source code in maggma/core/validator.py @abstractmethod def is_valid ( self , doc : Dict ) -> bool : \"\"\" Determines if the document is valid Args: doc: document to check \"\"\"","title":"is_valid()"},{"location":"reference/core_validator/#maggma.core.validator.Validator.validation_errors","text":"If document is not valid, provides a list of strings to display for why validation has failed Returns empty list if the document is valid Parameters: Name Type Description Default doc Dict document to check required Source code in maggma/core/validator.py @abstractmethod def validation_errors ( self , doc : Dict ) -> List [ str ]: \"\"\" If document is not valid, provides a list of strings to display for why validation has failed Returns empty list if the document is valid Args: doc: document to check \"\"\"","title":"validation_errors()"},{"location":"reference/stores/","text":"Module containing various definitions of Stores. Stores are a default access pattern to data and provide various utilities JSONStore ( MemoryStore ) \u00b6 A Store for access to a single or multiple JSON files Source code in maggma/stores/mongolike.py class JSONStore ( MemoryStore ): \"\"\" A Store for access to a single or multiple JSON files \"\"\" def __init__ ( self , paths : Union [ str , List [ str ]], read_only : bool = True , ** kwargs , ): \"\"\" Args: paths: paths for json files to turn into a Store read_only: whether this JSONStore is read only. When read_only=True, the JSONStore can still apply MongoDB-like writable operations (e.g. an update) because it behaves like a MemoryStore, but it will not write those changes to the file. On the other hand, if read_only=False (i.e., it is writeable), the JSON file will be automatically updated every time a write-like operation is performed. Note that when read_only=False, JSONStore only supports a single JSON file. If the file does not exist, it will be automatically created when the JSONStore is initialized. \"\"\" paths = paths if isinstance ( paths , ( list , tuple )) else [ paths ] self . paths = paths # file_writable overrides read_only for compatibility reasons if \"file_writable\" in kwargs : file_writable = kwargs . pop ( \"file_writable\" ) warnings . warn ( \"file_writable is deprecated; use read only instead.\" , DeprecationWarning , ) self . read_only = not file_writable if self . read_only != read_only : warnings . warn ( f \"Received conflicting keyword arguments file_writable= { file_writable } \" f \" and read_only= { read_only } . Setting read_only= { file_writable } .\" , UserWarning , ) else : self . read_only = read_only self . kwargs = kwargs if not self . read_only and len ( paths ) > 1 : raise RuntimeError ( \"Cannot instantiate file-writable JSONStore with multiple JSON files.\" ) # create the .json file if it does not exist if not self . read_only and not Path ( self . paths [ 0 ]) . exists (): with zopen ( self . paths [ 0 ], \"w\" ) as f : data : List [ dict ] = [] bytesdata = orjson . dumps ( data ) f . write ( bytesdata . decode ( \"utf-8\" )) super () . __init__ ( ** kwargs ) def connect ( self , force_reset = False ): \"\"\" Loads the files into the collection in memory \"\"\" super () . connect ( force_reset = force_reset ) for path in self . paths : objects = self . read_json_file ( path ) try : self . update ( objects ) except KeyError : raise KeyError ( f \"\"\" Key field ' { self . key } ' not found in { path . name } . This could mean that this JSONStore was initially created with a different key field. The keys found in the .json file are { list ( objects [ 0 ] . keys ()) } . Try re-initializing your JSONStore using one of these as the key arguments. \"\"\" ) def read_json_file ( self , path ) -> List : \"\"\" Helper method to read the contents of a JSON file and generate a list of docs. Args: path: Path to the JSON file to be read \"\"\" with zopen ( path ) as f : data = f . read () data = data . decode () if isinstance ( data , bytes ) else data objects = orjson . loads ( data ) objects = [ objects ] if not isinstance ( objects , list ) else objects return objects def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store. For a file-writable JSONStore, the json file is updated. Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" super () . update ( docs = docs , key = key ) if not self . read_only : self . update_json_file () def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary. For a file-writable JSONStore, the json file is updated. Args: criteria: query dictionary to match \"\"\" super () . remove_docs ( criteria = criteria ) if not self . read_only : self . update_json_file () def update_json_file ( self ): \"\"\" Updates the json file when a write-like operation is performed. \"\"\" with zopen ( self . paths [ 0 ], \"w\" ) as f : data = [ d for d in self . query ()] for d in data : d . pop ( \"_id\" ) bytesdata = orjson . dumps ( data ) f . write ( bytesdata . decode ( \"utf-8\" )) def __hash__ ( self ): return hash (( * self . paths , self . last_updated_field )) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for JSONStore Args: other: other JSONStore to compare with \"\"\" if not isinstance ( other , JSONStore ): return False fields = [ \"paths\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) __eq__ ( self , other ) special \u00b6 Check equality for JSONStore Parameters: Name Type Description Default other object other JSONStore to compare with required Source code in maggma/stores/mongolike.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for JSONStore Args: other: other JSONStore to compare with \"\"\" if not isinstance ( other , JSONStore ): return False fields = [ \"paths\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) __init__ ( self , paths , read_only = True , ** kwargs ) special \u00b6 Parameters: Name Type Description Default paths Union[str, List[str]] paths for json files to turn into a Store required read_only bool whether this JSONStore is read only. When read_only=True, the JSONStore can still apply MongoDB-like writable operations (e.g. an update) because it behaves like a MemoryStore, but it will not write those changes to the file. On the other hand, if read_only=False (i.e., it is writeable), the JSON file will be automatically updated every time a write-like operation is performed. Note that when read_only = False , JSONStore only supports a single JSON file . If the file does not exist , it will be automatically created when the JSONStore is initialized . True Source code in maggma/stores/mongolike.py def __init__ ( self , paths : Union [ str , List [ str ]], read_only : bool = True , ** kwargs , ): \"\"\" Args: paths: paths for json files to turn into a Store read_only: whether this JSONStore is read only. When read_only=True, the JSONStore can still apply MongoDB-like writable operations (e.g. an update) because it behaves like a MemoryStore, but it will not write those changes to the file. On the other hand, if read_only=False (i.e., it is writeable), the JSON file will be automatically updated every time a write-like operation is performed. Note that when read_only=False, JSONStore only supports a single JSON file. If the file does not exist, it will be automatically created when the JSONStore is initialized. \"\"\" paths = paths if isinstance ( paths , ( list , tuple )) else [ paths ] self . paths = paths # file_writable overrides read_only for compatibility reasons if \"file_writable\" in kwargs : file_writable = kwargs . pop ( \"file_writable\" ) warnings . warn ( \"file_writable is deprecated; use read only instead.\" , DeprecationWarning , ) self . read_only = not file_writable if self . read_only != read_only : warnings . warn ( f \"Received conflicting keyword arguments file_writable= { file_writable } \" f \" and read_only= { read_only } . Setting read_only= { file_writable } .\" , UserWarning , ) else : self . read_only = read_only self . kwargs = kwargs if not self . read_only and len ( paths ) > 1 : raise RuntimeError ( \"Cannot instantiate file-writable JSONStore with multiple JSON files.\" ) # create the .json file if it does not exist if not self . read_only and not Path ( self . paths [ 0 ]) . exists (): with zopen ( self . paths [ 0 ], \"w\" ) as f : data : List [ dict ] = [] bytesdata = orjson . dumps ( data ) f . write ( bytesdata . decode ( \"utf-8\" )) super () . __init__ ( ** kwargs ) connect ( self , force_reset = False ) \u00b6 Loads the files into the collection in memory Source code in maggma/stores/mongolike.py def connect ( self , force_reset = False ): \"\"\" Loads the files into the collection in memory \"\"\" super () . connect ( force_reset = force_reset ) for path in self . paths : objects = self . read_json_file ( path ) try : self . update ( objects ) except KeyError : raise KeyError ( f \"\"\" Key field ' { self . key } ' not found in { path . name } . This could mean that this JSONStore was initially created with a different key field. The keys found in the .json file are { list ( objects [ 0 ] . keys ()) } . Try re-initializing your JSONStore using one of these as the key arguments. \"\"\" ) read_json_file ( self , path ) \u00b6 Helper method to read the contents of a JSON file and generate a list of docs. Parameters: Name Type Description Default path Path to the JSON file to be read required Source code in maggma/stores/mongolike.py def read_json_file ( self , path ) -> List : \"\"\" Helper method to read the contents of a JSON file and generate a list of docs. Args: path: Path to the JSON file to be read \"\"\" with zopen ( path ) as f : data = f . read () data = data . decode () if isinstance ( data , bytes ) else data objects = orjson . loads ( data ) objects = [ objects ] if not isinstance ( objects , list ) else objects return objects remove_docs ( self , criteria ) \u00b6 Remove docs matching the query dictionary. For a file-writable JSONStore, the json file is updated. Parameters: Name Type Description Default criteria Dict query dictionary to match required Source code in maggma/stores/mongolike.py def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary. For a file-writable JSONStore, the json file is updated. Args: criteria: query dictionary to match \"\"\" super () . remove_docs ( criteria = criteria ) if not self . read_only : self . update_json_file () update ( self , docs , key = None ) \u00b6 Update documents into the Store. For a file-writable JSONStore, the json file is updated. Parameters: Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Source code in maggma/stores/mongolike.py def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store. For a file-writable JSONStore, the json file is updated. Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" super () . update ( docs = docs , key = key ) if not self . read_only : self . update_json_file () update_json_file ( self ) \u00b6 Updates the json file when a write-like operation is performed. Source code in maggma/stores/mongolike.py def update_json_file ( self ): \"\"\" Updates the json file when a write-like operation is performed. \"\"\" with zopen ( self . paths [ 0 ], \"w\" ) as f : data = [ d for d in self . query ()] for d in data : d . pop ( \"_id\" ) bytesdata = orjson . dumps ( data ) f . write ( bytesdata . decode ( \"utf-8\" )) MemoryStore ( MongoStore ) \u00b6 An in-memory Store that functions similarly to a MongoStore Source code in maggma/stores/mongolike.py class MemoryStore ( MongoStore ): \"\"\" An in-memory Store that functions similarly to a MongoStore \"\"\" def __init__ ( self , collection_name : str = \"memory_db\" , ** kwargs ): \"\"\" Initializes the Memory Store Args: collection_name: name for the collection in memory \"\"\" self . collection_name = collection_name self . _coll = None self . kwargs = kwargs super ( MongoStore , self ) . __init__ ( ** kwargs ) # noqa def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if self . _coll is None or force_reset : self . _coll = mongomock . MongoClient () . db [ self . name ] def close ( self ): \"\"\"Close up all collections\"\"\" self . _coll . database . client . close () @property def name ( self ): \"\"\"Name for the store\"\"\" return f \"mem:// { self . collection_name } \" def __hash__ ( self ): \"\"\"Hash for the store\"\"\" return hash (( self . name , self . last_updated_field )) def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (key, list of elemnts) \"\"\" keys = keys if isinstance ( keys , list ) else [ keys ] if properties is None : properties = [] if isinstance ( properties , dict ): properties = list ( properties . keys ()) data = [ doc for doc in self . query ( properties = keys + properties , criteria = criteria ) if all ( has ( doc , k ) for k in keys ) ] def grouping_keys ( doc ): return tuple ( get ( doc , k ) for k in keys ) for vals , group in groupby ( sorted ( data , key = grouping_keys ), key = grouping_keys ): doc = {} # type: Dict[Any,Any] for k , v in zip ( keys , vals ): set_ ( doc , k , v ) yield doc , list ( group ) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MemoryStore other: other MemoryStore to compare with \"\"\" if not isinstance ( other , MemoryStore ): return False fields = [ \"collection_name\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) name property readonly \u00b6 Name for the store __eq__ ( self , other ) special \u00b6 Check equality for MemoryStore other: other MemoryStore to compare with Source code in maggma/stores/mongolike.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MemoryStore other: other MemoryStore to compare with \"\"\" if not isinstance ( other , MemoryStore ): return False fields = [ \"collection_name\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) __hash__ ( self ) special \u00b6 Hash for the store Source code in maggma/stores/mongolike.py def __hash__ ( self ): \"\"\"Hash for the store\"\"\" return hash (( self . name , self . last_updated_field )) __init__ ( self , collection_name = 'memory_db' , ** kwargs ) special \u00b6 Initializes the Memory Store Parameters: Name Type Description Default collection_name str name for the collection in memory 'memory_db' Source code in maggma/stores/mongolike.py def __init__ ( self , collection_name : str = \"memory_db\" , ** kwargs ): \"\"\" Initializes the Memory Store Args: collection_name: name for the collection in memory \"\"\" self . collection_name = collection_name self . _coll = None self . kwargs = kwargs super ( MongoStore , self ) . __init__ ( ** kwargs ) # noqa close ( self ) \u00b6 Close up all collections Source code in maggma/stores/mongolike.py def close ( self ): \"\"\"Close up all collections\"\"\" self . _coll . database . client . close () connect ( self , force_reset = False ) \u00b6 Connect to the source data Source code in maggma/stores/mongolike.py def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if self . _coll is None or force_reset : self . _coll = mongomock . MongoClient () . db [ self . name ] groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Simple grouping function that will group documents by keys. Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (key, list of elemnts) Source code in maggma/stores/mongolike.py def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (key, list of elemnts) \"\"\" keys = keys if isinstance ( keys , list ) else [ keys ] if properties is None : properties = [] if isinstance ( properties , dict ): properties = list ( properties . keys ()) data = [ doc for doc in self . query ( properties = keys + properties , criteria = criteria ) if all ( has ( doc , k ) for k in keys ) ] def grouping_keys ( doc ): return tuple ( get ( doc , k ) for k in keys ) for vals , group in groupby ( sorted ( data , key = grouping_keys ), key = grouping_keys ): doc = {} # type: Dict[Any,Any] for k , v in zip ( keys , vals ): set_ ( doc , k , v ) yield doc , list ( group ) MongoStore ( Store ) \u00b6 A Store that connects to a Mongo collection Source code in maggma/stores/mongolike.py class MongoStore ( Store ): \"\"\" A Store that connects to a Mongo collection \"\"\" def __init__ ( self , database : str , collection_name : str , host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , ssh_tunnel : Optional [ SSHTunnel ] = None , safe_update : bool = False , auth_source : Optional [ str ] = None , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Args: database: The database name collection_name: The collection name host: Hostname for the database port: TCP port to connect to username: Username for the collection password: Password to connect with safe_update: fail gracefully on DocumentTooLarge errors on update auth_source: The database to authenticate on. Defaults to the database name. \"\"\" self . database = database self . collection_name = collection_name self . host = host self . port = port self . username = username self . password = password self . ssh_tunnel = ssh_tunnel self . safe_update = safe_update self . _coll = None # type: Any self . kwargs = kwargs if auth_source is None : auth_source = self . database self . auth_source = auth_source self . mongoclient_kwargs = mongoclient_kwargs or {} super () . __init__ ( ** kwargs ) @property def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" return f \"mongo:// { self . host } / { self . database } / { self . collection_name } \" def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if self . _coll is None or force_reset : if self . ssh_tunnel is None : host = self . host port = self . port else : self . ssh_tunnel . start () host , port = self . ssh_tunnel . local_address conn = ( MongoClient ( host = host , port = port , username = self . username , password = self . password , authSource = self . auth_source , ** self . mongoclient_kwargs , ) if self . username != \"\" else MongoClient ( host , port , ** self . mongoclient_kwargs ) ) db = conn [ self . database ] self . _coll = db [ self . collection_name ] def __hash__ ( self ) -> int : \"\"\"Hash for MongoStore\"\"\" return hash (( self . database , self . collection_name , self . last_updated_field )) @classmethod def from_db_file ( cls , filename : str , ** kwargs ): \"\"\" Convenience method to construct MongoStore from db_file from old QueryEngine format \"\"\" kwargs = loadfn ( filename ) if \"collection\" in kwargs : kwargs [ \"collection_name\" ] = kwargs . pop ( \"collection\" ) # Get rid of aliases from traditional query engine db docs kwargs . pop ( \"aliases\" , None ) return cls ( ** kwargs ) @classmethod def from_launchpad_file ( cls , lp_file , collection_name , ** kwargs ): \"\"\" Convenience method to construct MongoStore from a launchpad file Note: A launchpad file is a special formatted yaml file used in fireworks Returns: \"\"\" with open ( lp_file , \"r\" ) as f : lp_creds = yaml . load ( f , Loader = yaml . FullLoader ) db_creds = lp_creds . copy () db_creds [ \"database\" ] = db_creds [ \"name\" ] for key in list ( db_creds . keys ()): if key not in [ \"database\" , \"host\" , \"port\" , \"username\" , \"password\" ]: db_creds . pop ( key ) db_creds [ \"collection_name\" ] = collection_name return cls ( ** db_creds , ** kwargs ) def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria or {} try : distinct_vals = self . _collection . distinct ( field , criteria ) except ( OperationFailure , DocumentTooLarge ): distinct_vals = [ d [ \"_id\" ] for d in self . _collection . aggregate ( [{ \"$match\" : criteria }, { \"$group\" : { \"_id\" : f \"$ { field } \" }}] ) ] if all ( isinstance ( d , list ) for d in filter ( None , distinct_vals )): # type: ignore distinct_vals = list ( chain . from_iterable ( filter ( None , distinct_vals ))) return distinct_vals if distinct_vals is not None else [] def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (key, list of docs) \"\"\" pipeline = [] if isinstance ( keys , str ): keys = [ keys ] if properties is None : properties = [] if isinstance ( properties , dict ): properties = list ( properties . keys ()) if criteria is not None : pipeline . append ({ \"$match\" : criteria }) if len ( properties ) > 0 : pipeline . append ({ \"$project\" : { p : 1 for p in properties + keys }}) alpha = \"abcdefghijklmnopqrstuvwxyz\" group_id = { letter : f \"$ { key } \" for letter , key in zip ( alpha , keys )} pipeline . append ({ \"$group\" : { \"_id\" : group_id , \"docs\" : { \"$push\" : \"$$ROOT\" }}}) for d in self . _collection . aggregate ( pipeline , allowDiskUse = True ): id_doc = {} # type: Dict[str,Any] for letter , key in group_id . items (): if has ( d [ \"_id\" ], letter ): set_ ( id_doc , key [ 1 :], d [ \"_id\" ][ letter ]) yield ( id_doc , d [ \"docs\" ]) @classmethod def from_collection ( cls , collection ): \"\"\" Generates a MongoStore from a pymongo collection object This is not a fully safe operation as it gives dummy information to the MongoStore As a result, this will not serialize and can not reset its connection Args: collection: the PyMongo collection to create a MongoStore around \"\"\" # TODO: How do we make this safer? coll_name = collection . name db_name = collection . database . name store = cls ( db_name , coll_name ) store . _coll = collection return store @property def _collection ( self ): \"\"\"Property referring to underlying pymongo collection\"\"\" if self . _coll is None : raise StoreError ( \"Must connect Mongo-like store before attemping to use it\" ) return self . _coll def count ( self , criteria : Optional [ Dict ] = None , hint : Optional [ Dict [ str , Union [ Sort , int ]]] = None , ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in hint: Dictionary of indexes to use as hints for query optimizer. Keys are field names and values are 1 for ascending or -1 for descending. \"\"\" criteria = criteria if criteria else {} hint_list = ( [ ( k , Sort ( v ) . value ) if isinstance ( v , int ) else ( k , v . value ) for k , v in hint . items () ] if hint else None ) if hint_list is not None : # pragma: no cover return self . _collection . count_documents ( filter = criteria , hint = hint_list ) return self . _collection . count_documents ( filter = criteria ) def query ( # type: ignore self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , hint : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. hint: Dictionary of indexes to use as hints for query optimizer. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( properties , list ): properties = { p : 1 for p in properties } sort_list = ( [ ( k , Sort ( v ) . value ) if isinstance ( v , int ) else ( k , v . value ) for k , v in sort . items () ] if sort else None ) hint_list = ( [ ( k , Sort ( v ) . value ) if isinstance ( v , int ) else ( k , v . value ) for k , v in hint . items () ] if hint else None ) for d in self . _collection . find ( filter = criteria , projection = properties , skip = skip , limit = limit , sort = sort_list , hint = hint_list , ): yield d def ensure_index ( self , key : str , unique : Optional [ bool ] = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" if confirm_field_index ( self . _collection , key ): return True else : try : self . _collection . create_index ( key , unique = unique , background = True ) return True except Exception : return False def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" requests = [] if not isinstance ( docs , list ): docs = [ docs ] for d in docs : d = jsanitize ( d , allow_bson = True ) # document-level validation is optional validates = True if self . validator : validates = self . validator . is_valid ( d ) if not validates : if self . validator . strict : raise ValueError ( self . validator . validation_errors ( d )) else : self . logger . error ( self . validator . validation_errors ( d )) if validates : key = key or self . key if isinstance ( key , list ): search_doc = { k : d [ k ] for k in key } else : search_doc = { key : d [ key ]} requests . append ( ReplaceOne ( search_doc , d , upsert = True )) if len ( requests ) > 0 : try : self . _collection . bulk_write ( requests , ordered = False ) except ( OperationFailure , DocumentTooLarge ) as e : if self . safe_update : for req in requests : req . _filter try : self . _collection . bulk_write ([ req ], ordered = False ) except ( OperationFailure , DocumentTooLarge ): self . logger . error ( f \"Could not upload document for { req . _filter } as it was too large for Mongo\" ) else : raise e def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" self . _collection . delete_many ( filter = criteria ) def close ( self ): \"\"\"Close up all collections\"\"\" self . _collection . database . client . close () self . _coll = None if self . ssh_tunnel is not None : self . ssh_tunnel . stop () def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MongoStore other: other mongostore to compare with \"\"\" if not isinstance ( other , MongoStore ): return False fields = [ \"database\" , \"collection_name\" , \"host\" , \"port\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) name : str property readonly \u00b6 Return a string representing this data source __eq__ ( self , other ) special \u00b6 Check equality for MongoStore other: other mongostore to compare with Source code in maggma/stores/mongolike.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MongoStore other: other mongostore to compare with \"\"\" if not isinstance ( other , MongoStore ): return False fields = [ \"database\" , \"collection_name\" , \"host\" , \"port\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) __hash__ ( self ) special \u00b6 Hash for MongoStore Source code in maggma/stores/mongolike.py def __hash__ ( self ) -> int : \"\"\"Hash for MongoStore\"\"\" return hash (( self . database , self . collection_name , self . last_updated_field )) __init__ ( self , database , collection_name , host = 'localhost' , port = 27017 , username = '' , password = '' , ssh_tunnel = None , safe_update = False , auth_source = None , mongoclient_kwargs = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default database str The database name required collection_name str The collection name required host str Hostname for the database 'localhost' port int TCP port to connect to 27017 username str Username for the collection '' password str Password to connect with '' safe_update bool fail gracefully on DocumentTooLarge errors on update False auth_source Optional[str] The database to authenticate on. Defaults to the database name. None Source code in maggma/stores/mongolike.py def __init__ ( self , database : str , collection_name : str , host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , ssh_tunnel : Optional [ SSHTunnel ] = None , safe_update : bool = False , auth_source : Optional [ str ] = None , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Args: database: The database name collection_name: The collection name host: Hostname for the database port: TCP port to connect to username: Username for the collection password: Password to connect with safe_update: fail gracefully on DocumentTooLarge errors on update auth_source: The database to authenticate on. Defaults to the database name. \"\"\" self . database = database self . collection_name = collection_name self . host = host self . port = port self . username = username self . password = password self . ssh_tunnel = ssh_tunnel self . safe_update = safe_update self . _coll = None # type: Any self . kwargs = kwargs if auth_source is None : auth_source = self . database self . auth_source = auth_source self . mongoclient_kwargs = mongoclient_kwargs or {} super () . __init__ ( ** kwargs ) close ( self ) \u00b6 Close up all collections Source code in maggma/stores/mongolike.py def close ( self ): \"\"\"Close up all collections\"\"\" self . _collection . database . client . close () self . _coll = None if self . ssh_tunnel is not None : self . ssh_tunnel . stop () connect ( self , force_reset = False ) \u00b6 Connect to the source data Source code in maggma/stores/mongolike.py def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if self . _coll is None or force_reset : if self . ssh_tunnel is None : host = self . host port = self . port else : self . ssh_tunnel . start () host , port = self . ssh_tunnel . local_address conn = ( MongoClient ( host = host , port = port , username = self . username , password = self . password , authSource = self . auth_source , ** self . mongoclient_kwargs , ) if self . username != \"\" else MongoClient ( host , port , ** self . mongoclient_kwargs ) ) db = conn [ self . database ] self . _coll = db [ self . collection_name ] count ( self , criteria = None , hint = None ) \u00b6 Counts the number of documents matching the query criteria Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None hint Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of indexes to use as hints for query optimizer. Keys are field names and values are 1 for ascending or -1 for descending. None Source code in maggma/stores/mongolike.py def count ( self , criteria : Optional [ Dict ] = None , hint : Optional [ Dict [ str , Union [ Sort , int ]]] = None , ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in hint: Dictionary of indexes to use as hints for query optimizer. Keys are field names and values are 1 for ascending or -1 for descending. \"\"\" criteria = criteria if criteria else {} hint_list = ( [ ( k , Sort ( v ) . value ) if isinstance ( v , int ) else ( k , v . value ) for k , v in hint . items () ] if hint else None ) if hint_list is not None : # pragma: no cover return self . _collection . count_documents ( filter = criteria , hint = hint_list ) return self . _collection . count_documents ( filter = criteria ) distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Get all distinct values for a field Parameters: Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None Source code in maggma/stores/mongolike.py def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria or {} try : distinct_vals = self . _collection . distinct ( field , criteria ) except ( OperationFailure , DocumentTooLarge ): distinct_vals = [ d [ \"_id\" ] for d in self . _collection . aggregate ( [{ \"$match\" : criteria }, { \"$group\" : { \"_id\" : f \"$ { field } \" }}] ) ] if all ( isinstance ( d , list ) for d in filter ( None , distinct_vals )): # type: ignore distinct_vals = list ( chain . from_iterable ( filter ( None , distinct_vals ))) return distinct_vals if distinct_vals is not None else [] ensure_index ( self , key , unique = False ) \u00b6 Tries to create an index and return true if it suceeded Parameters: Name Type Description Default key str single key to index required unique Optional[bool] Whether or not this index contains only unique keys False Returns: Type Description bool bool indicating if the index exists/was created Source code in maggma/stores/mongolike.py def ensure_index ( self , key : str , unique : Optional [ bool ] = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" if confirm_field_index ( self . _collection , key ): return True else : try : self . _collection . create_index ( key , unique = unique , background = True ) return True except Exception : return False from_collection ( collection ) classmethod \u00b6 Generates a MongoStore from a pymongo collection object This is not a fully safe operation as it gives dummy information to the MongoStore As a result, this will not serialize and can not reset its connection Parameters: Name Type Description Default collection the PyMongo collection to create a MongoStore around required Source code in maggma/stores/mongolike.py @classmethod def from_collection ( cls , collection ): \"\"\" Generates a MongoStore from a pymongo collection object This is not a fully safe operation as it gives dummy information to the MongoStore As a result, this will not serialize and can not reset its connection Args: collection: the PyMongo collection to create a MongoStore around \"\"\" # TODO: How do we make this safer? coll_name = collection . name db_name = collection . database . name store = cls ( db_name , coll_name ) store . _coll = collection return store from_db_file ( filename , ** kwargs ) classmethod \u00b6 Convenience method to construct MongoStore from db_file from old QueryEngine format Source code in maggma/stores/mongolike.py @classmethod def from_db_file ( cls , filename : str , ** kwargs ): \"\"\" Convenience method to construct MongoStore from db_file from old QueryEngine format \"\"\" kwargs = loadfn ( filename ) if \"collection\" in kwargs : kwargs [ \"collection_name\" ] = kwargs . pop ( \"collection\" ) # Get rid of aliases from traditional query engine db docs kwargs . pop ( \"aliases\" , None ) return cls ( ** kwargs ) from_launchpad_file ( lp_file , collection_name , ** kwargs ) classmethod \u00b6 Convenience method to construct MongoStore from a launchpad file Note: A launchpad file is a special formatted yaml file used in fireworks Source code in maggma/stores/mongolike.py @classmethod def from_launchpad_file ( cls , lp_file , collection_name , ** kwargs ): \"\"\" Convenience method to construct MongoStore from a launchpad file Note: A launchpad file is a special formatted yaml file used in fireworks Returns: \"\"\" with open ( lp_file , \"r\" ) as f : lp_creds = yaml . load ( f , Loader = yaml . FullLoader ) db_creds = lp_creds . copy () db_creds [ \"database\" ] = db_creds [ \"name\" ] for key in list ( db_creds . keys ()): if key not in [ \"database\" , \"host\" , \"port\" , \"username\" , \"password\" ]: db_creds . pop ( key ) db_creds [ \"collection_name\" ] = collection_name return cls ( ** db_creds , ** kwargs ) groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Simple grouping function that will group documents by keys. Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (key, list of docs) Source code in maggma/stores/mongolike.py def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (key, list of docs) \"\"\" pipeline = [] if isinstance ( keys , str ): keys = [ keys ] if properties is None : properties = [] if isinstance ( properties , dict ): properties = list ( properties . keys ()) if criteria is not None : pipeline . append ({ \"$match\" : criteria }) if len ( properties ) > 0 : pipeline . append ({ \"$project\" : { p : 1 for p in properties + keys }}) alpha = \"abcdefghijklmnopqrstuvwxyz\" group_id = { letter : f \"$ { key } \" for letter , key in zip ( alpha , keys )} pipeline . append ({ \"$group\" : { \"_id\" : group_id , \"docs\" : { \"$push\" : \"$$ROOT\" }}}) for d in self . _collection . aggregate ( pipeline , allowDiskUse = True ): id_doc = {} # type: Dict[str,Any] for letter , key in group_id . items (): if has ( d [ \"_id\" ], letter ): set_ ( id_doc , key [ 1 :], d [ \"_id\" ][ letter ]) yield ( id_doc , d [ \"docs\" ]) query ( self , criteria = None , properties = None , sort = None , hint = None , skip = 0 , limit = 0 ) \u00b6 Queries the Store for a set of documents Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None hint Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of indexes to use as hints for query optimizer. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Source code in maggma/stores/mongolike.py def query ( # type: ignore self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , hint : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. hint: Dictionary of indexes to use as hints for query optimizer. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( properties , list ): properties = { p : 1 for p in properties } sort_list = ( [ ( k , Sort ( v ) . value ) if isinstance ( v , int ) else ( k , v . value ) for k , v in sort . items () ] if sort else None ) hint_list = ( [ ( k , Sort ( v ) . value ) if isinstance ( v , int ) else ( k , v . value ) for k , v in hint . items () ] if hint else None ) for d in self . _collection . find ( filter = criteria , projection = properties , skip = skip , limit = limit , sort = sort_list , hint = hint_list , ): yield d remove_docs ( self , criteria ) \u00b6 Remove docs matching the query dictionary Parameters: Name Type Description Default criteria Dict query dictionary to match required Source code in maggma/stores/mongolike.py def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" self . _collection . delete_many ( filter = criteria ) update ( self , docs , key = None ) \u00b6 Update documents into the Store Parameters: Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Source code in maggma/stores/mongolike.py def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" requests = [] if not isinstance ( docs , list ): docs = [ docs ] for d in docs : d = jsanitize ( d , allow_bson = True ) # document-level validation is optional validates = True if self . validator : validates = self . validator . is_valid ( d ) if not validates : if self . validator . strict : raise ValueError ( self . validator . validation_errors ( d )) else : self . logger . error ( self . validator . validation_errors ( d )) if validates : key = key or self . key if isinstance ( key , list ): search_doc = { k : d [ k ] for k in key } else : search_doc = { key : d [ key ]} requests . append ( ReplaceOne ( search_doc , d , upsert = True )) if len ( requests ) > 0 : try : self . _collection . bulk_write ( requests , ordered = False ) except ( OperationFailure , DocumentTooLarge ) as e : if self . safe_update : for req in requests : req . _filter try : self . _collection . bulk_write ([ req ], ordered = False ) except ( OperationFailure , DocumentTooLarge ): self . logger . error ( f \"Could not upload document for { req . _filter } as it was too large for Mongo\" ) else : raise e MongoURIStore ( MongoStore ) \u00b6 A Store that connects to a Mongo collection via a URI This is expected to be a special mongodb+srv:// URIs that include client parameters via TXT records Source code in maggma/stores/mongolike.py class MongoURIStore ( MongoStore ): \"\"\" A Store that connects to a Mongo collection via a URI This is expected to be a special mongodb+srv:// URIs that include client parameters via TXT records \"\"\" def __init__ ( self , uri : str , collection_name : str , database : str = None , ssh_tunnel : Optional [ SSHTunnel ] = None , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Args: uri: MongoDB+SRV URI database: database to connect to collection_name: The collection name \"\"\" self . uri = uri self . ssh_tunnel = ssh_tunnel self . mongoclient_kwargs = mongoclient_kwargs or {} # parse the dbname from the uri if database is None : d_uri = uri_parser . parse_uri ( uri ) if d_uri [ \"database\" ] is None : raise ConfigurationError ( \"If database name is not supplied, a database must be set in the uri\" ) self . database = d_uri [ \"database\" ] else : self . database = database self . collection_name = collection_name self . kwargs = kwargs self . _coll = None super ( MongoStore , self ) . __init__ ( ** kwargs ) # lgtm @property def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" # TODO: This is not very safe since it exposes the username/password info return self . uri def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if self . _coll is None or force_reset : # pragma: no cover conn = MongoClient ( self . uri , ** self . mongoclient_kwargs ) db = conn [ self . database ] self . _coll = db [ self . collection_name ] name : str property readonly \u00b6 Return a string representing this data source __init__ ( self , uri , collection_name , database = None , ssh_tunnel = None , mongoclient_kwargs = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default uri str MongoDB+SRV URI required database str database to connect to None collection_name str The collection name required Source code in maggma/stores/mongolike.py def __init__ ( self , uri : str , collection_name : str , database : str = None , ssh_tunnel : Optional [ SSHTunnel ] = None , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Args: uri: MongoDB+SRV URI database: database to connect to collection_name: The collection name \"\"\" self . uri = uri self . ssh_tunnel = ssh_tunnel self . mongoclient_kwargs = mongoclient_kwargs or {} # parse the dbname from the uri if database is None : d_uri = uri_parser . parse_uri ( uri ) if d_uri [ \"database\" ] is None : raise ConfigurationError ( \"If database name is not supplied, a database must be set in the uri\" ) self . database = d_uri [ \"database\" ] else : self . database = database self . collection_name = collection_name self . kwargs = kwargs self . _coll = None super ( MongoStore , self ) . __init__ ( ** kwargs ) # lgtm connect ( self , force_reset = False ) \u00b6 Connect to the source data Source code in maggma/stores/mongolike.py def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if self . _coll is None or force_reset : # pragma: no cover conn = MongoClient ( self . uri , ** self . mongoclient_kwargs ) db = conn [ self . database ] self . _coll = db [ self . collection_name ] MontyStore ( MemoryStore ) \u00b6 A MongoDB compatible store that uses on disk files for storage. This is handled under the hood using MontyDB. A number of on-disk storage options are available but MontyDB provides a mongo style interface for all options. The options include: sqlite: Uses an sqlite database to store documents. lightning: Uses Lightning Memory-Mapped Database (LMDB) for storage. This can provide fast read and write times but requires lmdb to be installed (in most cases this can be achieved using pip install lmdb ). flatfile: Uses a system of flat json files. This is not recommended as multiple simultaneous connections to the store will not work correctly. See the MontyDB repository for more information: https://github.com/davidlatwe/montydb Source code in maggma/stores/mongolike.py class MontyStore ( MemoryStore ): \"\"\" A MongoDB compatible store that uses on disk files for storage. This is handled under the hood using MontyDB. A number of on-disk storage options are available but MontyDB provides a mongo style interface for all options. The options include: - sqlite: Uses an sqlite database to store documents. - lightning: Uses Lightning Memory-Mapped Database (LMDB) for storage. This can provide fast read and write times but requires lmdb to be installed (in most cases this can be achieved using ``pip install lmdb``). - flatfile: Uses a system of flat json files. This is not recommended as multiple simultaneous connections to the store will not work correctly. See the MontyDB repository for more information: https://github.com/davidlatwe/montydb \"\"\" def __init__ ( self , collection_name , database_path : str = None , database_name : str = \"db\" , storage : str = \"sqlite\" , storage_kwargs : Optional [ dict ] = None , client_kwargs : Optional [ dict ] = None , ** kwargs , ): \"\"\" Initializes the Monty Store. Args: collection_name: Name for the collection. database_path: Path to on-disk database files. If None, the current working directory will be used. database_name: The database name. storage: The storage type. Options include \"sqlite\", \"lightning\", \"flatfile\". storage_kwargs: Keyword arguments passed to ``montydb.set_storage``. client_kwargs: Keyword arguments passed to the ``montydb.MontyClient`` constructor. **kwargs: Additional keyword arguments passed to the Store constructor. \"\"\" if database_path is None : database_path = str ( Path . cwd ()) self . database_path = database_path self . database_name = database_name self . collection_name = collection_name self . _coll = None self . ssh_tunnel = None # This is to fix issues with the tunnel on close self . kwargs = kwargs self . storage = storage self . storage_kwargs = storage_kwargs or { \"use_bson\" : True , \"monty_version\" : \"4.0\" , } self . client_kwargs = client_kwargs or {} super ( MongoStore , self ) . __init__ ( ** kwargs ) # noqa def connect ( self , force_reset : bool = False ): \"\"\" Connect to the database store. Args: force_reset: Force connection reset. \"\"\" from montydb import set_storage , MontyClient set_storage ( self . database_path , storage = self . storage , ** self . storage_kwargs ) client = MontyClient ( self . database_path , ** self . client_kwargs ) if not self . _coll or force_reset : self . _coll = client [ \"db\" ][ self . collection_name ] @property def name ( self ) -> str : \"\"\"Return a string representing this data source.\"\"\" return f \"monty:// { self . database_path } / { self . database } / { self . collection_name } \" def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store. Args: docs: The document or list of documents to update. key: Field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used. \"\"\" if not isinstance ( docs , list ): docs = [ docs ] for d in docs : d = jsanitize ( d , allow_bson = True ) # document-level validation is optional validates = True if self . validator : validates = self . validator . is_valid ( d ) if not validates : if self . validator . strict : raise ValueError ( self . validator . validation_errors ( d )) else : self . logger . error ( self . validator . validation_errors ( d )) if validates : key = key or self . key if isinstance ( key , list ): search_doc = { k : d [ k ] for k in key } else : search_doc = { key : d [ key ]} self . _collection . replace_one ( search_doc , d , upsert = True ) name : str property readonly \u00b6 Return a string representing this data source. __init__ ( self , collection_name , database_path = None , database_name = 'db' , storage = 'sqlite' , storage_kwargs = None , client_kwargs = None , ** kwargs ) special \u00b6 Initializes the Monty Store. Parameters: Name Type Description Default collection_name Name for the collection. required database_path str Path to on-disk database files. If None, the current working directory will be used. None database_name str The database name. 'db' storage str The storage type. Options include \"sqlite\", \"lightning\", \"flatfile\". 'sqlite' storage_kwargs Optional[dict] Keyword arguments passed to montydb.set_storage . None client_kwargs Optional[dict] Keyword arguments passed to the montydb.MontyClient constructor. None **kwargs Additional keyword arguments passed to the Store constructor. {} Source code in maggma/stores/mongolike.py def __init__ ( self , collection_name , database_path : str = None , database_name : str = \"db\" , storage : str = \"sqlite\" , storage_kwargs : Optional [ dict ] = None , client_kwargs : Optional [ dict ] = None , ** kwargs , ): \"\"\" Initializes the Monty Store. Args: collection_name: Name for the collection. database_path: Path to on-disk database files. If None, the current working directory will be used. database_name: The database name. storage: The storage type. Options include \"sqlite\", \"lightning\", \"flatfile\". storage_kwargs: Keyword arguments passed to ``montydb.set_storage``. client_kwargs: Keyword arguments passed to the ``montydb.MontyClient`` constructor. **kwargs: Additional keyword arguments passed to the Store constructor. \"\"\" if database_path is None : database_path = str ( Path . cwd ()) self . database_path = database_path self . database_name = database_name self . collection_name = collection_name self . _coll = None self . ssh_tunnel = None # This is to fix issues with the tunnel on close self . kwargs = kwargs self . storage = storage self . storage_kwargs = storage_kwargs or { \"use_bson\" : True , \"monty_version\" : \"4.0\" , } self . client_kwargs = client_kwargs or {} super ( MongoStore , self ) . __init__ ( ** kwargs ) # noqa connect ( self , force_reset = False ) \u00b6 Connect to the database store. Parameters: Name Type Description Default force_reset bool Force connection reset. False Source code in maggma/stores/mongolike.py def connect ( self , force_reset : bool = False ): \"\"\" Connect to the database store. Args: force_reset: Force connection reset. \"\"\" from montydb import set_storage , MontyClient set_storage ( self . database_path , storage = self . storage , ** self . storage_kwargs ) client = MontyClient ( self . database_path , ** self . client_kwargs ) if not self . _coll or force_reset : self . _coll = client [ \"db\" ][ self . collection_name ] update ( self , docs , key = None ) \u00b6 Update documents into the Store. Parameters: Name Type Description Default docs Union[List[Dict], Dict] The document or list of documents to update. required key Union[List, str] Field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used. None Source code in maggma/stores/mongolike.py def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store. Args: docs: The document or list of documents to update. key: Field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used. \"\"\" if not isinstance ( docs , list ): docs = [ docs ] for d in docs : d = jsanitize ( d , allow_bson = True ) # document-level validation is optional validates = True if self . validator : validates = self . validator . is_valid ( d ) if not validates : if self . validator . strict : raise ValueError ( self . validator . validation_errors ( d )) else : self . logger . error ( self . validator . validation_errors ( d )) if validates : key = key or self . key if isinstance ( key , list ): search_doc = { k : d [ k ] for k in key } else : search_doc = { key : d [ key ]} self . _collection . replace_one ( search_doc , d , upsert = True ) SSHTunnel ( MSONable ) \u00b6 Source code in maggma/stores/mongolike.py class SSHTunnel ( MSONable ): __TUNNELS : Dict [ str , SSHTunnelForwarder ] = {} def __init__ ( self , tunnel_server_address : str , remote_server_address : str , username : Optional [ str ] = None , password : Optional [ str ] = None , private_key : Optional [ str ] = None , ** kwargs , ): \"\"\" Args: tunnel_server_address: string address with port for the SSH tunnel server remote_server_address: string address with port for the server to connect to username: optional username for the ssh tunnel server password: optional password for the ssh tunnel server; If a private_key is supplied this password is assumed to be the private key password private_key: ssh private key to authenticate to the tunnel server kwargs: any extra args passed to the SSHTunnelForwarder \"\"\" self . tunnel_server_address = tunnel_server_address self . remote_server_address = remote_server_address self . username = username self . password = password self . private_key = private_key self . kwargs = kwargs if remote_server_address in SSHTunnel . __TUNNELS : self . tunnel = SSHTunnel . __TUNNELS [ remote_server_address ] else : open_port = _find_free_port ( \"127.0.0.1\" ) local_bind_address = ( \"127.0.0.1\" , open_port ) ssh_address , ssh_port = tunnel_server_address . split ( \":\" ) ssh_port = int ( ssh_port ) # type: ignore remote_bind_address , remote_bind_port = remote_server_address . split ( \":\" ) remote_bind_port = int ( remote_bind_port ) # type: ignore if private_key is not None : ssh_password = None ssh_private_key_password = password else : ssh_password = password ssh_private_key_password = None self . tunnel = SSHTunnelForwarder ( ssh_address_or_host = ( ssh_address , ssh_port ), local_bind_address = local_bind_address , remote_bind_address = ( remote_bind_address , remote_bind_port ), ssh_username = username , ssh_password = ssh_password , ssh_private_key_password = ssh_private_key_password , ssh_pkey = private_key , ** kwargs , ) def start ( self ): if not self . tunnel . is_active : self . tunnel . start () def stop ( self ): if self . tunnel . tunnel_is_up : self . tunnel . stop () @property def local_address ( self ) -> Tuple [ str , int ]: return self . tunnel . local_bind_address __init__ ( self , tunnel_server_address , remote_server_address , username = None , password = None , private_key = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default tunnel_server_address str string address with port for the SSH tunnel server required remote_server_address str string address with port for the server to connect to required username Optional[str] optional username for the ssh tunnel server None password Optional[str] optional password for the ssh tunnel server; If a private_key is supplied this password is assumed to be the private key password None private_key Optional[str] ssh private key to authenticate to the tunnel server None kwargs any extra args passed to the SSHTunnelForwarder {} Source code in maggma/stores/mongolike.py def __init__ ( self , tunnel_server_address : str , remote_server_address : str , username : Optional [ str ] = None , password : Optional [ str ] = None , private_key : Optional [ str ] = None , ** kwargs , ): \"\"\" Args: tunnel_server_address: string address with port for the SSH tunnel server remote_server_address: string address with port for the server to connect to username: optional username for the ssh tunnel server password: optional password for the ssh tunnel server; If a private_key is supplied this password is assumed to be the private key password private_key: ssh private key to authenticate to the tunnel server kwargs: any extra args passed to the SSHTunnelForwarder \"\"\" self . tunnel_server_address = tunnel_server_address self . remote_server_address = remote_server_address self . username = username self . password = password self . private_key = private_key self . kwargs = kwargs if remote_server_address in SSHTunnel . __TUNNELS : self . tunnel = SSHTunnel . __TUNNELS [ remote_server_address ] else : open_port = _find_free_port ( \"127.0.0.1\" ) local_bind_address = ( \"127.0.0.1\" , open_port ) ssh_address , ssh_port = tunnel_server_address . split ( \":\" ) ssh_port = int ( ssh_port ) # type: ignore remote_bind_address , remote_bind_port = remote_server_address . split ( \":\" ) remote_bind_port = int ( remote_bind_port ) # type: ignore if private_key is not None : ssh_password = None ssh_private_key_password = password else : ssh_password = password ssh_private_key_password = None self . tunnel = SSHTunnelForwarder ( ssh_address_or_host = ( ssh_address , ssh_port ), local_bind_address = local_bind_address , remote_bind_address = ( remote_bind_address , remote_bind_port ), ssh_username = username , ssh_password = ssh_password , ssh_private_key_password = ssh_private_key_password , ssh_pkey = private_key , ** kwargs , ) Module containing various definitions of Stores. Stores are a default access pattern to data and provide various utillities GridFSStore ( Store ) \u00b6 A Store for GrdiFS backend. Provides a common access method consistent with other stores Source code in maggma/stores/gridfs.py class GridFSStore ( Store ): \"\"\" A Store for GrdiFS backend. Provides a common access method consistent with other stores \"\"\" def __init__ ( self , database : str , collection_name : str , host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , compression : bool = False , ensure_metadata : bool = False , searchable_fields : List [ str ] = None , auth_source : Optional [ str ] = None , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Initializes a GrdiFS Store for binary data Args: database: database name collection_name: The name of the collection. This is the string portion before the GridFS extensions host: hostname for the database port: port to connect to username: username to connect as password: password to authenticate as compression: compress the data as it goes into GridFS ensure_metadata: ensure returned documents have the metadata fields searchable_fields: fields to keep in the index store auth_source: The database to authenticate on. Defaults to the database name. \"\"\" self . database = database self . collection_name = collection_name self . host = host self . port = port self . username = username self . password = password self . _coll = None # type: Any self . compression = compression self . ensure_metadata = ensure_metadata self . searchable_fields = [] if searchable_fields is None else searchable_fields self . kwargs = kwargs if auth_source is None : auth_source = self . database self . auth_source = auth_source self . mongoclient_kwargs = mongoclient_kwargs or {} if \"key\" not in kwargs : kwargs [ \"key\" ] = \"_id\" super () . __init__ ( ** kwargs ) @classmethod def from_launchpad_file ( cls , lp_file , collection_name , ** kwargs ): \"\"\" Convenience method to construct a GridFSStore from a launchpad file Note: A launchpad file is a special formatted yaml file used in fireworks Returns: \"\"\" with open ( lp_file , \"r\" ) as f : lp_creds = yaml . load ( f , Loader = yaml . FullLoader ) db_creds = lp_creds . copy () db_creds [ \"database\" ] = db_creds [ \"name\" ] for key in list ( db_creds . keys ()): if key not in [ \"database\" , \"host\" , \"port\" , \"username\" , \"password\" ]: db_creds . pop ( key ) db_creds [ \"collection_name\" ] = collection_name return cls ( ** db_creds , ** kwargs ) @property def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" return f \"gridfs:// { self . host } / { self . database } / { self . collection_name } \" def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" conn = ( MongoClient ( host = self . host , port = self . port , username = self . username , password = self . password , authSource = self . auth_source , ** self . mongoclient_kwargs , ) if self . username != \"\" else MongoClient ( self . host , self . port , ** self . mongoclient_kwargs ) ) if not self . _coll or force_reset : db = conn [ self . database ] self . _coll = gridfs . GridFS ( db , self . collection_name ) self . _files_collection = db [ \" {} .files\" . format ( self . collection_name )] self . _files_store = MongoStore . from_collection ( self . _files_collection ) self . _files_store . last_updated_field = f \"metadata. { self . last_updated_field } \" self . _files_store . key = self . key self . _chunks_collection = db [ \" {} .chunks\" . format ( self . collection_name )] @property def _collection ( self ): \"\"\"Property referring to underlying pymongo collection\"\"\" if self . _coll is None : raise StoreError ( \"Must connect Mongo-like store before attemping to use it\" ) return self . _coll @property def last_updated ( self ) -> datetime : \"\"\" Provides the most recent last_updated date time stamp from the documents in this Store \"\"\" return self . _files_store . last_updated @classmethod def transform_criteria ( cls , criteria : Dict ) -> Dict : \"\"\" Allow client to not need to prepend 'metadata.' to query fields. Args: criteria: Query criteria \"\"\" new_criteria = dict () for field in criteria : if field not in files_collection_fields and not field . startswith ( \"metadata.\" ): new_criteria [ \"metadata.\" + field ] = copy . copy ( criteria [ field ]) else : new_criteria [ field ] = copy . copy ( criteria [ field ]) return new_criteria def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) return self . _files_store . count ( criteria ) def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the GridFS Store for a set of documents. Will check to see if data can be returned from files store first. If the data from the gridfs is not a json serialized string a dict will be returned with the data in the \"data\" key plus the self.key and self.last_updated_field. Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) elif criteria is not None : raise ValueError ( \"Criteria must be a dictionary or None\" ) prop_keys = set () if isinstance ( properties , dict ): prop_keys = set ( properties . keys ()) elif isinstance ( properties , list ): prop_keys = set ( properties ) for doc in self . _files_store . query ( criteria = criteria , sort = sort , limit = limit , skip = skip ): if properties is not None and prop_keys . issubset ( set ( doc . keys ())): yield { p : doc [ p ] for p in properties if p in doc } else : metadata = doc . get ( \"metadata\" , {}) data = self . _collection . find_one ( filter = { \"_id\" : doc [ \"_id\" ]}, skip = skip , limit = limit , sort = sort , ) . read () if metadata . get ( \"compression\" , \"\" ) == \"zlib\" : data = zlib . decompress ( data ) . decode ( \"UTF-8\" ) try : data = json . loads ( data ) except Exception : if not isinstance ( data , dict ): data = { \"data\" : data , self . key : doc . get ( self . key ), self . last_updated_field : doc . get ( self . last_updated_field ), } if self . ensure_metadata and isinstance ( data , dict ): data . update ( metadata ) yield data def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field. This function only operates on the metadata in the files collection Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = ( self . transform_criteria ( criteria ) if isinstance ( criteria , dict ) else criteria ) field = ( f \"metadata. { field } \" if field not in files_collection_fields and not field . startswith ( \"metadata.\" ) else field ) return self . _files_store . distinct ( field = field , criteria = criteria ) def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Will only work if the keys are included in the files collection for GridFS Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" criteria = ( self . transform_criteria ( criteria ) if isinstance ( criteria , dict ) else criteria ) keys = [ keys ] if not isinstance ( keys , list ) else keys keys = [ f \"metadata. { k } \" if k not in files_collection_fields and not k . startswith ( \"metadata.\" ) else k for k in keys ] for group , ids in self . _files_store . groupby ( keys , criteria = criteria , properties = [ f \"metadata. { self . key } \" ] ): ids = [ get ( doc , f \"metadata. { self . key } \" ) for doc in ids if has ( doc , f \"metadata. { self . key } \" ) ] group = { k . replace ( \"metadata.\" , \"\" ): get ( group , k ) for k in keys if has ( group , k ) } yield group , list ( self . query ( criteria = { self . key : { \"$in\" : ids }})) def ensure_index ( self , key : str , unique : Optional [ bool ] = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Currently operators on the GridFS files collection Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" # Transform key for gridfs first if key not in files_collection_fields : files_col_key = \"metadata. {} \" . format ( key ) return self . _files_store . ensure_index ( files_col_key , unique = unique ) else : return self . _files_store . ensure_index ( key , unique = unique ) def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None , additional_metadata : Union [ str , List [ str ], None ] = None , ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used additional_metadata: field(s) to include in the gridfs metadata \"\"\" if not isinstance ( docs , list ): docs = [ docs ] if isinstance ( key , str ): key = [ key ] elif not key : key = [ self . key ] key = list ( set ( key ) - set ( files_collection_fields )) if additional_metadata is None : additional_metadata = [] elif isinstance ( additional_metadata , str ): additional_metadata = [ additional_metadata ] else : additional_metadata = list ( additional_metadata ) for d in docs : search_doc = { k : d [ k ] for k in key } metadata = { k : get ( d , k ) for k in [ self . last_updated_field ] + additional_metadata + self . searchable_fields if has ( d , k ) } metadata . update ( search_doc ) data = json . dumps ( jsanitize ( d )) . encode ( \"UTF-8\" ) if self . compression : data = zlib . compress ( data ) metadata [ \"compression\" ] = \"zlib\" self . _collection . put ( data , metadata = metadata ) search_doc = self . transform_criteria ( search_doc ) # Cleans up old gridfs entries for fdoc in ( self . _files_collection . find ( search_doc , [ \"_id\" ]) . sort ( \"uploadDate\" , - 1 ) . skip ( 1 ) ): self . _collection . delete ( fdoc [ \"_id\" ]) def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) ids = [ cursor . _id for cursor in self . _collection . find ( criteria )] for _id in ids : self . _collection . delete ( _id ) def close ( self ): self . _collection . database . client . close () def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for GridFSStore other: other GridFSStore to compare with \"\"\" if not isinstance ( other , GridFSStore ): return False fields = [ \"database\" , \"collection_name\" , \"host\" , \"port\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) last_updated : datetime property readonly \u00b6 Provides the most recent last_updated date time stamp from the documents in this Store name : str property readonly \u00b6 Return a string representing this data source __eq__ ( self , other ) special \u00b6 Check equality for GridFSStore other: other GridFSStore to compare with Source code in maggma/stores/gridfs.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for GridFSStore other: other GridFSStore to compare with \"\"\" if not isinstance ( other , GridFSStore ): return False fields = [ \"database\" , \"collection_name\" , \"host\" , \"port\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) __init__ ( self , database , collection_name , host = 'localhost' , port = 27017 , username = '' , password = '' , compression = False , ensure_metadata = False , searchable_fields = None , auth_source = None , mongoclient_kwargs = None , ** kwargs ) special \u00b6 Initializes a GrdiFS Store for binary data Parameters: Name Type Description Default database str database name required collection_name str The name of the collection. This is the string portion before the GridFS extensions required host str hostname for the database 'localhost' port int port to connect to 27017 username str username to connect as '' password str password to authenticate as '' compression bool compress the data as it goes into GridFS False ensure_metadata bool ensure returned documents have the metadata fields False searchable_fields List[str] fields to keep in the index store None auth_source Optional[str] The database to authenticate on. Defaults to the database name. None Source code in maggma/stores/gridfs.py def __init__ ( self , database : str , collection_name : str , host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , compression : bool = False , ensure_metadata : bool = False , searchable_fields : List [ str ] = None , auth_source : Optional [ str ] = None , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Initializes a GrdiFS Store for binary data Args: database: database name collection_name: The name of the collection. This is the string portion before the GridFS extensions host: hostname for the database port: port to connect to username: username to connect as password: password to authenticate as compression: compress the data as it goes into GridFS ensure_metadata: ensure returned documents have the metadata fields searchable_fields: fields to keep in the index store auth_source: The database to authenticate on. Defaults to the database name. \"\"\" self . database = database self . collection_name = collection_name self . host = host self . port = port self . username = username self . password = password self . _coll = None # type: Any self . compression = compression self . ensure_metadata = ensure_metadata self . searchable_fields = [] if searchable_fields is None else searchable_fields self . kwargs = kwargs if auth_source is None : auth_source = self . database self . auth_source = auth_source self . mongoclient_kwargs = mongoclient_kwargs or {} if \"key\" not in kwargs : kwargs [ \"key\" ] = \"_id\" super () . __init__ ( ** kwargs ) close ( self ) \u00b6 Closes any connections Source code in maggma/stores/gridfs.py def close ( self ): self . _collection . database . client . close () connect ( self , force_reset = False ) \u00b6 Connect to the source data Source code in maggma/stores/gridfs.py def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" conn = ( MongoClient ( host = self . host , port = self . port , username = self . username , password = self . password , authSource = self . auth_source , ** self . mongoclient_kwargs , ) if self . username != \"\" else MongoClient ( self . host , self . port , ** self . mongoclient_kwargs ) ) if not self . _coll or force_reset : db = conn [ self . database ] self . _coll = gridfs . GridFS ( db , self . collection_name ) self . _files_collection = db [ \" {} .files\" . format ( self . collection_name )] self . _files_store = MongoStore . from_collection ( self . _files_collection ) self . _files_store . last_updated_field = f \"metadata. { self . last_updated_field } \" self . _files_store . key = self . key self . _chunks_collection = db [ \" {} .chunks\" . format ( self . collection_name )] count ( self , criteria = None ) \u00b6 Counts the number of documents matching the query criteria Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None Source code in maggma/stores/gridfs.py def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) return self . _files_store . count ( criteria ) distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Get all distinct values for a field. This function only operates on the metadata in the files collection Parameters: Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None Source code in maggma/stores/gridfs.py def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field. This function only operates on the metadata in the files collection Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = ( self . transform_criteria ( criteria ) if isinstance ( criteria , dict ) else criteria ) field = ( f \"metadata. { field } \" if field not in files_collection_fields and not field . startswith ( \"metadata.\" ) else field ) return self . _files_store . distinct ( field = field , criteria = criteria ) ensure_index ( self , key , unique = False ) \u00b6 Tries to create an index and return true if it suceeded Currently operators on the GridFS files collection Parameters: Name Type Description Default key str single key to index required unique Optional[bool] Whether or not this index contains only unique keys False Returns: Type Description bool bool indicating if the index exists/was created Source code in maggma/stores/gridfs.py def ensure_index ( self , key : str , unique : Optional [ bool ] = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Currently operators on the GridFS files collection Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" # Transform key for gridfs first if key not in files_collection_fields : files_col_key = \"metadata. {} \" . format ( key ) return self . _files_store . ensure_index ( files_col_key , unique = unique ) else : return self . _files_store . ensure_index ( key , unique = unique ) from_launchpad_file ( lp_file , collection_name , ** kwargs ) classmethod \u00b6 Convenience method to construct a GridFSStore from a launchpad file Note: A launchpad file is a special formatted yaml file used in fireworks Source code in maggma/stores/gridfs.py @classmethod def from_launchpad_file ( cls , lp_file , collection_name , ** kwargs ): \"\"\" Convenience method to construct a GridFSStore from a launchpad file Note: A launchpad file is a special formatted yaml file used in fireworks Returns: \"\"\" with open ( lp_file , \"r\" ) as f : lp_creds = yaml . load ( f , Loader = yaml . FullLoader ) db_creds = lp_creds . copy () db_creds [ \"database\" ] = db_creds [ \"name\" ] for key in list ( db_creds . keys ()): if key not in [ \"database\" , \"host\" , \"port\" , \"username\" , \"password\" ]: db_creds . pop ( key ) db_creds [ \"collection_name\" ] = collection_name return cls ( ** db_creds , ** kwargs ) groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Simple grouping function that will group documents by keys. Will only work if the keys are included in the files collection for GridFS Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) Source code in maggma/stores/gridfs.py def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Will only work if the keys are included in the files collection for GridFS Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" criteria = ( self . transform_criteria ( criteria ) if isinstance ( criteria , dict ) else criteria ) keys = [ keys ] if not isinstance ( keys , list ) else keys keys = [ f \"metadata. { k } \" if k not in files_collection_fields and not k . startswith ( \"metadata.\" ) else k for k in keys ] for group , ids in self . _files_store . groupby ( keys , criteria = criteria , properties = [ f \"metadata. { self . key } \" ] ): ids = [ get ( doc , f \"metadata. { self . key } \" ) for doc in ids if has ( doc , f \"metadata. { self . key } \" ) ] group = { k . replace ( \"metadata.\" , \"\" ): get ( group , k ) for k in keys if has ( group , k ) } yield group , list ( self . query ( criteria = { self . key : { \"$in\" : ids }})) query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Queries the GridFS Store for a set of documents. Will check to see if data can be returned from files store first. If the data from the gridfs is not a json serialized string a dict will be returned with the data in the \"data\" key plus the self.key and self.last_updated_field. Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Source code in maggma/stores/gridfs.py def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the GridFS Store for a set of documents. Will check to see if data can be returned from files store first. If the data from the gridfs is not a json serialized string a dict will be returned with the data in the \"data\" key plus the self.key and self.last_updated_field. Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) elif criteria is not None : raise ValueError ( \"Criteria must be a dictionary or None\" ) prop_keys = set () if isinstance ( properties , dict ): prop_keys = set ( properties . keys ()) elif isinstance ( properties , list ): prop_keys = set ( properties ) for doc in self . _files_store . query ( criteria = criteria , sort = sort , limit = limit , skip = skip ): if properties is not None and prop_keys . issubset ( set ( doc . keys ())): yield { p : doc [ p ] for p in properties if p in doc } else : metadata = doc . get ( \"metadata\" , {}) data = self . _collection . find_one ( filter = { \"_id\" : doc [ \"_id\" ]}, skip = skip , limit = limit , sort = sort , ) . read () if metadata . get ( \"compression\" , \"\" ) == \"zlib\" : data = zlib . decompress ( data ) . decode ( \"UTF-8\" ) try : data = json . loads ( data ) except Exception : if not isinstance ( data , dict ): data = { \"data\" : data , self . key : doc . get ( self . key ), self . last_updated_field : doc . get ( self . last_updated_field ), } if self . ensure_metadata and isinstance ( data , dict ): data . update ( metadata ) yield data remove_docs ( self , criteria ) \u00b6 Remove docs matching the query dictionary Parameters: Name Type Description Default criteria Dict query dictionary to match required Source code in maggma/stores/gridfs.py def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) ids = [ cursor . _id for cursor in self . _collection . find ( criteria )] for _id in ids : self . _collection . delete ( _id ) transform_criteria ( criteria ) classmethod \u00b6 Allow client to not need to prepend 'metadata.' to query fields. Parameters: Name Type Description Default criteria Dict Query criteria required Source code in maggma/stores/gridfs.py @classmethod def transform_criteria ( cls , criteria : Dict ) -> Dict : \"\"\" Allow client to not need to prepend 'metadata.' to query fields. Args: criteria: Query criteria \"\"\" new_criteria = dict () for field in criteria : if field not in files_collection_fields and not field . startswith ( \"metadata.\" ): new_criteria [ \"metadata.\" + field ] = copy . copy ( criteria [ field ]) else : new_criteria [ field ] = copy . copy ( criteria [ field ]) return new_criteria update ( self , docs , key = None , additional_metadata = None ) \u00b6 Update documents into the Store Parameters: Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None additional_metadata Union[str, List[str]] field(s) to include in the gridfs metadata None Source code in maggma/stores/gridfs.py def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None , additional_metadata : Union [ str , List [ str ], None ] = None , ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used additional_metadata: field(s) to include in the gridfs metadata \"\"\" if not isinstance ( docs , list ): docs = [ docs ] if isinstance ( key , str ): key = [ key ] elif not key : key = [ self . key ] key = list ( set ( key ) - set ( files_collection_fields )) if additional_metadata is None : additional_metadata = [] elif isinstance ( additional_metadata , str ): additional_metadata = [ additional_metadata ] else : additional_metadata = list ( additional_metadata ) for d in docs : search_doc = { k : d [ k ] for k in key } metadata = { k : get ( d , k ) for k in [ self . last_updated_field ] + additional_metadata + self . searchable_fields if has ( d , k ) } metadata . update ( search_doc ) data = json . dumps ( jsanitize ( d )) . encode ( \"UTF-8\" ) if self . compression : data = zlib . compress ( data ) metadata [ \"compression\" ] = \"zlib\" self . _collection . put ( data , metadata = metadata ) search_doc = self . transform_criteria ( search_doc ) # Cleans up old gridfs entries for fdoc in ( self . _files_collection . find ( search_doc , [ \"_id\" ]) . sort ( \"uploadDate\" , - 1 ) . skip ( 1 ) ): self . _collection . delete ( fdoc [ \"_id\" ]) GridFSURIStore ( GridFSStore ) \u00b6 A Store for GridFS backend, with connection via a mongo URI string. This is expected to be a special mongodb+srv:// URIs that include client parameters via TXT records Source code in maggma/stores/gridfs.py class GridFSURIStore ( GridFSStore ): \"\"\" A Store for GridFS backend, with connection via a mongo URI string. This is expected to be a special mongodb+srv:// URIs that include client parameters via TXT records \"\"\" def __init__ ( self , uri : str , collection_name : str , database : str = None , compression : bool = False , ensure_metadata : bool = False , searchable_fields : List [ str ] = None , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Initializes a GrdiFS Store for binary data Args: uri: MongoDB+SRV URI database: database to connect to collection_name: The collection name compression: compress the data as it goes into GridFS ensure_metadata: ensure returned documents have the metadata fields searchable_fields: fields to keep in the index store \"\"\" self . uri = uri # parse the dbname from the uri if database is None : d_uri = uri_parser . parse_uri ( uri ) if d_uri [ \"database\" ] is None : raise ConfigurationError ( \"If database name is not supplied, a database must be set in the uri\" ) self . database = d_uri [ \"database\" ] else : self . database = database self . collection_name = collection_name self . _coll = None # type: Any self . compression = compression self . ensure_metadata = ensure_metadata self . searchable_fields = [] if searchable_fields is None else searchable_fields self . kwargs = kwargs self . mongoclient_kwargs = mongoclient_kwargs or {} if \"key\" not in kwargs : kwargs [ \"key\" ] = \"_id\" super ( GridFSStore , self ) . __init__ ( ** kwargs ) # lgtm def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if not self . _coll or force_reset : # pragma: no cover conn = MongoClient ( self . uri , ** self . mongoclient_kwargs ) db = conn [ self . database ] self . _coll = gridfs . GridFS ( db , self . collection_name ) self . _files_collection = db [ \" {} .files\" . format ( self . collection_name )] self . _files_store = MongoStore . from_collection ( self . _files_collection ) self . _files_store . last_updated_field = f \"metadata. { self . last_updated_field } \" self . _files_store . key = self . key self . _chunks_collection = db [ \" {} .chunks\" . format ( self . collection_name )] __init__ ( self , uri , collection_name , database = None , compression = False , ensure_metadata = False , searchable_fields = None , mongoclient_kwargs = None , ** kwargs ) special \u00b6 Initializes a GrdiFS Store for binary data Parameters: Name Type Description Default uri str MongoDB+SRV URI required database str database to connect to None collection_name str The collection name required compression bool compress the data as it goes into GridFS False ensure_metadata bool ensure returned documents have the metadata fields False searchable_fields List[str] fields to keep in the index store None Source code in maggma/stores/gridfs.py def __init__ ( self , uri : str , collection_name : str , database : str = None , compression : bool = False , ensure_metadata : bool = False , searchable_fields : List [ str ] = None , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Initializes a GrdiFS Store for binary data Args: uri: MongoDB+SRV URI database: database to connect to collection_name: The collection name compression: compress the data as it goes into GridFS ensure_metadata: ensure returned documents have the metadata fields searchable_fields: fields to keep in the index store \"\"\" self . uri = uri # parse the dbname from the uri if database is None : d_uri = uri_parser . parse_uri ( uri ) if d_uri [ \"database\" ] is None : raise ConfigurationError ( \"If database name is not supplied, a database must be set in the uri\" ) self . database = d_uri [ \"database\" ] else : self . database = database self . collection_name = collection_name self . _coll = None # type: Any self . compression = compression self . ensure_metadata = ensure_metadata self . searchable_fields = [] if searchable_fields is None else searchable_fields self . kwargs = kwargs self . mongoclient_kwargs = mongoclient_kwargs or {} if \"key\" not in kwargs : kwargs [ \"key\" ] = \"_id\" super ( GridFSStore , self ) . __init__ ( ** kwargs ) # lgtm connect ( self , force_reset = False ) \u00b6 Connect to the source data Source code in maggma/stores/gridfs.py def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if not self . _coll or force_reset : # pragma: no cover conn = MongoClient ( self . uri , ** self . mongoclient_kwargs ) db = conn [ self . database ] self . _coll = gridfs . GridFS ( db , self . collection_name ) self . _files_collection = db [ \" {} .files\" . format ( self . collection_name )] self . _files_store = MongoStore . from_collection ( self . _files_collection ) self . _files_store . last_updated_field = f \"metadata. { self . last_updated_field } \" self . _files_store . key = self . key self . _chunks_collection = db [ \" {} .chunks\" . format ( self . collection_name )] Advanced Stores for connecting to AWS data S3Store ( Store ) \u00b6 GridFS like storage using Amazon S3 and a regular store for indexing Assumes Amazon AWS key and secret key are set in environment or default config file Source code in maggma/stores/aws.py class S3Store ( Store ): \"\"\" GridFS like storage using Amazon S3 and a regular store for indexing Assumes Amazon AWS key and secret key are set in environment or default config file \"\"\" def __init__ ( self , index : Store , bucket : str , s3_profile : Optional [ Union [ str , dict ]] = None , compress : bool = False , endpoint_url : str = None , sub_dir : str = None , s3_workers : int = 1 , s3_resource_kwargs : Optional [ dict ] = None , key : str = \"fs_id\" , store_hash : bool = True , unpack_data : bool = True , searchable_fields : Optional [ List [ str ]] = None , ** kwargs , ): \"\"\" Initializes an S3 Store Args: index: a store to use to index the S3 Bucket bucket: name of the bucket s3_profile: name of aws profile containing credentials for role. Alternatively you can pass in a dictionary with the full credentials: aws_access_key_id (string) -- AWS access key ID aws_secret_access_key (string) -- AWS secret access key aws_session_token (string) -- AWS temporary session token region_name (string) -- Default region when creating new connections compress: compress files inserted into the store endpoint_url: endpoint_url to allow interface to minio service sub_dir: (optional) subdirectory of the s3 bucket to store the data s3_workers: number of concurrent S3 puts to run store_hash: store the sha1 hash right before insertion to the database. unpack_data: whether to decompress and unpack byte data when querying from the bucket. searchable_fields: fields to keep in the index store \"\"\" if boto3 is None : raise RuntimeError ( \"boto3 and botocore are required for S3Store\" ) self . index = index self . bucket = bucket self . s3_profile = s3_profile self . compress = compress self . endpoint_url = endpoint_url self . sub_dir = sub_dir . strip ( \"/\" ) + \"/\" if sub_dir else \"\" self . s3 = None # type: Any self . s3_bucket = None # type: Any self . s3_workers = s3_workers self . s3_resource_kwargs = ( s3_resource_kwargs if s3_resource_kwargs is not None else {} ) self . unpack_data = unpack_data self . searchable_fields = ( searchable_fields if searchable_fields is not None else [] ) self . store_hash = store_hash # Force the key to be the same as the index assert isinstance ( index . key , str ), \"Since we are using the key as a file name in S3, they key must be a string\" if key != index . key : warnings . warn ( f 'The desired S3Store key \" { key } \" does not match the index key \" { index . key } ,\"' \"the index key will be used\" , UserWarning , ) kwargs [ \"key\" ] = str ( index . key ) self . _thread_local = threading . local () super ( S3Store , self ) . __init__ ( ** kwargs ) @property def name ( self ) -> str : \"\"\" Returns: a string representing this data source \"\"\" return f \"s3:// { self . bucket } \" def connect ( self , * args , ** kwargs ): # lgtm[py/conflicting-attributes] \"\"\" Connect to the source data \"\"\" session = self . _get_session () resource = session . resource ( \"s3\" , endpoint_url = self . endpoint_url , ** self . s3_resource_kwargs ) if not self . s3 : self . s3 = resource try : self . s3 . meta . client . head_bucket ( Bucket = self . bucket ) except ClientError : raise RuntimeError ( \"Bucket not present on AWS: {} \" . format ( self . bucket )) self . s3_bucket = resource . Bucket ( self . bucket ) self . index . connect ( * args , ** kwargs ) def close ( self ): \"\"\" Closes any connections \"\"\" self . index . close () self . s3 = None self . s3_bucket = None @property def _collection ( self ): \"\"\" Returns: a handle to the pymongo collection object Important: Not guaranteed to exist in the future \"\"\" # For now returns the index collection since that is what we would \"search\" on return self . index . _collection def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" return self . index . count ( criteria ) def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" prop_keys = set () if isinstance ( properties , dict ): prop_keys = set ( properties . keys ()) elif isinstance ( properties , list ): prop_keys = set ( properties ) for doc in self . index . query ( criteria = criteria , sort = sort , limit = limit , skip = skip ): if properties is not None and prop_keys . issubset ( set ( doc . keys ())): yield { p : doc [ p ] for p in properties if p in doc } else : try : # TODO: THis is ugly and unsafe, do some real checking before pulling data data = ( self . s3_bucket . Object ( self . sub_dir + str ( doc [ self . key ])) . get ()[ \"Body\" ] . read () ) except botocore . exceptions . ClientError as e : # If a client error is thrown, then check that it was a 404 error. # If it was a 404 error, then the object does not exist. error_code = int ( e . response [ \"Error\" ][ \"Code\" ]) if error_code == 404 : self . logger . error ( \"Could not find S3 object {} \" . format ( doc [ self . key ]) ) break else : raise e if self . unpack_data : data = self . _unpack ( data = data , compressed = doc . get ( \"compression\" , \"\" ) == \"zlib\" ) if self . last_updated_field in doc : data [ self . last_updated_field ] = doc [ self . last_updated_field ] yield data @staticmethod def _unpack ( data : bytes , compressed : bool ): if compressed : data = zlib . decompress ( data ) # requires msgpack-python to be installed to fix string encoding problem # https://github.com/msgpack/msgpack/issues/121 # During recursion # msgpack.unpackb goes as deep as possible during reconstruction # MontyDecoder().process_decode only goes until it finds a from_dict # as such, we cannot just use msgpack.unpackb(data, object_hook=monty_object_hook, raw=False) # Should just return the unpacked object then let the user run process_decoded unpacked_data = msgpack . unpackb ( data , raw = False ) return unpacked_data def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" # Index is a store so it should have its own distinct function return self . index . distinct ( field , criteria = criteria ) def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" return self . index . groupby ( keys = keys , criteria = criteria , properties = properties , sort = sort , skip = skip , limit = limit , ) def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" return self . index . ensure_index ( key , unique = unique ) def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None , additional_metadata : Union [ str , List [ str ], None ] = None , ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used additional_metadata: field(s) to include in the s3 store's metadata \"\"\" if not isinstance ( docs , list ): docs = [ docs ] if isinstance ( key , str ): key = [ key ] elif not key : key = [ self . key ] if additional_metadata is None : additional_metadata = [] elif isinstance ( additional_metadata , str ): additional_metadata = [ additional_metadata ] else : additional_metadata = list ( additional_metadata ) with ThreadPoolExecutor ( max_workers = self . s3_workers ) as pool : fs = { pool . submit ( self . write_doc_to_s3 , doc = itr_doc , search_keys = key + additional_metadata + self . searchable_fields , ) for itr_doc in docs } fs , _ = wait ( fs ) search_docs = [ sdoc . result () for sdoc in fs ] # Use store's update to remove key clashes self . index . update ( search_docs , key = self . key ) def _get_session ( self ): if not hasattr ( self . _thread_local , \"s3_bucket\" ): if isinstance ( self . s3_profile , dict ): return Session ( ** self . s3_profile ) else : return Session ( profile_name = self . s3_profile ) def _get_bucket ( self ): \"\"\" If on the main thread return the bucket created above, else create a new bucket on each thread \"\"\" if threading . current_thread () . name == \"MainThread\" : return self . s3_bucket if not hasattr ( self . _thread_local , \"s3_bucket\" ): session = self . _get_session () resource = session . resource ( \"s3\" , endpoint_url = self . endpoint_url ) self . _thread_local . s3_bucket = resource . Bucket ( self . bucket ) return self . _thread_local . s3_bucket def write_doc_to_s3 ( self , doc : Dict , search_keys : List [ str ]): \"\"\" Write the data to s3 and return the metadata to be inserted into the index db Args: doc: the document search_keys: list of keys to pull from the docs and be inserted into the index db \"\"\" s3_bucket = self . _get_bucket () search_doc = { k : doc [ k ] for k in search_keys } search_doc [ self . key ] = doc [ self . key ] # Ensure key is in metadata if self . sub_dir != \"\" : search_doc [ \"sub_dir\" ] = self . sub_dir # Remove MongoDB _id from search if \"_id\" in search_doc : del search_doc [ \"_id\" ] # to make hashing more meaningful, make sure last updated field is removed lu_info = doc . pop ( self . last_updated_field , None ) data = msgpack . packb ( doc , default = monty_default ) if self . compress : # Compress with zlib if chosen search_doc [ \"compression\" ] = \"zlib\" data = zlib . compress ( data ) if self . last_updated_field in doc : # need this conversion for aws metadata insert search_doc [ self . last_updated_field ] = str ( to_isoformat_ceil_ms ( doc [ self . last_updated_field ]) ) # keep a record of original keys, in case these are important for the individual researcher # it is not expected that this information will be used except in disaster recovery s3_to_mongo_keys = { k : self . _sanitize_key ( k ) for k in search_doc . keys ()} s3_to_mongo_keys [ \"s3-to-mongo-keys\" ] = \"s3-to-mongo-keys\" # inception # encode dictionary since values have to be strings search_doc [ \"s3-to-mongo-keys\" ] = dumps ( s3_to_mongo_keys ) s3_bucket . put_object ( Key = self . sub_dir + str ( doc [ self . key ]), Body = data , Metadata = { s3_to_mongo_keys [ k ]: str ( v ) for k , v in search_doc . items ()}, ) if lu_info is not None : search_doc [ self . last_updated_field ] = lu_info if self . store_hash : hasher = sha1 () hasher . update ( data ) obj_hash = hasher . hexdigest () search_doc [ \"obj_hash\" ] = obj_hash return search_doc @staticmethod def _sanitize_key ( key ): \"\"\" Sanitize keys to store in S3/MinIO metadata. \"\"\" # Any underscores are encoded as double dashes in metadata, since keys with # underscores may be result in the corresponding HTTP header being stripped # by certain server configurations (e.g. default nginx), leading to: # `botocore.exceptions.ClientError: An error occurred (AccessDenied) when # calling the PutObject operation: There were headers present in the request # which were not signed` # Metadata stored in the MongoDB index (self.index) is stored unchanged. # Additionally, MinIO requires lowercase keys return str ( key ) . replace ( \"_\" , \"-\" ) . lower () def remove_docs ( self , criteria : Dict , remove_s3_object : bool = False ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match remove_s3_object: whether to remove the actual S3 Object or not \"\"\" if not remove_s3_object : self . index . remove_docs ( criteria = criteria ) else : to_remove = self . index . distinct ( self . key , criteria = criteria ) self . index . remove_docs ( criteria = criteria ) # Can remove up to 1000 items at a time via boto to_remove_chunks = list ( grouper ( to_remove , n = 1000 )) for chunk_to_remove in to_remove_chunks : objlist = [{ \"Key\" : f \" { self . sub_dir }{ obj } \" } for obj in chunk_to_remove ] self . s3_bucket . delete_objects ( Delete = { \"Objects\" : objlist }) @property def last_updated ( self ): return self . index . last_updated def newer_in ( self , target : Store , criteria : Optional [ Dict ] = None , exhaustive : bool = False ) -> List [ str ]: \"\"\" Returns the keys of documents that are newer in the target Store than this Store. Args: target: target Store criteria: PyMongo filter for documents to search in exhaustive: triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in \"\"\" if hasattr ( target , \"index\" ): return self . index . newer_in ( target = target . index , criteria = criteria , exhaustive = exhaustive ) else : return self . index . newer_in ( target = target , criteria = criteria , exhaustive = exhaustive ) def __hash__ ( self ): return hash (( self . index . __hash__ , self . bucket )) def rebuild_index_from_s3_data ( self , ** kwargs ): \"\"\" Rebuilds the index Store from the data in S3 Relies on the index document being stores as the metadata for the file This can help recover lost databases \"\"\" bucket = self . s3_bucket objects = bucket . objects . filter ( Prefix = self . sub_dir ) for obj in objects : key_ = self . sub_dir + obj . key data = self . s3_bucket . Object ( key_ ) . get ()[ \"Body\" ] . read () if self . compress : data = zlib . decompress ( data ) unpacked_data = msgpack . unpackb ( data , raw = False ) self . update ( unpacked_data , ** kwargs ) def rebuild_metadata_from_index ( self , index_query : dict = None ): \"\"\" Read data from the index store and populate the metadata of the S3 bucket Force all of the keys to be lower case to be Minio compatible Args: index_query: query on the index store \"\"\" qq = {} if index_query is None else index_query for index_doc in self . index . query ( qq ): key_ = self . sub_dir + index_doc [ self . key ] s3_object = self . s3_bucket . Object ( key_ ) new_meta = { self . _sanitize_key ( k ): v for k , v in s3_object . metadata . items ()} for k , v in index_doc . items (): new_meta [ str ( k ) . lower ()] = v new_meta . pop ( \"_id\" ) if self . last_updated_field in new_meta : new_meta [ self . last_updated_field ] = str ( to_isoformat_ceil_ms ( new_meta [ self . last_updated_field ]) ) # s3_object.metadata.update(new_meta) s3_object . copy_from ( CopySource = { \"Bucket\" : self . s3_bucket . name , \"Key\" : key_ }, Metadata = new_meta , MetadataDirective = \"REPLACE\" , ) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for S3Store other: other S3Store to compare with \"\"\" if not isinstance ( other , S3Store ): return False fields = [ \"index\" , \"bucket\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) last_updated property readonly \u00b6 Provides the most recent last_updated date time stamp from the documents in this Store name : str property readonly \u00b6 Returns: Type Description str a string representing this data source __eq__ ( self , other ) special \u00b6 Check equality for S3Store other: other S3Store to compare with Source code in maggma/stores/aws.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for S3Store other: other S3Store to compare with \"\"\" if not isinstance ( other , S3Store ): return False fields = [ \"index\" , \"bucket\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) __init__ ( self , index , bucket , s3_profile = None , compress = False , endpoint_url = None , sub_dir = None , s3_workers = 1 , s3_resource_kwargs = None , key = 'fs_id' , store_hash = True , unpack_data = True , searchable_fields = None , ** kwargs ) special \u00b6 Initializes an S3 Store Parameters: Name Type Description Default index Store a store to use to index the S3 Bucket required bucket str name of the bucket required s3_profile Union[str, dict] name of aws profile containing credentials for role. Alternatively you can pass in a dictionary with the full credentials: aws_access_key_id (string) -- AWS access key ID aws_secret_access_key (string) -- AWS secret access key aws_session_token (string) -- AWS temporary session token region_name (string) -- Default region when creating new connections None compress bool compress files inserted into the store False endpoint_url str endpoint_url to allow interface to minio service None sub_dir str (optional) subdirectory of the s3 bucket to store the data None s3_workers int number of concurrent S3 puts to run 1 store_hash bool store the sha1 hash right before insertion to the database. True unpack_data bool whether to decompress and unpack byte data when querying from the bucket. True searchable_fields Optional[List[str]] fields to keep in the index store None Source code in maggma/stores/aws.py def __init__ ( self , index : Store , bucket : str , s3_profile : Optional [ Union [ str , dict ]] = None , compress : bool = False , endpoint_url : str = None , sub_dir : str = None , s3_workers : int = 1 , s3_resource_kwargs : Optional [ dict ] = None , key : str = \"fs_id\" , store_hash : bool = True , unpack_data : bool = True , searchable_fields : Optional [ List [ str ]] = None , ** kwargs , ): \"\"\" Initializes an S3 Store Args: index: a store to use to index the S3 Bucket bucket: name of the bucket s3_profile: name of aws profile containing credentials for role. Alternatively you can pass in a dictionary with the full credentials: aws_access_key_id (string) -- AWS access key ID aws_secret_access_key (string) -- AWS secret access key aws_session_token (string) -- AWS temporary session token region_name (string) -- Default region when creating new connections compress: compress files inserted into the store endpoint_url: endpoint_url to allow interface to minio service sub_dir: (optional) subdirectory of the s3 bucket to store the data s3_workers: number of concurrent S3 puts to run store_hash: store the sha1 hash right before insertion to the database. unpack_data: whether to decompress and unpack byte data when querying from the bucket. searchable_fields: fields to keep in the index store \"\"\" if boto3 is None : raise RuntimeError ( \"boto3 and botocore are required for S3Store\" ) self . index = index self . bucket = bucket self . s3_profile = s3_profile self . compress = compress self . endpoint_url = endpoint_url self . sub_dir = sub_dir . strip ( \"/\" ) + \"/\" if sub_dir else \"\" self . s3 = None # type: Any self . s3_bucket = None # type: Any self . s3_workers = s3_workers self . s3_resource_kwargs = ( s3_resource_kwargs if s3_resource_kwargs is not None else {} ) self . unpack_data = unpack_data self . searchable_fields = ( searchable_fields if searchable_fields is not None else [] ) self . store_hash = store_hash # Force the key to be the same as the index assert isinstance ( index . key , str ), \"Since we are using the key as a file name in S3, they key must be a string\" if key != index . key : warnings . warn ( f 'The desired S3Store key \" { key } \" does not match the index key \" { index . key } ,\"' \"the index key will be used\" , UserWarning , ) kwargs [ \"key\" ] = str ( index . key ) self . _thread_local = threading . local () super ( S3Store , self ) . __init__ ( ** kwargs ) close ( self ) \u00b6 Closes any connections Source code in maggma/stores/aws.py def close ( self ): \"\"\" Closes any connections \"\"\" self . index . close () self . s3 = None self . s3_bucket = None connect ( self , * args , ** kwargs ) \u00b6 Connect to the source data Source code in maggma/stores/aws.py def connect ( self , * args , ** kwargs ): # lgtm[py/conflicting-attributes] \"\"\" Connect to the source data \"\"\" session = self . _get_session () resource = session . resource ( \"s3\" , endpoint_url = self . endpoint_url , ** self . s3_resource_kwargs ) if not self . s3 : self . s3 = resource try : self . s3 . meta . client . head_bucket ( Bucket = self . bucket ) except ClientError : raise RuntimeError ( \"Bucket not present on AWS: {} \" . format ( self . bucket )) self . s3_bucket = resource . Bucket ( self . bucket ) self . index . connect ( * args , ** kwargs ) count ( self , criteria = None ) \u00b6 Counts the number of documents matching the query criteria Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None Source code in maggma/stores/aws.py def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" return self . index . count ( criteria ) distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Get all distinct values for a field Parameters: Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None Source code in maggma/stores/aws.py def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" # Index is a store so it should have its own distinct function return self . index . distinct ( field , criteria = criteria ) ensure_index ( self , key , unique = False ) \u00b6 Tries to create an index and return true if it suceeded Parameters: Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns: Type Description bool bool indicating if the index exists/was created Source code in maggma/stores/aws.py def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" return self . index . ensure_index ( key , unique = unique ) groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Simple grouping function that will group documents by keys. Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) Source code in maggma/stores/aws.py def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" return self . index . groupby ( keys = keys , criteria = criteria , properties = properties , sort = sort , skip = skip , limit = limit , ) newer_in ( self , target , criteria = None , exhaustive = False ) \u00b6 Returns the keys of documents that are newer in the target Store than this Store. Parameters: Name Type Description Default target Store target Store required criteria Optional[Dict] PyMongo filter for documents to search in None exhaustive bool triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in False Source code in maggma/stores/aws.py def newer_in ( self , target : Store , criteria : Optional [ Dict ] = None , exhaustive : bool = False ) -> List [ str ]: \"\"\" Returns the keys of documents that are newer in the target Store than this Store. Args: target: target Store criteria: PyMongo filter for documents to search in exhaustive: triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in \"\"\" if hasattr ( target , \"index\" ): return self . index . newer_in ( target = target . index , criteria = criteria , exhaustive = exhaustive ) else : return self . index . newer_in ( target = target , criteria = criteria , exhaustive = exhaustive ) query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Queries the Store for a set of documents Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Source code in maggma/stores/aws.py def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" prop_keys = set () if isinstance ( properties , dict ): prop_keys = set ( properties . keys ()) elif isinstance ( properties , list ): prop_keys = set ( properties ) for doc in self . index . query ( criteria = criteria , sort = sort , limit = limit , skip = skip ): if properties is not None and prop_keys . issubset ( set ( doc . keys ())): yield { p : doc [ p ] for p in properties if p in doc } else : try : # TODO: THis is ugly and unsafe, do some real checking before pulling data data = ( self . s3_bucket . Object ( self . sub_dir + str ( doc [ self . key ])) . get ()[ \"Body\" ] . read () ) except botocore . exceptions . ClientError as e : # If a client error is thrown, then check that it was a 404 error. # If it was a 404 error, then the object does not exist. error_code = int ( e . response [ \"Error\" ][ \"Code\" ]) if error_code == 404 : self . logger . error ( \"Could not find S3 object {} \" . format ( doc [ self . key ]) ) break else : raise e if self . unpack_data : data = self . _unpack ( data = data , compressed = doc . get ( \"compression\" , \"\" ) == \"zlib\" ) if self . last_updated_field in doc : data [ self . last_updated_field ] = doc [ self . last_updated_field ] yield data rebuild_index_from_s3_data ( self , ** kwargs ) \u00b6 Rebuilds the index Store from the data in S3 Relies on the index document being stores as the metadata for the file This can help recover lost databases Source code in maggma/stores/aws.py def rebuild_index_from_s3_data ( self , ** kwargs ): \"\"\" Rebuilds the index Store from the data in S3 Relies on the index document being stores as the metadata for the file This can help recover lost databases \"\"\" bucket = self . s3_bucket objects = bucket . objects . filter ( Prefix = self . sub_dir ) for obj in objects : key_ = self . sub_dir + obj . key data = self . s3_bucket . Object ( key_ ) . get ()[ \"Body\" ] . read () if self . compress : data = zlib . decompress ( data ) unpacked_data = msgpack . unpackb ( data , raw = False ) self . update ( unpacked_data , ** kwargs ) rebuild_metadata_from_index ( self , index_query = None ) \u00b6 Read data from the index store and populate the metadata of the S3 bucket Force all of the keys to be lower case to be Minio compatible Parameters: Name Type Description Default index_query dict query on the index store None Source code in maggma/stores/aws.py def rebuild_metadata_from_index ( self , index_query : dict = None ): \"\"\" Read data from the index store and populate the metadata of the S3 bucket Force all of the keys to be lower case to be Minio compatible Args: index_query: query on the index store \"\"\" qq = {} if index_query is None else index_query for index_doc in self . index . query ( qq ): key_ = self . sub_dir + index_doc [ self . key ] s3_object = self . s3_bucket . Object ( key_ ) new_meta = { self . _sanitize_key ( k ): v for k , v in s3_object . metadata . items ()} for k , v in index_doc . items (): new_meta [ str ( k ) . lower ()] = v new_meta . pop ( \"_id\" ) if self . last_updated_field in new_meta : new_meta [ self . last_updated_field ] = str ( to_isoformat_ceil_ms ( new_meta [ self . last_updated_field ]) ) # s3_object.metadata.update(new_meta) s3_object . copy_from ( CopySource = { \"Bucket\" : self . s3_bucket . name , \"Key\" : key_ }, Metadata = new_meta , MetadataDirective = \"REPLACE\" , ) remove_docs ( self , criteria , remove_s3_object = False ) \u00b6 Remove docs matching the query dictionary Parameters: Name Type Description Default criteria Dict query dictionary to match required remove_s3_object bool whether to remove the actual S3 Object or not False Source code in maggma/stores/aws.py def remove_docs ( self , criteria : Dict , remove_s3_object : bool = False ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match remove_s3_object: whether to remove the actual S3 Object or not \"\"\" if not remove_s3_object : self . index . remove_docs ( criteria = criteria ) else : to_remove = self . index . distinct ( self . key , criteria = criteria ) self . index . remove_docs ( criteria = criteria ) # Can remove up to 1000 items at a time via boto to_remove_chunks = list ( grouper ( to_remove , n = 1000 )) for chunk_to_remove in to_remove_chunks : objlist = [{ \"Key\" : f \" { self . sub_dir }{ obj } \" } for obj in chunk_to_remove ] self . s3_bucket . delete_objects ( Delete = { \"Objects\" : objlist }) update ( self , docs , key = None , additional_metadata = None ) \u00b6 Update documents into the Store Parameters: Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None additional_metadata Union[str, List[str]] field(s) to include in the s3 store's metadata None Source code in maggma/stores/aws.py def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None , additional_metadata : Union [ str , List [ str ], None ] = None , ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used additional_metadata: field(s) to include in the s3 store's metadata \"\"\" if not isinstance ( docs , list ): docs = [ docs ] if isinstance ( key , str ): key = [ key ] elif not key : key = [ self . key ] if additional_metadata is None : additional_metadata = [] elif isinstance ( additional_metadata , str ): additional_metadata = [ additional_metadata ] else : additional_metadata = list ( additional_metadata ) with ThreadPoolExecutor ( max_workers = self . s3_workers ) as pool : fs = { pool . submit ( self . write_doc_to_s3 , doc = itr_doc , search_keys = key + additional_metadata + self . searchable_fields , ) for itr_doc in docs } fs , _ = wait ( fs ) search_docs = [ sdoc . result () for sdoc in fs ] # Use store's update to remove key clashes self . index . update ( search_docs , key = self . key ) write_doc_to_s3 ( self , doc , search_keys ) \u00b6 Write the data to s3 and return the metadata to be inserted into the index db Parameters: Name Type Description Default doc Dict the document required search_keys List[str] list of keys to pull from the docs and be inserted into the required Source code in maggma/stores/aws.py def write_doc_to_s3 ( self , doc : Dict , search_keys : List [ str ]): \"\"\" Write the data to s3 and return the metadata to be inserted into the index db Args: doc: the document search_keys: list of keys to pull from the docs and be inserted into the index db \"\"\" s3_bucket = self . _get_bucket () search_doc = { k : doc [ k ] for k in search_keys } search_doc [ self . key ] = doc [ self . key ] # Ensure key is in metadata if self . sub_dir != \"\" : search_doc [ \"sub_dir\" ] = self . sub_dir # Remove MongoDB _id from search if \"_id\" in search_doc : del search_doc [ \"_id\" ] # to make hashing more meaningful, make sure last updated field is removed lu_info = doc . pop ( self . last_updated_field , None ) data = msgpack . packb ( doc , default = monty_default ) if self . compress : # Compress with zlib if chosen search_doc [ \"compression\" ] = \"zlib\" data = zlib . compress ( data ) if self . last_updated_field in doc : # need this conversion for aws metadata insert search_doc [ self . last_updated_field ] = str ( to_isoformat_ceil_ms ( doc [ self . last_updated_field ]) ) # keep a record of original keys, in case these are important for the individual researcher # it is not expected that this information will be used except in disaster recovery s3_to_mongo_keys = { k : self . _sanitize_key ( k ) for k in search_doc . keys ()} s3_to_mongo_keys [ \"s3-to-mongo-keys\" ] = \"s3-to-mongo-keys\" # inception # encode dictionary since values have to be strings search_doc [ \"s3-to-mongo-keys\" ] = dumps ( s3_to_mongo_keys ) s3_bucket . put_object ( Key = self . sub_dir + str ( doc [ self . key ]), Body = data , Metadata = { s3_to_mongo_keys [ k ]: str ( v ) for k , v in search_doc . items ()}, ) if lu_info is not None : search_doc [ self . last_updated_field ] = lu_info if self . store_hash : hasher = sha1 () hasher . update ( data ) obj_hash = hasher . hexdigest () search_doc [ \"obj_hash\" ] = obj_hash return search_doc Advanced Stores for behavior outside normal access patterns AliasingStore ( Store ) \u00b6 Special Store that aliases for the primary accessors Source code in maggma/stores/advanced_stores.py class AliasingStore ( Store ): \"\"\" Special Store that aliases for the primary accessors \"\"\" def __init__ ( self , store : Store , aliases : Dict , ** kwargs ): \"\"\" Args: store: the store to wrap around aliases: dict of aliases of the form external key: internal key \"\"\" self . store = store # Given an external key tells what the internal key is self . aliases = aliases # Given the internal key tells us what the external key is self . reverse_aliases = { v : k for k , v in aliases . items ()} self . kwargs = kwargs kwargs . update ( { \"last_updated_field\" : store . last_updated_field , \"last_updated_type\" : store . last_updated_type , } ) super ( AliasingStore , self ) . __init__ ( ** kwargs ) @property def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" return self . store . name def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" criteria = criteria if criteria else {} lazy_substitute ( criteria , self . reverse_aliases ) return self . store . count ( criteria ) def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" criteria = criteria if criteria else {} if properties is not None : if isinstance ( properties , list ): properties = { p : 1 for p in properties } substitute ( properties , self . reverse_aliases ) lazy_substitute ( criteria , self . reverse_aliases ) for d in self . store . query ( properties = properties , criteria = criteria , sort = sort , limit = limit , skip = skip ): substitute ( d , self . aliases ) yield d def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria if criteria else {} lazy_substitute ( criteria , self . reverse_aliases ) # substitute forward return self . store . distinct ( self . aliases [ field ], criteria = criteria ) def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" # Convert to a list keys = keys if isinstance ( keys , list ) else [ keys ] # Make the aliasing transformations on keys keys = [ self . aliases [ k ] if k in self . aliases else k for k in keys ] # Update criteria and properties based on aliases criteria = criteria if criteria else {} if properties is not None : if isinstance ( properties , list ): properties = { p : 1 for p in properties } substitute ( properties , self . reverse_aliases ) lazy_substitute ( criteria , self . reverse_aliases ) return self . store . groupby ( keys = keys , properties = properties , criteria = criteria , skip = skip , limit = limit ) def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" key = key if key else self . key for d in docs : substitute ( d , self . reverse_aliases ) if key in self . aliases : key = self . aliases [ key ] self . store . update ( docs , key = key ) def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" # Update criteria and properties based on aliases lazy_substitute ( criteria , self . reverse_aliases ) self . store . remove_docs ( criteria ) def ensure_index ( self , key , unique = False , ** kwargs ): if key in self . aliases : key = self . aliases return self . store . ensure_index ( key , unique , ** kwargs ) def close ( self ): self . store . close () @property def _collection ( self ): return self . store . _collection def connect ( self , force_reset = False ): self . store . connect ( force_reset = force_reset ) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for AliasingStore Args: other: other AliasingStore to compare with \"\"\" if not isinstance ( other , AliasingStore ): return False fields = [ \"store\" , \"aliases\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) name : str property readonly \u00b6 Return a string representing this data source __eq__ ( self , other ) special \u00b6 Check equality for AliasingStore Parameters: Name Type Description Default other object other AliasingStore to compare with required Source code in maggma/stores/advanced_stores.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for AliasingStore Args: other: other AliasingStore to compare with \"\"\" if not isinstance ( other , AliasingStore ): return False fields = [ \"store\" , \"aliases\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) __init__ ( self , store , aliases , ** kwargs ) special \u00b6 Parameters: Name Type Description Default store Store the store to wrap around required aliases Dict dict of aliases of the form external key: internal key required Source code in maggma/stores/advanced_stores.py def __init__ ( self , store : Store , aliases : Dict , ** kwargs ): \"\"\" Args: store: the store to wrap around aliases: dict of aliases of the form external key: internal key \"\"\" self . store = store # Given an external key tells what the internal key is self . aliases = aliases # Given the internal key tells us what the external key is self . reverse_aliases = { v : k for k , v in aliases . items ()} self . kwargs = kwargs kwargs . update ( { \"last_updated_field\" : store . last_updated_field , \"last_updated_type\" : store . last_updated_type , } ) super ( AliasingStore , self ) . __init__ ( ** kwargs ) close ( self ) \u00b6 Closes any connections Source code in maggma/stores/advanced_stores.py def close ( self ): self . store . close () connect ( self , force_reset = False ) \u00b6 Connect to the source data Parameters: Name Type Description Default force_reset whether to reset the connection or not False Source code in maggma/stores/advanced_stores.py def connect ( self , force_reset = False ): self . store . connect ( force_reset = force_reset ) count ( self , criteria = None ) \u00b6 Counts the number of documents matching the query criteria Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None Source code in maggma/stores/advanced_stores.py def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" criteria = criteria if criteria else {} lazy_substitute ( criteria , self . reverse_aliases ) return self . store . count ( criteria ) distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Get all distinct values for a field Parameters: Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None Source code in maggma/stores/advanced_stores.py def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria if criteria else {} lazy_substitute ( criteria , self . reverse_aliases ) # substitute forward return self . store . distinct ( self . aliases [ field ], criteria = criteria ) ensure_index ( self , key , unique = False , ** kwargs ) \u00b6 Tries to create an index and return true if it suceeded Parameters: Name Type Description Default key single key to index required unique Whether or not this index contains only unique keys False Returns: Type Description bool indicating if the index exists/was created Source code in maggma/stores/advanced_stores.py def ensure_index ( self , key , unique = False , ** kwargs ): if key in self . aliases : key = self . aliases return self . store . ensure_index ( key , unique , ** kwargs ) groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Simple grouping function that will group documents by keys. Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) Source code in maggma/stores/advanced_stores.py def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" # Convert to a list keys = keys if isinstance ( keys , list ) else [ keys ] # Make the aliasing transformations on keys keys = [ self . aliases [ k ] if k in self . aliases else k for k in keys ] # Update criteria and properties based on aliases criteria = criteria if criteria else {} if properties is not None : if isinstance ( properties , list ): properties = { p : 1 for p in properties } substitute ( properties , self . reverse_aliases ) lazy_substitute ( criteria , self . reverse_aliases ) return self . store . groupby ( keys = keys , properties = properties , criteria = criteria , skip = skip , limit = limit ) query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Queries the Store for a set of documents Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Source code in maggma/stores/advanced_stores.py def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" criteria = criteria if criteria else {} if properties is not None : if isinstance ( properties , list ): properties = { p : 1 for p in properties } substitute ( properties , self . reverse_aliases ) lazy_substitute ( criteria , self . reverse_aliases ) for d in self . store . query ( properties = properties , criteria = criteria , sort = sort , limit = limit , skip = skip ): substitute ( d , self . aliases ) yield d remove_docs ( self , criteria ) \u00b6 Remove docs matching the query dictionary Parameters: Name Type Description Default criteria Dict query dictionary to match required Source code in maggma/stores/advanced_stores.py def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" # Update criteria and properties based on aliases lazy_substitute ( criteria , self . reverse_aliases ) self . store . remove_docs ( criteria ) update ( self , docs , key = None ) \u00b6 Update documents into the Store Parameters: Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Source code in maggma/stores/advanced_stores.py def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" key = key if key else self . key for d in docs : substitute ( d , self . reverse_aliases ) if key in self . aliases : key = self . aliases [ key ] self . store . update ( docs , key = key ) MongograntStore ( MongoStore ) \u00b6 Initialize a Store with a mongogrant \" <role> : <host> / <db> .\" spec. Some class methods of MongoStore, e.g. from_db_file and from_collection, are not supported. mongogrant documentation: https://github.com/materialsproject/mongogrant Source code in maggma/stores/advanced_stores.py class MongograntStore ( MongoStore ): \"\"\"Initialize a Store with a mongogrant \"`<role>`:`<host>`/`<db>`.\" spec. Some class methods of MongoStore, e.g. from_db_file and from_collection, are not supported. mongogrant documentation: https://github.com/materialsproject/mongogrant \"\"\" def __init__ ( self , mongogrant_spec : str , collection_name : str , mgclient_config_path : Optional [ str ] = None , ** kwargs , ): \"\"\" Args: mongogrant_spec: of the form `<role>`:`<host>`/`<db>`, where role is one of {\"read\", \"readWrite\"} or aliases {\"ro\", \"rw\"}; host is a db host (w/ optional port) or alias; and db is a db on that host, or alias. See mongogrant documentation. collection_name: name of mongo collection mgclient_config_path: Path to mongogrant client config file, or None if default path (`mongogrant.client.path`). \"\"\" self . mongogrant_spec = mongogrant_spec self . collection_name = collection_name self . mgclient_config_path = mgclient_config_path self . _coll = None if self . mgclient_config_path : config = Config ( check = check , path = self . mgclient_config_path ) client = Client ( config ) else : client = Client () if set (( \"username\" , \"password\" , \"database\" , \"host\" )) & set ( kwargs ): raise StoreError ( \"MongograntStore does not accept \" \"username, password, database, or host \" \"arguments. Use `mongogrant_spec`.\" ) self . kwargs = kwargs _auth_info = client . get_db_auth_from_spec ( self . mongogrant_spec ) super ( MongograntStore , self ) . __init__ ( host = _auth_info [ \"host\" ], database = _auth_info [ \"authSource\" ], username = _auth_info [ \"username\" ], password = _auth_info [ \"password\" ], collection_name = self . collection_name , ** kwargs , ) @property def name ( self ): return f \"mgrant:// { self . mongogrant_spec } / { self . collection_name } \" def __hash__ ( self ): return hash ( ( self . mongogrant_spec , self . collection_name , self . last_updated_field ) ) @classmethod def from_db_file ( cls , file ): \"\"\" Raises ValueError since MongograntStores can't be initialized from a file \"\"\" raise ValueError ( \"MongograntStore doesn't implement from_db_file\" ) @classmethod def from_collection ( cls , collection ): \"\"\" Raises ValueError since MongograntStores can't be initialized from a PyMongo collection \"\"\" raise ValueError ( \"MongograntStore doesn't implement from_collection\" ) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MongograntStore Args: other: other MongograntStore to compare with \"\"\" if not isinstance ( other , MongograntStore ): return False fields = [ \"mongogrant_spec\" , \"collection_name\" , \"mgclient_config_path\" , \"last_updated_field\" , ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) name property readonly \u00b6 Return a string representing this data source __eq__ ( self , other ) special \u00b6 Check equality for MongograntStore Parameters: Name Type Description Default other object other MongograntStore to compare with required Source code in maggma/stores/advanced_stores.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MongograntStore Args: other: other MongograntStore to compare with \"\"\" if not isinstance ( other , MongograntStore ): return False fields = [ \"mongogrant_spec\" , \"collection_name\" , \"mgclient_config_path\" , \"last_updated_field\" , ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) __init__ ( self , mongogrant_spec , collection_name , mgclient_config_path = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default mongogrant_spec str of the form <role> : <host> / <db> , where role is one of {\"read\", \"readWrite\"} or aliases {\"ro\", \"rw\"}; host is a db host (w/ optional port) or alias; and db is a db on that host, or alias. See mongogrant documentation. required collection_name str name of mongo collection required mgclient_config_path Optional[str] Path to mongogrant client config file, or None if default path ( mongogrant.client.path ). None Source code in maggma/stores/advanced_stores.py def __init__ ( self , mongogrant_spec : str , collection_name : str , mgclient_config_path : Optional [ str ] = None , ** kwargs , ): \"\"\" Args: mongogrant_spec: of the form `<role>`:`<host>`/`<db>`, where role is one of {\"read\", \"readWrite\"} or aliases {\"ro\", \"rw\"}; host is a db host (w/ optional port) or alias; and db is a db on that host, or alias. See mongogrant documentation. collection_name: name of mongo collection mgclient_config_path: Path to mongogrant client config file, or None if default path (`mongogrant.client.path`). \"\"\" self . mongogrant_spec = mongogrant_spec self . collection_name = collection_name self . mgclient_config_path = mgclient_config_path self . _coll = None if self . mgclient_config_path : config = Config ( check = check , path = self . mgclient_config_path ) client = Client ( config ) else : client = Client () if set (( \"username\" , \"password\" , \"database\" , \"host\" )) & set ( kwargs ): raise StoreError ( \"MongograntStore does not accept \" \"username, password, database, or host \" \"arguments. Use `mongogrant_spec`.\" ) self . kwargs = kwargs _auth_info = client . get_db_auth_from_spec ( self . mongogrant_spec ) super ( MongograntStore , self ) . __init__ ( host = _auth_info [ \"host\" ], database = _auth_info [ \"authSource\" ], username = _auth_info [ \"username\" ], password = _auth_info [ \"password\" ], collection_name = self . collection_name , ** kwargs , ) from_collection ( collection ) classmethod \u00b6 Raises ValueError since MongograntStores can't be initialized from a PyMongo collection Source code in maggma/stores/advanced_stores.py @classmethod def from_collection ( cls , collection ): \"\"\" Raises ValueError since MongograntStores can't be initialized from a PyMongo collection \"\"\" raise ValueError ( \"MongograntStore doesn't implement from_collection\" ) from_db_file ( file ) classmethod \u00b6 Raises ValueError since MongograntStores can't be initialized from a file Source code in maggma/stores/advanced_stores.py @classmethod def from_db_file ( cls , file ): \"\"\" Raises ValueError since MongograntStores can't be initialized from a file \"\"\" raise ValueError ( \"MongograntStore doesn't implement from_db_file\" ) SandboxStore ( Store ) \u00b6 Provides a sandboxed view to another store Source code in maggma/stores/advanced_stores.py class SandboxStore ( Store ): \"\"\" Provides a sandboxed view to another store \"\"\" def __init__ ( self , store : Store , sandbox : str , exclusive : bool = False ): \"\"\" Args: store: store to wrap sandboxing around sandbox: the corresponding sandbox exclusive: whether to be exclusively in this sandbox or include global items \"\"\" self . store = store self . sandbox = sandbox self . exclusive = exclusive super () . __init__ ( key = self . store . key , last_updated_field = self . store . last_updated_field , last_updated_type = self . store . last_updated_type , validator = self . store . validator , ) @property def name ( self ) -> str : \"\"\" Returns: a string representing this data source \"\"\" return f \"Sandbox[ { self . store . name } ][ { self . sandbox } ]\" @property def sbx_criteria ( self ) -> Dict : \"\"\" Returns: the sandbox criteria dict used to filter the source store \"\"\" if self . exclusive : return { \"sbxn\" : self . sandbox } else : return { \"$or\" : [{ \"sbxn\" : { \"$in\" : [ self . sandbox ]}}, { \"sbxn\" : { \"$exists\" : False }}] } def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . count ( criteria = criteria ) def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . query ( properties = properties , criteria = criteria , sort = sort , limit = limit , skip = skip ) def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . groupby ( keys = keys , properties = properties , criteria = criteria , skip = skip , limit = limit ) def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" for d in docs : if \"sbxn\" in d : d [ \"sbxn\" ] = list ( set ( d [ \"sbxn\" ] + [ self . sandbox ])) else : d [ \"sbxn\" ] = [ self . sandbox ] self . store . update ( docs , key = key ) def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" # Update criteria and properties based on aliases criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) self . store . remove_docs ( criteria ) def ensure_index ( self , key , unique = False , ** kwargs ): return self . store . ensure_index ( key , unique , ** kwargs ) def close ( self ): self . store . close () @property def _collection ( self ): return self . store . _collection def connect ( self , force_reset = False ): self . store . connect ( force_reset = force_reset ) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for SandboxStore Args: other: other SandboxStore to compare with \"\"\" if not isinstance ( other , SandboxStore ): return False fields = [ \"store\" , \"sandbox\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) name : str property readonly \u00b6 Returns: Type Description str a string representing this data source sbx_criteria : Dict property readonly \u00b6 Returns: Type Description Dict the sandbox criteria dict used to filter the source store __eq__ ( self , other ) special \u00b6 Check equality for SandboxStore Parameters: Name Type Description Default other object other SandboxStore to compare with required Source code in maggma/stores/advanced_stores.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for SandboxStore Args: other: other SandboxStore to compare with \"\"\" if not isinstance ( other , SandboxStore ): return False fields = [ \"store\" , \"sandbox\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) __init__ ( self , store , sandbox , exclusive = False ) special \u00b6 Parameters: Name Type Description Default store Store store to wrap sandboxing around required sandbox str the corresponding sandbox required exclusive bool whether to be exclusively in this sandbox or include global items False Source code in maggma/stores/advanced_stores.py def __init__ ( self , store : Store , sandbox : str , exclusive : bool = False ): \"\"\" Args: store: store to wrap sandboxing around sandbox: the corresponding sandbox exclusive: whether to be exclusively in this sandbox or include global items \"\"\" self . store = store self . sandbox = sandbox self . exclusive = exclusive super () . __init__ ( key = self . store . key , last_updated_field = self . store . last_updated_field , last_updated_type = self . store . last_updated_type , validator = self . store . validator , ) close ( self ) \u00b6 Closes any connections Source code in maggma/stores/advanced_stores.py def close ( self ): self . store . close () connect ( self , force_reset = False ) \u00b6 Connect to the source data Parameters: Name Type Description Default force_reset whether to reset the connection or not False Source code in maggma/stores/advanced_stores.py def connect ( self , force_reset = False ): self . store . connect ( force_reset = force_reset ) count ( self , criteria = None ) \u00b6 Counts the number of documents matching the query criteria Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None Source code in maggma/stores/advanced_stores.py def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . count ( criteria = criteria ) ensure_index ( self , key , unique = False , ** kwargs ) \u00b6 Tries to create an index and return true if it suceeded Parameters: Name Type Description Default key single key to index required unique Whether or not this index contains only unique keys False Returns: Type Description bool indicating if the index exists/was created Source code in maggma/stores/advanced_stores.py def ensure_index ( self , key , unique = False , ** kwargs ): return self . store . ensure_index ( key , unique , ** kwargs ) groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Simple grouping function that will group documents by keys. Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) Source code in maggma/stores/advanced_stores.py def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . groupby ( keys = keys , properties = properties , criteria = criteria , skip = skip , limit = limit ) query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Queries the Store for a set of documents Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Source code in maggma/stores/advanced_stores.py def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . query ( properties = properties , criteria = criteria , sort = sort , limit = limit , skip = skip ) remove_docs ( self , criteria ) \u00b6 Remove docs matching the query dictionary Parameters: Name Type Description Default criteria Dict query dictionary to match required Source code in maggma/stores/advanced_stores.py def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" # Update criteria and properties based on aliases criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) self . store . remove_docs ( criteria ) update ( self , docs , key = None ) \u00b6 Update documents into the Store Parameters: Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Source code in maggma/stores/advanced_stores.py def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" for d in docs : if \"sbxn\" in d : d [ \"sbxn\" ] = list ( set ( d [ \"sbxn\" ] + [ self . sandbox ])) else : d [ \"sbxn\" ] = [ self . sandbox ] self . store . update ( docs , key = key ) VaultStore ( MongoStore ) \u00b6 Extends MongoStore to read credentials out of Vault server and uses these values to initialize MongoStore instance Source code in maggma/stores/advanced_stores.py class VaultStore ( MongoStore ): \"\"\" Extends MongoStore to read credentials out of Vault server and uses these values to initialize MongoStore instance \"\"\" @requires ( hvac is not None , \"hvac is required to use VaultStore\" ) def __init__ ( self , collection_name : str , vault_secret_path : str ): \"\"\" Args: collection_name: name of mongo collection vault_secret_path: path on vault server with mongo creds object Important: Environment variables that must be set prior to invocation VAULT_ADDR - URL of vault server (eg. https://matgen8.lbl.gov:8200) VAULT_TOKEN or GITHUB_TOKEN - token used to authenticate to vault \"\"\" self . collection_name = collection_name self . vault_secret_path = vault_secret_path # TODO: Switch this over to Pydantic ConfigSettings vault_addr = os . getenv ( \"VAULT_ADDR\" ) if not vault_addr : raise RuntimeError ( \"VAULT_ADDR not set\" ) client = hvac . Client ( vault_addr ) # If we have a vault token use this token = os . getenv ( \"VAULT_TOKEN\" ) # Look for a github token instead if not token : github_token = os . getenv ( \"GITHUB_TOKEN\" ) if github_token : client . auth_github ( github_token ) else : raise RuntimeError ( \"VAULT_TOKEN or GITHUB_TOKEN not set\" ) else : client . token = token if not client . is_authenticated (): raise RuntimeError ( \"Bad token\" ) # Read the vault secret json_db_creds = client . read ( vault_secret_path ) db_creds = json . loads ( json_db_creds [ \"data\" ][ \"value\" ]) database = db_creds . get ( \"db\" ) host = db_creds . get ( \"host\" , \"localhost\" ) port = db_creds . get ( \"port\" , 27017 ) username = db_creds . get ( \"username\" , \"\" ) password = db_creds . get ( \"password\" , \"\" ) super ( VaultStore , self ) . __init__ ( database , collection_name , host , port , username , password ) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for VaultStore Args: other: other VaultStore to compare with \"\"\" if not isinstance ( other , VaultStore ): return False fields = [ \"vault_secret_path\" , \"collection_name\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) __eq__ ( self , other ) special \u00b6 Check equality for VaultStore Parameters: Name Type Description Default other object other VaultStore to compare with required Source code in maggma/stores/advanced_stores.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for VaultStore Args: other: other VaultStore to compare with \"\"\" if not isinstance ( other , VaultStore ): return False fields = [ \"vault_secret_path\" , \"collection_name\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) __init__ ( self , collection_name , vault_secret_path ) special \u00b6 Parameters: Name Type Description Default collection_name str name of mongo collection required vault_secret_path str path on vault server with mongo creds object required Important Environment variables that must be set prior to invocation VAULT_ADDR - URL of vault server (eg. https://matgen8.lbl.gov:8200) VAULT_TOKEN or GITHUB_TOKEN - token used to authenticate to vault Source code in maggma/stores/advanced_stores.py @requires ( hvac is not None , \"hvac is required to use VaultStore\" ) def __init__ ( self , collection_name : str , vault_secret_path : str ): \"\"\" Args: collection_name: name of mongo collection vault_secret_path: path on vault server with mongo creds object Important: Environment variables that must be set prior to invocation VAULT_ADDR - URL of vault server (eg. https://matgen8.lbl.gov:8200) VAULT_TOKEN or GITHUB_TOKEN - token used to authenticate to vault \"\"\" self . collection_name = collection_name self . vault_secret_path = vault_secret_path # TODO: Switch this over to Pydantic ConfigSettings vault_addr = os . getenv ( \"VAULT_ADDR\" ) if not vault_addr : raise RuntimeError ( \"VAULT_ADDR not set\" ) client = hvac . Client ( vault_addr ) # If we have a vault token use this token = os . getenv ( \"VAULT_TOKEN\" ) # Look for a github token instead if not token : github_token = os . getenv ( \"GITHUB_TOKEN\" ) if github_token : client . auth_github ( github_token ) else : raise RuntimeError ( \"VAULT_TOKEN or GITHUB_TOKEN not set\" ) else : client . token = token if not client . is_authenticated (): raise RuntimeError ( \"Bad token\" ) # Read the vault secret json_db_creds = client . read ( vault_secret_path ) db_creds = json . loads ( json_db_creds [ \"data\" ][ \"value\" ]) database = db_creds . get ( \"db\" ) host = db_creds . get ( \"host\" , \"localhost\" ) port = db_creds . get ( \"port\" , 27017 ) username = db_creds . get ( \"username\" , \"\" ) password = db_creds . get ( \"password\" , \"\" ) super ( VaultStore , self ) . __init__ ( database , collection_name , host , port , username , password ) Special stores that combine underlying Stores together ConcatStore ( Store ) \u00b6 Store concatting multiple stores Source code in maggma/stores/compound_stores.py class ConcatStore ( Store ): \"\"\"Store concatting multiple stores\"\"\" def __init__ ( self , stores : List [ Store ], ** kwargs ): \"\"\" Initialize a ConcatStore that concatenates multiple stores together to appear as one store Args: stores: list of stores to concatenate together \"\"\" self . stores = stores self . kwargs = kwargs super ( ConcatStore , self ) . __init__ ( ** kwargs ) @property def name ( self ) -> str : \"\"\" A string representing this data source \"\"\" compound_name = \",\" . join ([ store . name for store in self . stores ]) return f \"Concat[ { compound_name } ]\" def connect ( self , force_reset : bool = False ): \"\"\" Connect all stores in this ConcatStore Args: force_reset: Whether to forcibly reset the connection for all stores \"\"\" for store in self . stores : store . connect ( force_reset ) def close ( self ): \"\"\" Close all connections in this ConcatStore \"\"\" for store in self . stores : store . close () @property def _collection ( self ): raise NotImplementedError ( \"No collection property for ConcatStore\" ) @property def last_updated ( self ) -> datetime : \"\"\" Finds the most recent last_updated across all the stores. This might not be the most usefull way to do this for this type of Store since it could very easily over-estimate the last_updated based on what stores are used \"\"\" lus = [] for store in self . stores : lu = store . last_updated lus . append ( lu ) return max ( lus ) def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Not implemented in ConcatStore Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" raise NotImplementedError ( \"No update method for ConcatStore\" ) def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" distincts = [] for store in self . stores : distincts . extend ( store . distinct ( field = field , criteria = criteria )) return list ( set ( distincts )) def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Ensure an index is properly set. Returns whether all stores support this index or not Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created on all stores \"\"\" return all ([ store . ensure_index ( key , unique ) for store in self . stores ]) def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" counts = [ store . count ( criteria ) for store in self . stores ] return sum ( counts ) def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries across all Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" # TODO: skip, sort and limit are broken. implement properly for store in self . stores : for d in store . query ( criteria = criteria , properties = properties ): yield d def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" if isinstance ( keys , str ): keys = [ keys ] docs = [] for store in self . stores : temp_docs = list ( store . groupby ( keys = keys , criteria = criteria , properties = properties , sort = sort , skip = skip , limit = limit , ) ) for key , group in temp_docs : docs . extend ( group ) def key_set ( d : Dict ) -> Tuple : \"index function based on passed in keys\" test_d = tuple ( d . get ( k , None ) for k in keys ) return test_d sorted_docs = sorted ( docs , key = key_set ) for vals , group_iter in groupby ( sorted_docs , key = key_set ): id_dict = { key : val for key , val in zip ( keys , vals )} yield id_dict , list ( group_iter ) def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" raise NotImplementedError ( \"No remove_docs method for JointStore\" ) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for ConcatStore Args: other: other JointStore to compare with \"\"\" if not isinstance ( other , ConcatStore ): return False fields = [ \"stores\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) last_updated : datetime property readonly \u00b6 Finds the most recent last_updated across all the stores. This might not be the most usefull way to do this for this type of Store since it could very easily over-estimate the last_updated based on what stores are used name : str property readonly \u00b6 A string representing this data source __eq__ ( self , other ) special \u00b6 Check equality for ConcatStore Parameters: Name Type Description Default other object other JointStore to compare with required Source code in maggma/stores/compound_stores.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for ConcatStore Args: other: other JointStore to compare with \"\"\" if not isinstance ( other , ConcatStore ): return False fields = [ \"stores\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) __init__ ( self , stores , ** kwargs ) special \u00b6 Initialize a ConcatStore that concatenates multiple stores together to appear as one store Parameters: Name Type Description Default stores List[maggma.core.store.Store] list of stores to concatenate together required Source code in maggma/stores/compound_stores.py def __init__ ( self , stores : List [ Store ], ** kwargs ): \"\"\" Initialize a ConcatStore that concatenates multiple stores together to appear as one store Args: stores: list of stores to concatenate together \"\"\" self . stores = stores self . kwargs = kwargs super ( ConcatStore , self ) . __init__ ( ** kwargs ) close ( self ) \u00b6 Close all connections in this ConcatStore Source code in maggma/stores/compound_stores.py def close ( self ): \"\"\" Close all connections in this ConcatStore \"\"\" for store in self . stores : store . close () connect ( self , force_reset = False ) \u00b6 Connect all stores in this ConcatStore Parameters: Name Type Description Default force_reset bool Whether to forcibly reset the connection for all stores False Source code in maggma/stores/compound_stores.py def connect ( self , force_reset : bool = False ): \"\"\" Connect all stores in this ConcatStore Args: force_reset: Whether to forcibly reset the connection for all stores \"\"\" for store in self . stores : store . connect ( force_reset ) count ( self , criteria = None ) \u00b6 Counts the number of documents matching the query criteria Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None Source code in maggma/stores/compound_stores.py def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" counts = [ store . count ( criteria ) for store in self . stores ] return sum ( counts ) distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Get all distinct values for a field Parameters: Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None Source code in maggma/stores/compound_stores.py def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" distincts = [] for store in self . stores : distincts . extend ( store . distinct ( field = field , criteria = criteria )) return list ( set ( distincts )) ensure_index ( self , key , unique = False ) \u00b6 Ensure an index is properly set. Returns whether all stores support this index or not Parameters: Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns: Type Description bool bool indicating if the index exists/was created on all stores Source code in maggma/stores/compound_stores.py def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Ensure an index is properly set. Returns whether all stores support this index or not Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created on all stores \"\"\" return all ([ store . ensure_index ( key , unique ) for store in self . stores ]) groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Simple grouping function that will group documents by keys. Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) Source code in maggma/stores/compound_stores.py def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" if isinstance ( keys , str ): keys = [ keys ] docs = [] for store in self . stores : temp_docs = list ( store . groupby ( keys = keys , criteria = criteria , properties = properties , sort = sort , skip = skip , limit = limit , ) ) for key , group in temp_docs : docs . extend ( group ) def key_set ( d : Dict ) -> Tuple : \"index function based on passed in keys\" test_d = tuple ( d . get ( k , None ) for k in keys ) return test_d sorted_docs = sorted ( docs , key = key_set ) for vals , group_iter in groupby ( sorted_docs , key = key_set ): id_dict = { key : val for key , val in zip ( keys , vals )} yield id_dict , list ( group_iter ) query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Queries across all Store for a set of documents Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Source code in maggma/stores/compound_stores.py def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries across all Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" # TODO: skip, sort and limit are broken. implement properly for store in self . stores : for d in store . query ( criteria = criteria , properties = properties ): yield d remove_docs ( self , criteria ) \u00b6 Remove docs matching the query dictionary Parameters: Name Type Description Default criteria Dict query dictionary to match required Source code in maggma/stores/compound_stores.py def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" raise NotImplementedError ( \"No remove_docs method for JointStore\" ) update ( self , docs , key = None ) \u00b6 Update documents into the Store Not implemented in ConcatStore Parameters: Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Source code in maggma/stores/compound_stores.py def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Not implemented in ConcatStore Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" raise NotImplementedError ( \"No update method for ConcatStore\" ) JointStore ( Store ) \u00b6 Store that implements a on-the-fly join across multiple collections all in the same MongoDB database. This is a Read-Only Store designed to combine data from multiple collections. Source code in maggma/stores/compound_stores.py class JointStore ( Store ): \"\"\" Store that implements a on-the-fly join across multiple collections all in the same MongoDB database. This is a Read-Only Store designed to combine data from multiple collections. \"\"\" def __init__ ( self , database : str , collection_names : List [ str ], host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , main : Optional [ str ] = None , merge_at_root : bool = False , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Args: database: The database name collection_names: list of all collections to join host: Hostname for the database port: TCP port to connect to username: Username for the collection password: Password to connect with main: name for the main collection if not specified this defaults to the first in collection_names list \"\"\" self . database = database self . collection_names = collection_names self . host = host self . port = port self . username = username self . password = password self . _coll = None # type: Any self . main = main or collection_names [ 0 ] self . merge_at_root = merge_at_root self . mongoclient_kwargs = mongoclient_kwargs or {} self . kwargs = kwargs super ( JointStore , self ) . __init__ ( ** kwargs ) @property def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" compound_name = \",\" . join ( self . collection_names ) return f \"Compound[ { self . host } / { self . database } ][ { compound_name } ]\" def connect ( self , force_reset : bool = False ): \"\"\" Connects the underlying Mongo database and all collection connections Args: force_reset: whether to forcibly reset the connection \"\"\" conn = ( MongoClient ( host = self . host , port = self . port , username = self . username , password = self . password , ** self . mongoclient_kwargs , ) if self . username != \"\" else MongoClient ( self . host , self . port , ** self . mongoclient_kwargs ) ) db = conn [ self . database ] self . _coll = db [ self . main ] self . _has_merge_objects = ( self . _collection . database . client . server_info ()[ \"version\" ] > \"3.6\" ) def close ( self ): \"\"\" Closes underlying database connections \"\"\" self . _collection . database . client . close () @property def _collection ( self ): \"\"\"Property referring to the root pymongo collection\"\"\" if self . _coll is None : raise StoreError ( \"Must connect Mongo-like store before attemping to use it\" ) return self . _coll @property def nonmain_names ( self ) -> List : \"\"\" alll non-main collection names \"\"\" return list ( set ( self . collection_names ) - { self . main }) @property def last_updated ( self ) -> datetime : \"\"\" Special last_updated for this JointStore that checks all underlying collections \"\"\" lus = [] for cname in self . collection_names : store = MongoStore . from_collection ( self . _collection . database [ cname ]) store . last_updated_field = self . last_updated_field lu = store . last_updated lus . append ( lu ) return max ( lus ) # TODO: implement update? def update ( self , docs , update_lu = True , key = None , ** kwargs ): \"\"\" Update documents into the underlying collections Not Implemented for JointStore \"\"\" raise NotImplementedError ( \"JointStore is a read-only store\" ) def _get_store_by_name ( self , name ) -> MongoStore : \"\"\" Gets an underlying collection as a mongoStore \"\"\" if name not in self . collection_names : raise ValueError ( \"Asking for collection not referenced in this Store\" ) return MongoStore . from_collection ( self . _collection . database [ name ]) def ensure_index ( self , key , unique = False , ** kwargs ): \"\"\" Can't ensure index for JointStore \"\"\" raise NotImplementedError ( \"No ensure_index method for JointStore\" ) def _get_pipeline ( self , criteria = None , properties = None , skip = 0 , limit = 0 ): \"\"\" Gets the aggregation pipeline for query and query_one Args: properties: properties to be returned criteria: criteria to filter by skip: docs to skip limit: limit results to N docs Returns: list of aggregation operators \"\"\" pipeline = [] collection_names = list ( set ( self . collection_names ) - set ( self . main )) for cname in collection_names : pipeline . append ( { \"$lookup\" : { \"from\" : cname , \"localField\" : self . key , \"foreignField\" : self . key , \"as\" : cname , } } ) if self . merge_at_root : if not self . _has_merge_objects : raise Exception ( \"MongoDB server version too low to use $mergeObjects.\" ) pipeline . append ( { \"$replaceRoot\" : { \"newRoot\" : { \"$mergeObjects\" : [ { \"$arrayElemAt\" : [ \"$ {} \" . format ( cname ), 0 ]}, \"$$ROOT\" , ] } } } ) else : pipeline . append ( { \"$unwind\" : { \"path\" : \"$ {} \" . format ( cname ), \"preserveNullAndEmptyArrays\" : True , } } ) # Do projection for max last_updated lu_max_fields = [ \"$ {} \" . format ( self . last_updated_field )] lu_max_fields . extend ( [ \"$ {} . {} \" . format ( cname , self . last_updated_field ) for cname in self . collection_names ] ) lu_proj = { self . last_updated_field : { \"$max\" : lu_max_fields }} pipeline . append ({ \"$addFields\" : lu_proj }) if criteria : pipeline . append ({ \"$match\" : criteria }) if isinstance ( properties , list ): properties = { k : 1 for k in properties } if properties : pipeline . append ({ \"$project\" : properties }) if skip > 0 : pipeline . append ({ \"$skip\" : skip }) if limit > 0 : pipeline . append ({ \"$limit\" : limit }) return pipeline def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" pipeline = self . _get_pipeline ( criteria = criteria ) pipeline . append ({ \"$count\" : \"count\" }) agg = list ( self . _collection . aggregate ( pipeline )) return agg [ 0 ] . get ( \"count\" , 0 ) if len ( agg ) > 0 else 0 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: pipeline = self . _get_pipeline ( criteria = criteria , properties = properties , skip = skip , limit = limit ) agg = self . _collection . aggregate ( pipeline ) for d in agg : yield d def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: pipeline = self . _get_pipeline ( criteria = criteria , properties = properties , skip = skip , limit = limit ) if not isinstance ( keys , list ): keys = [ keys ] group_id = {} # type: Dict[str,Any] for key in keys : set_ ( group_id , key , \"$ {} \" . format ( key )) pipeline . append ({ \"$group\" : { \"_id\" : group_id , \"docs\" : { \"$push\" : \"$$ROOT\" }}}) agg = self . _collection . aggregate ( pipeline ) for d in agg : yield d [ \"_id\" ], d [ \"docs\" ] def query_one ( self , criteria = None , properties = None , ** kwargs ): \"\"\" Get one document Args: properties: properties to return in query criteria: filter for matching kwargs: kwargs for collection.aggregate Returns: single document \"\"\" # TODO: maybe adding explicit limit in agg pipeline is better as below? # pipeline = self._get_pipeline(properties, criteria) # pipeline.append({\"$limit\": 1}) query = self . query ( criteria = criteria , properties = properties , ** kwargs ) try : doc = next ( query ) return doc except StopIteration : return None def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" raise NotImplementedError ( \"No remove_docs method for JointStore\" ) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for JointStore Args: other: other JointStore to compare with \"\"\" if not isinstance ( other , JointStore ): return False fields = [ \"database\" , \"collection_names\" , \"host\" , \"port\" , \"main\" , \"merge_at_root\" , ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) last_updated : datetime property readonly \u00b6 Special last_updated for this JointStore that checks all underlying collections name : str property readonly \u00b6 Return a string representing this data source nonmain_names : List property readonly \u00b6 alll non-main collection names __eq__ ( self , other ) special \u00b6 Check equality for JointStore Parameters: Name Type Description Default other object other JointStore to compare with required Source code in maggma/stores/compound_stores.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for JointStore Args: other: other JointStore to compare with \"\"\" if not isinstance ( other , JointStore ): return False fields = [ \"database\" , \"collection_names\" , \"host\" , \"port\" , \"main\" , \"merge_at_root\" , ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) __init__ ( self , database , collection_names , host = 'localhost' , port = 27017 , username = '' , password = '' , main = None , merge_at_root = False , mongoclient_kwargs = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default database str The database name required collection_names List[str] list of all collections to join required host str Hostname for the database 'localhost' port int TCP port to connect to 27017 username str Username for the collection '' password str Password to connect with '' main Optional[str] name for the main collection if not specified this defaults to the first in collection_names list None Source code in maggma/stores/compound_stores.py def __init__ ( self , database : str , collection_names : List [ str ], host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , main : Optional [ str ] = None , merge_at_root : bool = False , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Args: database: The database name collection_names: list of all collections to join host: Hostname for the database port: TCP port to connect to username: Username for the collection password: Password to connect with main: name for the main collection if not specified this defaults to the first in collection_names list \"\"\" self . database = database self . collection_names = collection_names self . host = host self . port = port self . username = username self . password = password self . _coll = None # type: Any self . main = main or collection_names [ 0 ] self . merge_at_root = merge_at_root self . mongoclient_kwargs = mongoclient_kwargs or {} self . kwargs = kwargs super ( JointStore , self ) . __init__ ( ** kwargs ) close ( self ) \u00b6 Closes underlying database connections Source code in maggma/stores/compound_stores.py def close ( self ): \"\"\" Closes underlying database connections \"\"\" self . _collection . database . client . close () connect ( self , force_reset = False ) \u00b6 Connects the underlying Mongo database and all collection connections Parameters: Name Type Description Default force_reset bool whether to forcibly reset the connection False Source code in maggma/stores/compound_stores.py def connect ( self , force_reset : bool = False ): \"\"\" Connects the underlying Mongo database and all collection connections Args: force_reset: whether to forcibly reset the connection \"\"\" conn = ( MongoClient ( host = self . host , port = self . port , username = self . username , password = self . password , ** self . mongoclient_kwargs , ) if self . username != \"\" else MongoClient ( self . host , self . port , ** self . mongoclient_kwargs ) ) db = conn [ self . database ] self . _coll = db [ self . main ] self . _has_merge_objects = ( self . _collection . database . client . server_info ()[ \"version\" ] > \"3.6\" ) count ( self , criteria = None ) \u00b6 Counts the number of documents matching the query criteria Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None Source code in maggma/stores/compound_stores.py def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" pipeline = self . _get_pipeline ( criteria = criteria ) pipeline . append ({ \"$count\" : \"count\" }) agg = list ( self . _collection . aggregate ( pipeline )) return agg [ 0 ] . get ( \"count\" , 0 ) if len ( agg ) > 0 else 0 ensure_index ( self , key , unique = False , ** kwargs ) \u00b6 Can't ensure index for JointStore Source code in maggma/stores/compound_stores.py def ensure_index ( self , key , unique = False , ** kwargs ): \"\"\" Can't ensure index for JointStore \"\"\" raise NotImplementedError ( \"No ensure_index method for JointStore\" ) groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Simple grouping function that will group documents by keys. Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) Source code in maggma/stores/compound_stores.py def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: pipeline = self . _get_pipeline ( criteria = criteria , properties = properties , skip = skip , limit = limit ) if not isinstance ( keys , list ): keys = [ keys ] group_id = {} # type: Dict[str,Any] for key in keys : set_ ( group_id , key , \"$ {} \" . format ( key )) pipeline . append ({ \"$group\" : { \"_id\" : group_id , \"docs\" : { \"$push\" : \"$$ROOT\" }}}) agg = self . _collection . aggregate ( pipeline ) for d in agg : yield d [ \"_id\" ], d [ \"docs\" ] query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Queries the Store for a set of documents Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Source code in maggma/stores/compound_stores.py def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: pipeline = self . _get_pipeline ( criteria = criteria , properties = properties , skip = skip , limit = limit ) agg = self . _collection . aggregate ( pipeline ) for d in agg : yield d query_one ( self , criteria = None , properties = None , ** kwargs ) \u00b6 Get one document Parameters: Name Type Description Default properties properties to return in query None criteria filter for matching None kwargs kwargs for collection.aggregate {} Returns: Type Description single document Source code in maggma/stores/compound_stores.py def query_one ( self , criteria = None , properties = None , ** kwargs ): \"\"\" Get one document Args: properties: properties to return in query criteria: filter for matching kwargs: kwargs for collection.aggregate Returns: single document \"\"\" # TODO: maybe adding explicit limit in agg pipeline is better as below? # pipeline = self._get_pipeline(properties, criteria) # pipeline.append({\"$limit\": 1}) query = self . query ( criteria = criteria , properties = properties , ** kwargs ) try : doc = next ( query ) return doc except StopIteration : return None remove_docs ( self , criteria ) \u00b6 Remove docs matching the query dictionary Parameters: Name Type Description Default criteria Dict query dictionary to match required Source code in maggma/stores/compound_stores.py def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" raise NotImplementedError ( \"No remove_docs method for JointStore\" ) update ( self , docs , update_lu = True , key = None , ** kwargs ) \u00b6 Update documents into the underlying collections Not Implemented for JointStore Source code in maggma/stores/compound_stores.py def update ( self , docs , update_lu = True , key = None , ** kwargs ): \"\"\" Update documents into the underlying collections Not Implemented for JointStore \"\"\" raise NotImplementedError ( \"JointStore is a read-only store\" )","title":"Stores"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore","text":"A Store for access to a single or multiple JSON files Source code in maggma/stores/mongolike.py class JSONStore ( MemoryStore ): \"\"\" A Store for access to a single or multiple JSON files \"\"\" def __init__ ( self , paths : Union [ str , List [ str ]], read_only : bool = True , ** kwargs , ): \"\"\" Args: paths: paths for json files to turn into a Store read_only: whether this JSONStore is read only. When read_only=True, the JSONStore can still apply MongoDB-like writable operations (e.g. an update) because it behaves like a MemoryStore, but it will not write those changes to the file. On the other hand, if read_only=False (i.e., it is writeable), the JSON file will be automatically updated every time a write-like operation is performed. Note that when read_only=False, JSONStore only supports a single JSON file. If the file does not exist, it will be automatically created when the JSONStore is initialized. \"\"\" paths = paths if isinstance ( paths , ( list , tuple )) else [ paths ] self . paths = paths # file_writable overrides read_only for compatibility reasons if \"file_writable\" in kwargs : file_writable = kwargs . pop ( \"file_writable\" ) warnings . warn ( \"file_writable is deprecated; use read only instead.\" , DeprecationWarning , ) self . read_only = not file_writable if self . read_only != read_only : warnings . warn ( f \"Received conflicting keyword arguments file_writable= { file_writable } \" f \" and read_only= { read_only } . Setting read_only= { file_writable } .\" , UserWarning , ) else : self . read_only = read_only self . kwargs = kwargs if not self . read_only and len ( paths ) > 1 : raise RuntimeError ( \"Cannot instantiate file-writable JSONStore with multiple JSON files.\" ) # create the .json file if it does not exist if not self . read_only and not Path ( self . paths [ 0 ]) . exists (): with zopen ( self . paths [ 0 ], \"w\" ) as f : data : List [ dict ] = [] bytesdata = orjson . dumps ( data ) f . write ( bytesdata . decode ( \"utf-8\" )) super () . __init__ ( ** kwargs ) def connect ( self , force_reset = False ): \"\"\" Loads the files into the collection in memory \"\"\" super () . connect ( force_reset = force_reset ) for path in self . paths : objects = self . read_json_file ( path ) try : self . update ( objects ) except KeyError : raise KeyError ( f \"\"\" Key field ' { self . key } ' not found in { path . name } . This could mean that this JSONStore was initially created with a different key field. The keys found in the .json file are { list ( objects [ 0 ] . keys ()) } . Try re-initializing your JSONStore using one of these as the key arguments. \"\"\" ) def read_json_file ( self , path ) -> List : \"\"\" Helper method to read the contents of a JSON file and generate a list of docs. Args: path: Path to the JSON file to be read \"\"\" with zopen ( path ) as f : data = f . read () data = data . decode () if isinstance ( data , bytes ) else data objects = orjson . loads ( data ) objects = [ objects ] if not isinstance ( objects , list ) else objects return objects def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store. For a file-writable JSONStore, the json file is updated. Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" super () . update ( docs = docs , key = key ) if not self . read_only : self . update_json_file () def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary. For a file-writable JSONStore, the json file is updated. Args: criteria: query dictionary to match \"\"\" super () . remove_docs ( criteria = criteria ) if not self . read_only : self . update_json_file () def update_json_file ( self ): \"\"\" Updates the json file when a write-like operation is performed. \"\"\" with zopen ( self . paths [ 0 ], \"w\" ) as f : data = [ d for d in self . query ()] for d in data : d . pop ( \"_id\" ) bytesdata = orjson . dumps ( data ) f . write ( bytesdata . decode ( \"utf-8\" )) def __hash__ ( self ): return hash (( * self . paths , self . last_updated_field )) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for JSONStore Args: other: other JSONStore to compare with \"\"\" if not isinstance ( other , JSONStore ): return False fields = [ \"paths\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"JSONStore"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.__eq__","text":"Check equality for JSONStore Parameters: Name Type Description Default other object other JSONStore to compare with required Source code in maggma/stores/mongolike.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for JSONStore Args: other: other JSONStore to compare with \"\"\" if not isinstance ( other , JSONStore ): return False fields = [ \"paths\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.__init__","text":"Parameters: Name Type Description Default paths Union[str, List[str]] paths for json files to turn into a Store required read_only bool whether this JSONStore is read only. When read_only=True, the JSONStore can still apply MongoDB-like writable operations (e.g. an update) because it behaves like a MemoryStore, but it will not write those changes to the file. On the other hand, if read_only=False (i.e., it is writeable), the JSON file will be automatically updated every time a write-like operation is performed. Note that when read_only = False , JSONStore only supports a single JSON file . If the file does not exist , it will be automatically created when the JSONStore is initialized . True Source code in maggma/stores/mongolike.py def __init__ ( self , paths : Union [ str , List [ str ]], read_only : bool = True , ** kwargs , ): \"\"\" Args: paths: paths for json files to turn into a Store read_only: whether this JSONStore is read only. When read_only=True, the JSONStore can still apply MongoDB-like writable operations (e.g. an update) because it behaves like a MemoryStore, but it will not write those changes to the file. On the other hand, if read_only=False (i.e., it is writeable), the JSON file will be automatically updated every time a write-like operation is performed. Note that when read_only=False, JSONStore only supports a single JSON file. If the file does not exist, it will be automatically created when the JSONStore is initialized. \"\"\" paths = paths if isinstance ( paths , ( list , tuple )) else [ paths ] self . paths = paths # file_writable overrides read_only for compatibility reasons if \"file_writable\" in kwargs : file_writable = kwargs . pop ( \"file_writable\" ) warnings . warn ( \"file_writable is deprecated; use read only instead.\" , DeprecationWarning , ) self . read_only = not file_writable if self . read_only != read_only : warnings . warn ( f \"Received conflicting keyword arguments file_writable= { file_writable } \" f \" and read_only= { read_only } . Setting read_only= { file_writable } .\" , UserWarning , ) else : self . read_only = read_only self . kwargs = kwargs if not self . read_only and len ( paths ) > 1 : raise RuntimeError ( \"Cannot instantiate file-writable JSONStore with multiple JSON files.\" ) # create the .json file if it does not exist if not self . read_only and not Path ( self . paths [ 0 ]) . exists (): with zopen ( self . paths [ 0 ], \"w\" ) as f : data : List [ dict ] = [] bytesdata = orjson . dumps ( data ) f . write ( bytesdata . decode ( \"utf-8\" )) super () . __init__ ( ** kwargs )","title":"__init__()"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.connect","text":"Loads the files into the collection in memory Source code in maggma/stores/mongolike.py def connect ( self , force_reset = False ): \"\"\" Loads the files into the collection in memory \"\"\" super () . connect ( force_reset = force_reset ) for path in self . paths : objects = self . read_json_file ( path ) try : self . update ( objects ) except KeyError : raise KeyError ( f \"\"\" Key field ' { self . key } ' not found in { path . name } . This could mean that this JSONStore was initially created with a different key field. The keys found in the .json file are { list ( objects [ 0 ] . keys ()) } . Try re-initializing your JSONStore using one of these as the key arguments. \"\"\" )","title":"connect()"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.read_json_file","text":"Helper method to read the contents of a JSON file and generate a list of docs. Parameters: Name Type Description Default path Path to the JSON file to be read required Source code in maggma/stores/mongolike.py def read_json_file ( self , path ) -> List : \"\"\" Helper method to read the contents of a JSON file and generate a list of docs. Args: path: Path to the JSON file to be read \"\"\" with zopen ( path ) as f : data = f . read () data = data . decode () if isinstance ( data , bytes ) else data objects = orjson . loads ( data ) objects = [ objects ] if not isinstance ( objects , list ) else objects return objects","title":"read_json_file()"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.remove_docs","text":"Remove docs matching the query dictionary. For a file-writable JSONStore, the json file is updated. Parameters: Name Type Description Default criteria Dict query dictionary to match required Source code in maggma/stores/mongolike.py def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary. For a file-writable JSONStore, the json file is updated. Args: criteria: query dictionary to match \"\"\" super () . remove_docs ( criteria = criteria ) if not self . read_only : self . update_json_file ()","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.update","text":"Update documents into the Store. For a file-writable JSONStore, the json file is updated. Parameters: Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Source code in maggma/stores/mongolike.py def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store. For a file-writable JSONStore, the json file is updated. Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" super () . update ( docs = docs , key = key ) if not self . read_only : self . update_json_file ()","title":"update()"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.update_json_file","text":"Updates the json file when a write-like operation is performed. Source code in maggma/stores/mongolike.py def update_json_file ( self ): \"\"\" Updates the json file when a write-like operation is performed. \"\"\" with zopen ( self . paths [ 0 ], \"w\" ) as f : data = [ d for d in self . query ()] for d in data : d . pop ( \"_id\" ) bytesdata = orjson . dumps ( data ) f . write ( bytesdata . decode ( \"utf-8\" ))","title":"update_json_file()"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore","text":"An in-memory Store that functions similarly to a MongoStore Source code in maggma/stores/mongolike.py class MemoryStore ( MongoStore ): \"\"\" An in-memory Store that functions similarly to a MongoStore \"\"\" def __init__ ( self , collection_name : str = \"memory_db\" , ** kwargs ): \"\"\" Initializes the Memory Store Args: collection_name: name for the collection in memory \"\"\" self . collection_name = collection_name self . _coll = None self . kwargs = kwargs super ( MongoStore , self ) . __init__ ( ** kwargs ) # noqa def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if self . _coll is None or force_reset : self . _coll = mongomock . MongoClient () . db [ self . name ] def close ( self ): \"\"\"Close up all collections\"\"\" self . _coll . database . client . close () @property def name ( self ): \"\"\"Name for the store\"\"\" return f \"mem:// { self . collection_name } \" def __hash__ ( self ): \"\"\"Hash for the store\"\"\" return hash (( self . name , self . last_updated_field )) def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (key, list of elemnts) \"\"\" keys = keys if isinstance ( keys , list ) else [ keys ] if properties is None : properties = [] if isinstance ( properties , dict ): properties = list ( properties . keys ()) data = [ doc for doc in self . query ( properties = keys + properties , criteria = criteria ) if all ( has ( doc , k ) for k in keys ) ] def grouping_keys ( doc ): return tuple ( get ( doc , k ) for k in keys ) for vals , group in groupby ( sorted ( data , key = grouping_keys ), key = grouping_keys ): doc = {} # type: Dict[Any,Any] for k , v in zip ( keys , vals ): set_ ( doc , k , v ) yield doc , list ( group ) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MemoryStore other: other MemoryStore to compare with \"\"\" if not isinstance ( other , MemoryStore ): return False fields = [ \"collection_name\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"MemoryStore"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.name","text":"Name for the store","title":"name"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.__eq__","text":"Check equality for MemoryStore other: other MemoryStore to compare with Source code in maggma/stores/mongolike.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MemoryStore other: other MemoryStore to compare with \"\"\" if not isinstance ( other , MemoryStore ): return False fields = [ \"collection_name\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.__hash__","text":"Hash for the store Source code in maggma/stores/mongolike.py def __hash__ ( self ): \"\"\"Hash for the store\"\"\" return hash (( self . name , self . last_updated_field ))","title":"__hash__()"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.__init__","text":"Initializes the Memory Store Parameters: Name Type Description Default collection_name str name for the collection in memory 'memory_db' Source code in maggma/stores/mongolike.py def __init__ ( self , collection_name : str = \"memory_db\" , ** kwargs ): \"\"\" Initializes the Memory Store Args: collection_name: name for the collection in memory \"\"\" self . collection_name = collection_name self . _coll = None self . kwargs = kwargs super ( MongoStore , self ) . __init__ ( ** kwargs ) # noqa","title":"__init__()"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.close","text":"Close up all collections Source code in maggma/stores/mongolike.py def close ( self ): \"\"\"Close up all collections\"\"\" self . _coll . database . client . close ()","title":"close()"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.connect","text":"Connect to the source data Source code in maggma/stores/mongolike.py def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if self . _coll is None or force_reset : self . _coll = mongomock . MongoClient () . db [ self . name ]","title":"connect()"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.groupby","text":"Simple grouping function that will group documents by keys. Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (key, list of elemnts) Source code in maggma/stores/mongolike.py def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (key, list of elemnts) \"\"\" keys = keys if isinstance ( keys , list ) else [ keys ] if properties is None : properties = [] if isinstance ( properties , dict ): properties = list ( properties . keys ()) data = [ doc for doc in self . query ( properties = keys + properties , criteria = criteria ) if all ( has ( doc , k ) for k in keys ) ] def grouping_keys ( doc ): return tuple ( get ( doc , k ) for k in keys ) for vals , group in groupby ( sorted ( data , key = grouping_keys ), key = grouping_keys ): doc = {} # type: Dict[Any,Any] for k , v in zip ( keys , vals ): set_ ( doc , k , v ) yield doc , list ( group )","title":"groupby()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore","text":"A Store that connects to a Mongo collection Source code in maggma/stores/mongolike.py class MongoStore ( Store ): \"\"\" A Store that connects to a Mongo collection \"\"\" def __init__ ( self , database : str , collection_name : str , host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , ssh_tunnel : Optional [ SSHTunnel ] = None , safe_update : bool = False , auth_source : Optional [ str ] = None , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Args: database: The database name collection_name: The collection name host: Hostname for the database port: TCP port to connect to username: Username for the collection password: Password to connect with safe_update: fail gracefully on DocumentTooLarge errors on update auth_source: The database to authenticate on. Defaults to the database name. \"\"\" self . database = database self . collection_name = collection_name self . host = host self . port = port self . username = username self . password = password self . ssh_tunnel = ssh_tunnel self . safe_update = safe_update self . _coll = None # type: Any self . kwargs = kwargs if auth_source is None : auth_source = self . database self . auth_source = auth_source self . mongoclient_kwargs = mongoclient_kwargs or {} super () . __init__ ( ** kwargs ) @property def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" return f \"mongo:// { self . host } / { self . database } / { self . collection_name } \" def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if self . _coll is None or force_reset : if self . ssh_tunnel is None : host = self . host port = self . port else : self . ssh_tunnel . start () host , port = self . ssh_tunnel . local_address conn = ( MongoClient ( host = host , port = port , username = self . username , password = self . password , authSource = self . auth_source , ** self . mongoclient_kwargs , ) if self . username != \"\" else MongoClient ( host , port , ** self . mongoclient_kwargs ) ) db = conn [ self . database ] self . _coll = db [ self . collection_name ] def __hash__ ( self ) -> int : \"\"\"Hash for MongoStore\"\"\" return hash (( self . database , self . collection_name , self . last_updated_field )) @classmethod def from_db_file ( cls , filename : str , ** kwargs ): \"\"\" Convenience method to construct MongoStore from db_file from old QueryEngine format \"\"\" kwargs = loadfn ( filename ) if \"collection\" in kwargs : kwargs [ \"collection_name\" ] = kwargs . pop ( \"collection\" ) # Get rid of aliases from traditional query engine db docs kwargs . pop ( \"aliases\" , None ) return cls ( ** kwargs ) @classmethod def from_launchpad_file ( cls , lp_file , collection_name , ** kwargs ): \"\"\" Convenience method to construct MongoStore from a launchpad file Note: A launchpad file is a special formatted yaml file used in fireworks Returns: \"\"\" with open ( lp_file , \"r\" ) as f : lp_creds = yaml . load ( f , Loader = yaml . FullLoader ) db_creds = lp_creds . copy () db_creds [ \"database\" ] = db_creds [ \"name\" ] for key in list ( db_creds . keys ()): if key not in [ \"database\" , \"host\" , \"port\" , \"username\" , \"password\" ]: db_creds . pop ( key ) db_creds [ \"collection_name\" ] = collection_name return cls ( ** db_creds , ** kwargs ) def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria or {} try : distinct_vals = self . _collection . distinct ( field , criteria ) except ( OperationFailure , DocumentTooLarge ): distinct_vals = [ d [ \"_id\" ] for d in self . _collection . aggregate ( [{ \"$match\" : criteria }, { \"$group\" : { \"_id\" : f \"$ { field } \" }}] ) ] if all ( isinstance ( d , list ) for d in filter ( None , distinct_vals )): # type: ignore distinct_vals = list ( chain . from_iterable ( filter ( None , distinct_vals ))) return distinct_vals if distinct_vals is not None else [] def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (key, list of docs) \"\"\" pipeline = [] if isinstance ( keys , str ): keys = [ keys ] if properties is None : properties = [] if isinstance ( properties , dict ): properties = list ( properties . keys ()) if criteria is not None : pipeline . append ({ \"$match\" : criteria }) if len ( properties ) > 0 : pipeline . append ({ \"$project\" : { p : 1 for p in properties + keys }}) alpha = \"abcdefghijklmnopqrstuvwxyz\" group_id = { letter : f \"$ { key } \" for letter , key in zip ( alpha , keys )} pipeline . append ({ \"$group\" : { \"_id\" : group_id , \"docs\" : { \"$push\" : \"$$ROOT\" }}}) for d in self . _collection . aggregate ( pipeline , allowDiskUse = True ): id_doc = {} # type: Dict[str,Any] for letter , key in group_id . items (): if has ( d [ \"_id\" ], letter ): set_ ( id_doc , key [ 1 :], d [ \"_id\" ][ letter ]) yield ( id_doc , d [ \"docs\" ]) @classmethod def from_collection ( cls , collection ): \"\"\" Generates a MongoStore from a pymongo collection object This is not a fully safe operation as it gives dummy information to the MongoStore As a result, this will not serialize and can not reset its connection Args: collection: the PyMongo collection to create a MongoStore around \"\"\" # TODO: How do we make this safer? coll_name = collection . name db_name = collection . database . name store = cls ( db_name , coll_name ) store . _coll = collection return store @property def _collection ( self ): \"\"\"Property referring to underlying pymongo collection\"\"\" if self . _coll is None : raise StoreError ( \"Must connect Mongo-like store before attemping to use it\" ) return self . _coll def count ( self , criteria : Optional [ Dict ] = None , hint : Optional [ Dict [ str , Union [ Sort , int ]]] = None , ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in hint: Dictionary of indexes to use as hints for query optimizer. Keys are field names and values are 1 for ascending or -1 for descending. \"\"\" criteria = criteria if criteria else {} hint_list = ( [ ( k , Sort ( v ) . value ) if isinstance ( v , int ) else ( k , v . value ) for k , v in hint . items () ] if hint else None ) if hint_list is not None : # pragma: no cover return self . _collection . count_documents ( filter = criteria , hint = hint_list ) return self . _collection . count_documents ( filter = criteria ) def query ( # type: ignore self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , hint : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. hint: Dictionary of indexes to use as hints for query optimizer. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( properties , list ): properties = { p : 1 for p in properties } sort_list = ( [ ( k , Sort ( v ) . value ) if isinstance ( v , int ) else ( k , v . value ) for k , v in sort . items () ] if sort else None ) hint_list = ( [ ( k , Sort ( v ) . value ) if isinstance ( v , int ) else ( k , v . value ) for k , v in hint . items () ] if hint else None ) for d in self . _collection . find ( filter = criteria , projection = properties , skip = skip , limit = limit , sort = sort_list , hint = hint_list , ): yield d def ensure_index ( self , key : str , unique : Optional [ bool ] = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" if confirm_field_index ( self . _collection , key ): return True else : try : self . _collection . create_index ( key , unique = unique , background = True ) return True except Exception : return False def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" requests = [] if not isinstance ( docs , list ): docs = [ docs ] for d in docs : d = jsanitize ( d , allow_bson = True ) # document-level validation is optional validates = True if self . validator : validates = self . validator . is_valid ( d ) if not validates : if self . validator . strict : raise ValueError ( self . validator . validation_errors ( d )) else : self . logger . error ( self . validator . validation_errors ( d )) if validates : key = key or self . key if isinstance ( key , list ): search_doc = { k : d [ k ] for k in key } else : search_doc = { key : d [ key ]} requests . append ( ReplaceOne ( search_doc , d , upsert = True )) if len ( requests ) > 0 : try : self . _collection . bulk_write ( requests , ordered = False ) except ( OperationFailure , DocumentTooLarge ) as e : if self . safe_update : for req in requests : req . _filter try : self . _collection . bulk_write ([ req ], ordered = False ) except ( OperationFailure , DocumentTooLarge ): self . logger . error ( f \"Could not upload document for { req . _filter } as it was too large for Mongo\" ) else : raise e def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" self . _collection . delete_many ( filter = criteria ) def close ( self ): \"\"\"Close up all collections\"\"\" self . _collection . database . client . close () self . _coll = None if self . ssh_tunnel is not None : self . ssh_tunnel . stop () def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MongoStore other: other mongostore to compare with \"\"\" if not isinstance ( other , MongoStore ): return False fields = [ \"database\" , \"collection_name\" , \"host\" , \"port\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"MongoStore"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.name","text":"Return a string representing this data source","title":"name"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.__eq__","text":"Check equality for MongoStore other: other mongostore to compare with Source code in maggma/stores/mongolike.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MongoStore other: other mongostore to compare with \"\"\" if not isinstance ( other , MongoStore ): return False fields = [ \"database\" , \"collection_name\" , \"host\" , \"port\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.__hash__","text":"Hash for MongoStore Source code in maggma/stores/mongolike.py def __hash__ ( self ) -> int : \"\"\"Hash for MongoStore\"\"\" return hash (( self . database , self . collection_name , self . last_updated_field ))","title":"__hash__()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.__init__","text":"Parameters: Name Type Description Default database str The database name required collection_name str The collection name required host str Hostname for the database 'localhost' port int TCP port to connect to 27017 username str Username for the collection '' password str Password to connect with '' safe_update bool fail gracefully on DocumentTooLarge errors on update False auth_source Optional[str] The database to authenticate on. Defaults to the database name. None Source code in maggma/stores/mongolike.py def __init__ ( self , database : str , collection_name : str , host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , ssh_tunnel : Optional [ SSHTunnel ] = None , safe_update : bool = False , auth_source : Optional [ str ] = None , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Args: database: The database name collection_name: The collection name host: Hostname for the database port: TCP port to connect to username: Username for the collection password: Password to connect with safe_update: fail gracefully on DocumentTooLarge errors on update auth_source: The database to authenticate on. Defaults to the database name. \"\"\" self . database = database self . collection_name = collection_name self . host = host self . port = port self . username = username self . password = password self . ssh_tunnel = ssh_tunnel self . safe_update = safe_update self . _coll = None # type: Any self . kwargs = kwargs if auth_source is None : auth_source = self . database self . auth_source = auth_source self . mongoclient_kwargs = mongoclient_kwargs or {} super () . __init__ ( ** kwargs )","title":"__init__()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.close","text":"Close up all collections Source code in maggma/stores/mongolike.py def close ( self ): \"\"\"Close up all collections\"\"\" self . _collection . database . client . close () self . _coll = None if self . ssh_tunnel is not None : self . ssh_tunnel . stop ()","title":"close()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.connect","text":"Connect to the source data Source code in maggma/stores/mongolike.py def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if self . _coll is None or force_reset : if self . ssh_tunnel is None : host = self . host port = self . port else : self . ssh_tunnel . start () host , port = self . ssh_tunnel . local_address conn = ( MongoClient ( host = host , port = port , username = self . username , password = self . password , authSource = self . auth_source , ** self . mongoclient_kwargs , ) if self . username != \"\" else MongoClient ( host , port , ** self . mongoclient_kwargs ) ) db = conn [ self . database ] self . _coll = db [ self . collection_name ]","title":"connect()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.count","text":"Counts the number of documents matching the query criteria Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None hint Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of indexes to use as hints for query optimizer. Keys are field names and values are 1 for ascending or -1 for descending. None Source code in maggma/stores/mongolike.py def count ( self , criteria : Optional [ Dict ] = None , hint : Optional [ Dict [ str , Union [ Sort , int ]]] = None , ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in hint: Dictionary of indexes to use as hints for query optimizer. Keys are field names and values are 1 for ascending or -1 for descending. \"\"\" criteria = criteria if criteria else {} hint_list = ( [ ( k , Sort ( v ) . value ) if isinstance ( v , int ) else ( k , v . value ) for k , v in hint . items () ] if hint else None ) if hint_list is not None : # pragma: no cover return self . _collection . count_documents ( filter = criteria , hint = hint_list ) return self . _collection . count_documents ( filter = criteria )","title":"count()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.distinct","text":"Get all distinct values for a field Parameters: Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None Source code in maggma/stores/mongolike.py def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria or {} try : distinct_vals = self . _collection . distinct ( field , criteria ) except ( OperationFailure , DocumentTooLarge ): distinct_vals = [ d [ \"_id\" ] for d in self . _collection . aggregate ( [{ \"$match\" : criteria }, { \"$group\" : { \"_id\" : f \"$ { field } \" }}] ) ] if all ( isinstance ( d , list ) for d in filter ( None , distinct_vals )): # type: ignore distinct_vals = list ( chain . from_iterable ( filter ( None , distinct_vals ))) return distinct_vals if distinct_vals is not None else []","title":"distinct()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.ensure_index","text":"Tries to create an index and return true if it suceeded Parameters: Name Type Description Default key str single key to index required unique Optional[bool] Whether or not this index contains only unique keys False Returns: Type Description bool bool indicating if the index exists/was created Source code in maggma/stores/mongolike.py def ensure_index ( self , key : str , unique : Optional [ bool ] = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" if confirm_field_index ( self . _collection , key ): return True else : try : self . _collection . create_index ( key , unique = unique , background = True ) return True except Exception : return False","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.from_collection","text":"Generates a MongoStore from a pymongo collection object This is not a fully safe operation as it gives dummy information to the MongoStore As a result, this will not serialize and can not reset its connection Parameters: Name Type Description Default collection the PyMongo collection to create a MongoStore around required Source code in maggma/stores/mongolike.py @classmethod def from_collection ( cls , collection ): \"\"\" Generates a MongoStore from a pymongo collection object This is not a fully safe operation as it gives dummy information to the MongoStore As a result, this will not serialize and can not reset its connection Args: collection: the PyMongo collection to create a MongoStore around \"\"\" # TODO: How do we make this safer? coll_name = collection . name db_name = collection . database . name store = cls ( db_name , coll_name ) store . _coll = collection return store","title":"from_collection()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.from_db_file","text":"Convenience method to construct MongoStore from db_file from old QueryEngine format Source code in maggma/stores/mongolike.py @classmethod def from_db_file ( cls , filename : str , ** kwargs ): \"\"\" Convenience method to construct MongoStore from db_file from old QueryEngine format \"\"\" kwargs = loadfn ( filename ) if \"collection\" in kwargs : kwargs [ \"collection_name\" ] = kwargs . pop ( \"collection\" ) # Get rid of aliases from traditional query engine db docs kwargs . pop ( \"aliases\" , None ) return cls ( ** kwargs )","title":"from_db_file()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.from_launchpad_file","text":"Convenience method to construct MongoStore from a launchpad file Note: A launchpad file is a special formatted yaml file used in fireworks Source code in maggma/stores/mongolike.py @classmethod def from_launchpad_file ( cls , lp_file , collection_name , ** kwargs ): \"\"\" Convenience method to construct MongoStore from a launchpad file Note: A launchpad file is a special formatted yaml file used in fireworks Returns: \"\"\" with open ( lp_file , \"r\" ) as f : lp_creds = yaml . load ( f , Loader = yaml . FullLoader ) db_creds = lp_creds . copy () db_creds [ \"database\" ] = db_creds [ \"name\" ] for key in list ( db_creds . keys ()): if key not in [ \"database\" , \"host\" , \"port\" , \"username\" , \"password\" ]: db_creds . pop ( key ) db_creds [ \"collection_name\" ] = collection_name return cls ( ** db_creds , ** kwargs )","title":"from_launchpad_file()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.groupby","text":"Simple grouping function that will group documents by keys. Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (key, list of docs) Source code in maggma/stores/mongolike.py def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (key, list of docs) \"\"\" pipeline = [] if isinstance ( keys , str ): keys = [ keys ] if properties is None : properties = [] if isinstance ( properties , dict ): properties = list ( properties . keys ()) if criteria is not None : pipeline . append ({ \"$match\" : criteria }) if len ( properties ) > 0 : pipeline . append ({ \"$project\" : { p : 1 for p in properties + keys }}) alpha = \"abcdefghijklmnopqrstuvwxyz\" group_id = { letter : f \"$ { key } \" for letter , key in zip ( alpha , keys )} pipeline . append ({ \"$group\" : { \"_id\" : group_id , \"docs\" : { \"$push\" : \"$$ROOT\" }}}) for d in self . _collection . aggregate ( pipeline , allowDiskUse = True ): id_doc = {} # type: Dict[str,Any] for letter , key in group_id . items (): if has ( d [ \"_id\" ], letter ): set_ ( id_doc , key [ 1 :], d [ \"_id\" ][ letter ]) yield ( id_doc , d [ \"docs\" ])","title":"groupby()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.query","text":"Queries the Store for a set of documents Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None hint Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of indexes to use as hints for query optimizer. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Source code in maggma/stores/mongolike.py def query ( # type: ignore self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , hint : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. hint: Dictionary of indexes to use as hints for query optimizer. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( properties , list ): properties = { p : 1 for p in properties } sort_list = ( [ ( k , Sort ( v ) . value ) if isinstance ( v , int ) else ( k , v . value ) for k , v in sort . items () ] if sort else None ) hint_list = ( [ ( k , Sort ( v ) . value ) if isinstance ( v , int ) else ( k , v . value ) for k , v in hint . items () ] if hint else None ) for d in self . _collection . find ( filter = criteria , projection = properties , skip = skip , limit = limit , sort = sort_list , hint = hint_list , ): yield d","title":"query()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.remove_docs","text":"Remove docs matching the query dictionary Parameters: Name Type Description Default criteria Dict query dictionary to match required Source code in maggma/stores/mongolike.py def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" self . _collection . delete_many ( filter = criteria )","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.update","text":"Update documents into the Store Parameters: Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Source code in maggma/stores/mongolike.py def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" requests = [] if not isinstance ( docs , list ): docs = [ docs ] for d in docs : d = jsanitize ( d , allow_bson = True ) # document-level validation is optional validates = True if self . validator : validates = self . validator . is_valid ( d ) if not validates : if self . validator . strict : raise ValueError ( self . validator . validation_errors ( d )) else : self . logger . error ( self . validator . validation_errors ( d )) if validates : key = key or self . key if isinstance ( key , list ): search_doc = { k : d [ k ] for k in key } else : search_doc = { key : d [ key ]} requests . append ( ReplaceOne ( search_doc , d , upsert = True )) if len ( requests ) > 0 : try : self . _collection . bulk_write ( requests , ordered = False ) except ( OperationFailure , DocumentTooLarge ) as e : if self . safe_update : for req in requests : req . _filter try : self . _collection . bulk_write ([ req ], ordered = False ) except ( OperationFailure , DocumentTooLarge ): self . logger . error ( f \"Could not upload document for { req . _filter } as it was too large for Mongo\" ) else : raise e","title":"update()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoURIStore","text":"A Store that connects to a Mongo collection via a URI This is expected to be a special mongodb+srv:// URIs that include client parameters via TXT records Source code in maggma/stores/mongolike.py class MongoURIStore ( MongoStore ): \"\"\" A Store that connects to a Mongo collection via a URI This is expected to be a special mongodb+srv:// URIs that include client parameters via TXT records \"\"\" def __init__ ( self , uri : str , collection_name : str , database : str = None , ssh_tunnel : Optional [ SSHTunnel ] = None , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Args: uri: MongoDB+SRV URI database: database to connect to collection_name: The collection name \"\"\" self . uri = uri self . ssh_tunnel = ssh_tunnel self . mongoclient_kwargs = mongoclient_kwargs or {} # parse the dbname from the uri if database is None : d_uri = uri_parser . parse_uri ( uri ) if d_uri [ \"database\" ] is None : raise ConfigurationError ( \"If database name is not supplied, a database must be set in the uri\" ) self . database = d_uri [ \"database\" ] else : self . database = database self . collection_name = collection_name self . kwargs = kwargs self . _coll = None super ( MongoStore , self ) . __init__ ( ** kwargs ) # lgtm @property def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" # TODO: This is not very safe since it exposes the username/password info return self . uri def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if self . _coll is None or force_reset : # pragma: no cover conn = MongoClient ( self . uri , ** self . mongoclient_kwargs ) db = conn [ self . database ] self . _coll = db [ self . collection_name ]","title":"MongoURIStore"},{"location":"reference/stores/#maggma.stores.mongolike.MongoURIStore.name","text":"Return a string representing this data source","title":"name"},{"location":"reference/stores/#maggma.stores.mongolike.MongoURIStore.__init__","text":"Parameters: Name Type Description Default uri str MongoDB+SRV URI required database str database to connect to None collection_name str The collection name required Source code in maggma/stores/mongolike.py def __init__ ( self , uri : str , collection_name : str , database : str = None , ssh_tunnel : Optional [ SSHTunnel ] = None , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Args: uri: MongoDB+SRV URI database: database to connect to collection_name: The collection name \"\"\" self . uri = uri self . ssh_tunnel = ssh_tunnel self . mongoclient_kwargs = mongoclient_kwargs or {} # parse the dbname from the uri if database is None : d_uri = uri_parser . parse_uri ( uri ) if d_uri [ \"database\" ] is None : raise ConfigurationError ( \"If database name is not supplied, a database must be set in the uri\" ) self . database = d_uri [ \"database\" ] else : self . database = database self . collection_name = collection_name self . kwargs = kwargs self . _coll = None super ( MongoStore , self ) . __init__ ( ** kwargs ) # lgtm","title":"__init__()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoURIStore.connect","text":"Connect to the source data Source code in maggma/stores/mongolike.py def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if self . _coll is None or force_reset : # pragma: no cover conn = MongoClient ( self . uri , ** self . mongoclient_kwargs ) db = conn [ self . database ] self . _coll = db [ self . collection_name ]","title":"connect()"},{"location":"reference/stores/#maggma.stores.mongolike.MontyStore","text":"A MongoDB compatible store that uses on disk files for storage. This is handled under the hood using MontyDB. A number of on-disk storage options are available but MontyDB provides a mongo style interface for all options. The options include: sqlite: Uses an sqlite database to store documents. lightning: Uses Lightning Memory-Mapped Database (LMDB) for storage. This can provide fast read and write times but requires lmdb to be installed (in most cases this can be achieved using pip install lmdb ). flatfile: Uses a system of flat json files. This is not recommended as multiple simultaneous connections to the store will not work correctly. See the MontyDB repository for more information: https://github.com/davidlatwe/montydb Source code in maggma/stores/mongolike.py class MontyStore ( MemoryStore ): \"\"\" A MongoDB compatible store that uses on disk files for storage. This is handled under the hood using MontyDB. A number of on-disk storage options are available but MontyDB provides a mongo style interface for all options. The options include: - sqlite: Uses an sqlite database to store documents. - lightning: Uses Lightning Memory-Mapped Database (LMDB) for storage. This can provide fast read and write times but requires lmdb to be installed (in most cases this can be achieved using ``pip install lmdb``). - flatfile: Uses a system of flat json files. This is not recommended as multiple simultaneous connections to the store will not work correctly. See the MontyDB repository for more information: https://github.com/davidlatwe/montydb \"\"\" def __init__ ( self , collection_name , database_path : str = None , database_name : str = \"db\" , storage : str = \"sqlite\" , storage_kwargs : Optional [ dict ] = None , client_kwargs : Optional [ dict ] = None , ** kwargs , ): \"\"\" Initializes the Monty Store. Args: collection_name: Name for the collection. database_path: Path to on-disk database files. If None, the current working directory will be used. database_name: The database name. storage: The storage type. Options include \"sqlite\", \"lightning\", \"flatfile\". storage_kwargs: Keyword arguments passed to ``montydb.set_storage``. client_kwargs: Keyword arguments passed to the ``montydb.MontyClient`` constructor. **kwargs: Additional keyword arguments passed to the Store constructor. \"\"\" if database_path is None : database_path = str ( Path . cwd ()) self . database_path = database_path self . database_name = database_name self . collection_name = collection_name self . _coll = None self . ssh_tunnel = None # This is to fix issues with the tunnel on close self . kwargs = kwargs self . storage = storage self . storage_kwargs = storage_kwargs or { \"use_bson\" : True , \"monty_version\" : \"4.0\" , } self . client_kwargs = client_kwargs or {} super ( MongoStore , self ) . __init__ ( ** kwargs ) # noqa def connect ( self , force_reset : bool = False ): \"\"\" Connect to the database store. Args: force_reset: Force connection reset. \"\"\" from montydb import set_storage , MontyClient set_storage ( self . database_path , storage = self . storage , ** self . storage_kwargs ) client = MontyClient ( self . database_path , ** self . client_kwargs ) if not self . _coll or force_reset : self . _coll = client [ \"db\" ][ self . collection_name ] @property def name ( self ) -> str : \"\"\"Return a string representing this data source.\"\"\" return f \"monty:// { self . database_path } / { self . database } / { self . collection_name } \" def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store. Args: docs: The document or list of documents to update. key: Field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used. \"\"\" if not isinstance ( docs , list ): docs = [ docs ] for d in docs : d = jsanitize ( d , allow_bson = True ) # document-level validation is optional validates = True if self . validator : validates = self . validator . is_valid ( d ) if not validates : if self . validator . strict : raise ValueError ( self . validator . validation_errors ( d )) else : self . logger . error ( self . validator . validation_errors ( d )) if validates : key = key or self . key if isinstance ( key , list ): search_doc = { k : d [ k ] for k in key } else : search_doc = { key : d [ key ]} self . _collection . replace_one ( search_doc , d , upsert = True )","title":"MontyStore"},{"location":"reference/stores/#maggma.stores.mongolike.MontyStore.name","text":"Return a string representing this data source.","title":"name"},{"location":"reference/stores/#maggma.stores.mongolike.MontyStore.__init__","text":"Initializes the Monty Store. Parameters: Name Type Description Default collection_name Name for the collection. required database_path str Path to on-disk database files. If None, the current working directory will be used. None database_name str The database name. 'db' storage str The storage type. Options include \"sqlite\", \"lightning\", \"flatfile\". 'sqlite' storage_kwargs Optional[dict] Keyword arguments passed to montydb.set_storage . None client_kwargs Optional[dict] Keyword arguments passed to the montydb.MontyClient constructor. None **kwargs Additional keyword arguments passed to the Store constructor. {} Source code in maggma/stores/mongolike.py def __init__ ( self , collection_name , database_path : str = None , database_name : str = \"db\" , storage : str = \"sqlite\" , storage_kwargs : Optional [ dict ] = None , client_kwargs : Optional [ dict ] = None , ** kwargs , ): \"\"\" Initializes the Monty Store. Args: collection_name: Name for the collection. database_path: Path to on-disk database files. If None, the current working directory will be used. database_name: The database name. storage: The storage type. Options include \"sqlite\", \"lightning\", \"flatfile\". storage_kwargs: Keyword arguments passed to ``montydb.set_storage``. client_kwargs: Keyword arguments passed to the ``montydb.MontyClient`` constructor. **kwargs: Additional keyword arguments passed to the Store constructor. \"\"\" if database_path is None : database_path = str ( Path . cwd ()) self . database_path = database_path self . database_name = database_name self . collection_name = collection_name self . _coll = None self . ssh_tunnel = None # This is to fix issues with the tunnel on close self . kwargs = kwargs self . storage = storage self . storage_kwargs = storage_kwargs or { \"use_bson\" : True , \"monty_version\" : \"4.0\" , } self . client_kwargs = client_kwargs or {} super ( MongoStore , self ) . __init__ ( ** kwargs ) # noqa","title":"__init__()"},{"location":"reference/stores/#maggma.stores.mongolike.MontyStore.connect","text":"Connect to the database store. Parameters: Name Type Description Default force_reset bool Force connection reset. False Source code in maggma/stores/mongolike.py def connect ( self , force_reset : bool = False ): \"\"\" Connect to the database store. Args: force_reset: Force connection reset. \"\"\" from montydb import set_storage , MontyClient set_storage ( self . database_path , storage = self . storage , ** self . storage_kwargs ) client = MontyClient ( self . database_path , ** self . client_kwargs ) if not self . _coll or force_reset : self . _coll = client [ \"db\" ][ self . collection_name ]","title":"connect()"},{"location":"reference/stores/#maggma.stores.mongolike.MontyStore.update","text":"Update documents into the Store. Parameters: Name Type Description Default docs Union[List[Dict], Dict] The document or list of documents to update. required key Union[List, str] Field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used. None Source code in maggma/stores/mongolike.py def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store. Args: docs: The document or list of documents to update. key: Field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used. \"\"\" if not isinstance ( docs , list ): docs = [ docs ] for d in docs : d = jsanitize ( d , allow_bson = True ) # document-level validation is optional validates = True if self . validator : validates = self . validator . is_valid ( d ) if not validates : if self . validator . strict : raise ValueError ( self . validator . validation_errors ( d )) else : self . logger . error ( self . validator . validation_errors ( d )) if validates : key = key or self . key if isinstance ( key , list ): search_doc = { k : d [ k ] for k in key } else : search_doc = { key : d [ key ]} self . _collection . replace_one ( search_doc , d , upsert = True )","title":"update()"},{"location":"reference/stores/#maggma.stores.mongolike.SSHTunnel","text":"Source code in maggma/stores/mongolike.py class SSHTunnel ( MSONable ): __TUNNELS : Dict [ str , SSHTunnelForwarder ] = {} def __init__ ( self , tunnel_server_address : str , remote_server_address : str , username : Optional [ str ] = None , password : Optional [ str ] = None , private_key : Optional [ str ] = None , ** kwargs , ): \"\"\" Args: tunnel_server_address: string address with port for the SSH tunnel server remote_server_address: string address with port for the server to connect to username: optional username for the ssh tunnel server password: optional password for the ssh tunnel server; If a private_key is supplied this password is assumed to be the private key password private_key: ssh private key to authenticate to the tunnel server kwargs: any extra args passed to the SSHTunnelForwarder \"\"\" self . tunnel_server_address = tunnel_server_address self . remote_server_address = remote_server_address self . username = username self . password = password self . private_key = private_key self . kwargs = kwargs if remote_server_address in SSHTunnel . __TUNNELS : self . tunnel = SSHTunnel . __TUNNELS [ remote_server_address ] else : open_port = _find_free_port ( \"127.0.0.1\" ) local_bind_address = ( \"127.0.0.1\" , open_port ) ssh_address , ssh_port = tunnel_server_address . split ( \":\" ) ssh_port = int ( ssh_port ) # type: ignore remote_bind_address , remote_bind_port = remote_server_address . split ( \":\" ) remote_bind_port = int ( remote_bind_port ) # type: ignore if private_key is not None : ssh_password = None ssh_private_key_password = password else : ssh_password = password ssh_private_key_password = None self . tunnel = SSHTunnelForwarder ( ssh_address_or_host = ( ssh_address , ssh_port ), local_bind_address = local_bind_address , remote_bind_address = ( remote_bind_address , remote_bind_port ), ssh_username = username , ssh_password = ssh_password , ssh_private_key_password = ssh_private_key_password , ssh_pkey = private_key , ** kwargs , ) def start ( self ): if not self . tunnel . is_active : self . tunnel . start () def stop ( self ): if self . tunnel . tunnel_is_up : self . tunnel . stop () @property def local_address ( self ) -> Tuple [ str , int ]: return self . tunnel . local_bind_address","title":"SSHTunnel"},{"location":"reference/stores/#maggma.stores.mongolike.SSHTunnel.__init__","text":"Parameters: Name Type Description Default tunnel_server_address str string address with port for the SSH tunnel server required remote_server_address str string address with port for the server to connect to required username Optional[str] optional username for the ssh tunnel server None password Optional[str] optional password for the ssh tunnel server; If a private_key is supplied this password is assumed to be the private key password None private_key Optional[str] ssh private key to authenticate to the tunnel server None kwargs any extra args passed to the SSHTunnelForwarder {} Source code in maggma/stores/mongolike.py def __init__ ( self , tunnel_server_address : str , remote_server_address : str , username : Optional [ str ] = None , password : Optional [ str ] = None , private_key : Optional [ str ] = None , ** kwargs , ): \"\"\" Args: tunnel_server_address: string address with port for the SSH tunnel server remote_server_address: string address with port for the server to connect to username: optional username for the ssh tunnel server password: optional password for the ssh tunnel server; If a private_key is supplied this password is assumed to be the private key password private_key: ssh private key to authenticate to the tunnel server kwargs: any extra args passed to the SSHTunnelForwarder \"\"\" self . tunnel_server_address = tunnel_server_address self . remote_server_address = remote_server_address self . username = username self . password = password self . private_key = private_key self . kwargs = kwargs if remote_server_address in SSHTunnel . __TUNNELS : self . tunnel = SSHTunnel . __TUNNELS [ remote_server_address ] else : open_port = _find_free_port ( \"127.0.0.1\" ) local_bind_address = ( \"127.0.0.1\" , open_port ) ssh_address , ssh_port = tunnel_server_address . split ( \":\" ) ssh_port = int ( ssh_port ) # type: ignore remote_bind_address , remote_bind_port = remote_server_address . split ( \":\" ) remote_bind_port = int ( remote_bind_port ) # type: ignore if private_key is not None : ssh_password = None ssh_private_key_password = password else : ssh_password = password ssh_private_key_password = None self . tunnel = SSHTunnelForwarder ( ssh_address_or_host = ( ssh_address , ssh_port ), local_bind_address = local_bind_address , remote_bind_address = ( remote_bind_address , remote_bind_port ), ssh_username = username , ssh_password = ssh_password , ssh_private_key_password = ssh_private_key_password , ssh_pkey = private_key , ** kwargs , ) Module containing various definitions of Stores. Stores are a default access pattern to data and provide various utillities","title":"__init__()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore","text":"A Store for GrdiFS backend. Provides a common access method consistent with other stores Source code in maggma/stores/gridfs.py class GridFSStore ( Store ): \"\"\" A Store for GrdiFS backend. Provides a common access method consistent with other stores \"\"\" def __init__ ( self , database : str , collection_name : str , host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , compression : bool = False , ensure_metadata : bool = False , searchable_fields : List [ str ] = None , auth_source : Optional [ str ] = None , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Initializes a GrdiFS Store for binary data Args: database: database name collection_name: The name of the collection. This is the string portion before the GridFS extensions host: hostname for the database port: port to connect to username: username to connect as password: password to authenticate as compression: compress the data as it goes into GridFS ensure_metadata: ensure returned documents have the metadata fields searchable_fields: fields to keep in the index store auth_source: The database to authenticate on. Defaults to the database name. \"\"\" self . database = database self . collection_name = collection_name self . host = host self . port = port self . username = username self . password = password self . _coll = None # type: Any self . compression = compression self . ensure_metadata = ensure_metadata self . searchable_fields = [] if searchable_fields is None else searchable_fields self . kwargs = kwargs if auth_source is None : auth_source = self . database self . auth_source = auth_source self . mongoclient_kwargs = mongoclient_kwargs or {} if \"key\" not in kwargs : kwargs [ \"key\" ] = \"_id\" super () . __init__ ( ** kwargs ) @classmethod def from_launchpad_file ( cls , lp_file , collection_name , ** kwargs ): \"\"\" Convenience method to construct a GridFSStore from a launchpad file Note: A launchpad file is a special formatted yaml file used in fireworks Returns: \"\"\" with open ( lp_file , \"r\" ) as f : lp_creds = yaml . load ( f , Loader = yaml . FullLoader ) db_creds = lp_creds . copy () db_creds [ \"database\" ] = db_creds [ \"name\" ] for key in list ( db_creds . keys ()): if key not in [ \"database\" , \"host\" , \"port\" , \"username\" , \"password\" ]: db_creds . pop ( key ) db_creds [ \"collection_name\" ] = collection_name return cls ( ** db_creds , ** kwargs ) @property def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" return f \"gridfs:// { self . host } / { self . database } / { self . collection_name } \" def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" conn = ( MongoClient ( host = self . host , port = self . port , username = self . username , password = self . password , authSource = self . auth_source , ** self . mongoclient_kwargs , ) if self . username != \"\" else MongoClient ( self . host , self . port , ** self . mongoclient_kwargs ) ) if not self . _coll or force_reset : db = conn [ self . database ] self . _coll = gridfs . GridFS ( db , self . collection_name ) self . _files_collection = db [ \" {} .files\" . format ( self . collection_name )] self . _files_store = MongoStore . from_collection ( self . _files_collection ) self . _files_store . last_updated_field = f \"metadata. { self . last_updated_field } \" self . _files_store . key = self . key self . _chunks_collection = db [ \" {} .chunks\" . format ( self . collection_name )] @property def _collection ( self ): \"\"\"Property referring to underlying pymongo collection\"\"\" if self . _coll is None : raise StoreError ( \"Must connect Mongo-like store before attemping to use it\" ) return self . _coll @property def last_updated ( self ) -> datetime : \"\"\" Provides the most recent last_updated date time stamp from the documents in this Store \"\"\" return self . _files_store . last_updated @classmethod def transform_criteria ( cls , criteria : Dict ) -> Dict : \"\"\" Allow client to not need to prepend 'metadata.' to query fields. Args: criteria: Query criteria \"\"\" new_criteria = dict () for field in criteria : if field not in files_collection_fields and not field . startswith ( \"metadata.\" ): new_criteria [ \"metadata.\" + field ] = copy . copy ( criteria [ field ]) else : new_criteria [ field ] = copy . copy ( criteria [ field ]) return new_criteria def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) return self . _files_store . count ( criteria ) def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the GridFS Store for a set of documents. Will check to see if data can be returned from files store first. If the data from the gridfs is not a json serialized string a dict will be returned with the data in the \"data\" key plus the self.key and self.last_updated_field. Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) elif criteria is not None : raise ValueError ( \"Criteria must be a dictionary or None\" ) prop_keys = set () if isinstance ( properties , dict ): prop_keys = set ( properties . keys ()) elif isinstance ( properties , list ): prop_keys = set ( properties ) for doc in self . _files_store . query ( criteria = criteria , sort = sort , limit = limit , skip = skip ): if properties is not None and prop_keys . issubset ( set ( doc . keys ())): yield { p : doc [ p ] for p in properties if p in doc } else : metadata = doc . get ( \"metadata\" , {}) data = self . _collection . find_one ( filter = { \"_id\" : doc [ \"_id\" ]}, skip = skip , limit = limit , sort = sort , ) . read () if metadata . get ( \"compression\" , \"\" ) == \"zlib\" : data = zlib . decompress ( data ) . decode ( \"UTF-8\" ) try : data = json . loads ( data ) except Exception : if not isinstance ( data , dict ): data = { \"data\" : data , self . key : doc . get ( self . key ), self . last_updated_field : doc . get ( self . last_updated_field ), } if self . ensure_metadata and isinstance ( data , dict ): data . update ( metadata ) yield data def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field. This function only operates on the metadata in the files collection Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = ( self . transform_criteria ( criteria ) if isinstance ( criteria , dict ) else criteria ) field = ( f \"metadata. { field } \" if field not in files_collection_fields and not field . startswith ( \"metadata.\" ) else field ) return self . _files_store . distinct ( field = field , criteria = criteria ) def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Will only work if the keys are included in the files collection for GridFS Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" criteria = ( self . transform_criteria ( criteria ) if isinstance ( criteria , dict ) else criteria ) keys = [ keys ] if not isinstance ( keys , list ) else keys keys = [ f \"metadata. { k } \" if k not in files_collection_fields and not k . startswith ( \"metadata.\" ) else k for k in keys ] for group , ids in self . _files_store . groupby ( keys , criteria = criteria , properties = [ f \"metadata. { self . key } \" ] ): ids = [ get ( doc , f \"metadata. { self . key } \" ) for doc in ids if has ( doc , f \"metadata. { self . key } \" ) ] group = { k . replace ( \"metadata.\" , \"\" ): get ( group , k ) for k in keys if has ( group , k ) } yield group , list ( self . query ( criteria = { self . key : { \"$in\" : ids }})) def ensure_index ( self , key : str , unique : Optional [ bool ] = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Currently operators on the GridFS files collection Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" # Transform key for gridfs first if key not in files_collection_fields : files_col_key = \"metadata. {} \" . format ( key ) return self . _files_store . ensure_index ( files_col_key , unique = unique ) else : return self . _files_store . ensure_index ( key , unique = unique ) def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None , additional_metadata : Union [ str , List [ str ], None ] = None , ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used additional_metadata: field(s) to include in the gridfs metadata \"\"\" if not isinstance ( docs , list ): docs = [ docs ] if isinstance ( key , str ): key = [ key ] elif not key : key = [ self . key ] key = list ( set ( key ) - set ( files_collection_fields )) if additional_metadata is None : additional_metadata = [] elif isinstance ( additional_metadata , str ): additional_metadata = [ additional_metadata ] else : additional_metadata = list ( additional_metadata ) for d in docs : search_doc = { k : d [ k ] for k in key } metadata = { k : get ( d , k ) for k in [ self . last_updated_field ] + additional_metadata + self . searchable_fields if has ( d , k ) } metadata . update ( search_doc ) data = json . dumps ( jsanitize ( d )) . encode ( \"UTF-8\" ) if self . compression : data = zlib . compress ( data ) metadata [ \"compression\" ] = \"zlib\" self . _collection . put ( data , metadata = metadata ) search_doc = self . transform_criteria ( search_doc ) # Cleans up old gridfs entries for fdoc in ( self . _files_collection . find ( search_doc , [ \"_id\" ]) . sort ( \"uploadDate\" , - 1 ) . skip ( 1 ) ): self . _collection . delete ( fdoc [ \"_id\" ]) def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) ids = [ cursor . _id for cursor in self . _collection . find ( criteria )] for _id in ids : self . _collection . delete ( _id ) def close ( self ): self . _collection . database . client . close () def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for GridFSStore other: other GridFSStore to compare with \"\"\" if not isinstance ( other , GridFSStore ): return False fields = [ \"database\" , \"collection_name\" , \"host\" , \"port\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"GridFSStore"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.last_updated","text":"Provides the most recent last_updated date time stamp from the documents in this Store","title":"last_updated"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.name","text":"Return a string representing this data source","title":"name"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.__eq__","text":"Check equality for GridFSStore other: other GridFSStore to compare with Source code in maggma/stores/gridfs.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for GridFSStore other: other GridFSStore to compare with \"\"\" if not isinstance ( other , GridFSStore ): return False fields = [ \"database\" , \"collection_name\" , \"host\" , \"port\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.__init__","text":"Initializes a GrdiFS Store for binary data Parameters: Name Type Description Default database str database name required collection_name str The name of the collection. This is the string portion before the GridFS extensions required host str hostname for the database 'localhost' port int port to connect to 27017 username str username to connect as '' password str password to authenticate as '' compression bool compress the data as it goes into GridFS False ensure_metadata bool ensure returned documents have the metadata fields False searchable_fields List[str] fields to keep in the index store None auth_source Optional[str] The database to authenticate on. Defaults to the database name. None Source code in maggma/stores/gridfs.py def __init__ ( self , database : str , collection_name : str , host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , compression : bool = False , ensure_metadata : bool = False , searchable_fields : List [ str ] = None , auth_source : Optional [ str ] = None , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Initializes a GrdiFS Store for binary data Args: database: database name collection_name: The name of the collection. This is the string portion before the GridFS extensions host: hostname for the database port: port to connect to username: username to connect as password: password to authenticate as compression: compress the data as it goes into GridFS ensure_metadata: ensure returned documents have the metadata fields searchable_fields: fields to keep in the index store auth_source: The database to authenticate on. Defaults to the database name. \"\"\" self . database = database self . collection_name = collection_name self . host = host self . port = port self . username = username self . password = password self . _coll = None # type: Any self . compression = compression self . ensure_metadata = ensure_metadata self . searchable_fields = [] if searchable_fields is None else searchable_fields self . kwargs = kwargs if auth_source is None : auth_source = self . database self . auth_source = auth_source self . mongoclient_kwargs = mongoclient_kwargs or {} if \"key\" not in kwargs : kwargs [ \"key\" ] = \"_id\" super () . __init__ ( ** kwargs )","title":"__init__()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.close","text":"Closes any connections Source code in maggma/stores/gridfs.py def close ( self ): self . _collection . database . client . close ()","title":"close()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.connect","text":"Connect to the source data Source code in maggma/stores/gridfs.py def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" conn = ( MongoClient ( host = self . host , port = self . port , username = self . username , password = self . password , authSource = self . auth_source , ** self . mongoclient_kwargs , ) if self . username != \"\" else MongoClient ( self . host , self . port , ** self . mongoclient_kwargs ) ) if not self . _coll or force_reset : db = conn [ self . database ] self . _coll = gridfs . GridFS ( db , self . collection_name ) self . _files_collection = db [ \" {} .files\" . format ( self . collection_name )] self . _files_store = MongoStore . from_collection ( self . _files_collection ) self . _files_store . last_updated_field = f \"metadata. { self . last_updated_field } \" self . _files_store . key = self . key self . _chunks_collection = db [ \" {} .chunks\" . format ( self . collection_name )]","title":"connect()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.count","text":"Counts the number of documents matching the query criteria Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None Source code in maggma/stores/gridfs.py def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) return self . _files_store . count ( criteria )","title":"count()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.distinct","text":"Get all distinct values for a field. This function only operates on the metadata in the files collection Parameters: Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None Source code in maggma/stores/gridfs.py def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field. This function only operates on the metadata in the files collection Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = ( self . transform_criteria ( criteria ) if isinstance ( criteria , dict ) else criteria ) field = ( f \"metadata. { field } \" if field not in files_collection_fields and not field . startswith ( \"metadata.\" ) else field ) return self . _files_store . distinct ( field = field , criteria = criteria )","title":"distinct()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.ensure_index","text":"Tries to create an index and return true if it suceeded Currently operators on the GridFS files collection Parameters: Name Type Description Default key str single key to index required unique Optional[bool] Whether or not this index contains only unique keys False Returns: Type Description bool bool indicating if the index exists/was created Source code in maggma/stores/gridfs.py def ensure_index ( self , key : str , unique : Optional [ bool ] = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Currently operators on the GridFS files collection Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" # Transform key for gridfs first if key not in files_collection_fields : files_col_key = \"metadata. {} \" . format ( key ) return self . _files_store . ensure_index ( files_col_key , unique = unique ) else : return self . _files_store . ensure_index ( key , unique = unique )","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.from_launchpad_file","text":"Convenience method to construct a GridFSStore from a launchpad file Note: A launchpad file is a special formatted yaml file used in fireworks Source code in maggma/stores/gridfs.py @classmethod def from_launchpad_file ( cls , lp_file , collection_name , ** kwargs ): \"\"\" Convenience method to construct a GridFSStore from a launchpad file Note: A launchpad file is a special formatted yaml file used in fireworks Returns: \"\"\" with open ( lp_file , \"r\" ) as f : lp_creds = yaml . load ( f , Loader = yaml . FullLoader ) db_creds = lp_creds . copy () db_creds [ \"database\" ] = db_creds [ \"name\" ] for key in list ( db_creds . keys ()): if key not in [ \"database\" , \"host\" , \"port\" , \"username\" , \"password\" ]: db_creds . pop ( key ) db_creds [ \"collection_name\" ] = collection_name return cls ( ** db_creds , ** kwargs )","title":"from_launchpad_file()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.groupby","text":"Simple grouping function that will group documents by keys. Will only work if the keys are included in the files collection for GridFS Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) Source code in maggma/stores/gridfs.py def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Will only work if the keys are included in the files collection for GridFS Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" criteria = ( self . transform_criteria ( criteria ) if isinstance ( criteria , dict ) else criteria ) keys = [ keys ] if not isinstance ( keys , list ) else keys keys = [ f \"metadata. { k } \" if k not in files_collection_fields and not k . startswith ( \"metadata.\" ) else k for k in keys ] for group , ids in self . _files_store . groupby ( keys , criteria = criteria , properties = [ f \"metadata. { self . key } \" ] ): ids = [ get ( doc , f \"metadata. { self . key } \" ) for doc in ids if has ( doc , f \"metadata. { self . key } \" ) ] group = { k . replace ( \"metadata.\" , \"\" ): get ( group , k ) for k in keys if has ( group , k ) } yield group , list ( self . query ( criteria = { self . key : { \"$in\" : ids }}))","title":"groupby()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.query","text":"Queries the GridFS Store for a set of documents. Will check to see if data can be returned from files store first. If the data from the gridfs is not a json serialized string a dict will be returned with the data in the \"data\" key plus the self.key and self.last_updated_field. Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Source code in maggma/stores/gridfs.py def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the GridFS Store for a set of documents. Will check to see if data can be returned from files store first. If the data from the gridfs is not a json serialized string a dict will be returned with the data in the \"data\" key plus the self.key and self.last_updated_field. Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) elif criteria is not None : raise ValueError ( \"Criteria must be a dictionary or None\" ) prop_keys = set () if isinstance ( properties , dict ): prop_keys = set ( properties . keys ()) elif isinstance ( properties , list ): prop_keys = set ( properties ) for doc in self . _files_store . query ( criteria = criteria , sort = sort , limit = limit , skip = skip ): if properties is not None and prop_keys . issubset ( set ( doc . keys ())): yield { p : doc [ p ] for p in properties if p in doc } else : metadata = doc . get ( \"metadata\" , {}) data = self . _collection . find_one ( filter = { \"_id\" : doc [ \"_id\" ]}, skip = skip , limit = limit , sort = sort , ) . read () if metadata . get ( \"compression\" , \"\" ) == \"zlib\" : data = zlib . decompress ( data ) . decode ( \"UTF-8\" ) try : data = json . loads ( data ) except Exception : if not isinstance ( data , dict ): data = { \"data\" : data , self . key : doc . get ( self . key ), self . last_updated_field : doc . get ( self . last_updated_field ), } if self . ensure_metadata and isinstance ( data , dict ): data . update ( metadata ) yield data","title":"query()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.remove_docs","text":"Remove docs matching the query dictionary Parameters: Name Type Description Default criteria Dict query dictionary to match required Source code in maggma/stores/gridfs.py def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) ids = [ cursor . _id for cursor in self . _collection . find ( criteria )] for _id in ids : self . _collection . delete ( _id )","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.transform_criteria","text":"Allow client to not need to prepend 'metadata.' to query fields. Parameters: Name Type Description Default criteria Dict Query criteria required Source code in maggma/stores/gridfs.py @classmethod def transform_criteria ( cls , criteria : Dict ) -> Dict : \"\"\" Allow client to not need to prepend 'metadata.' to query fields. Args: criteria: Query criteria \"\"\" new_criteria = dict () for field in criteria : if field not in files_collection_fields and not field . startswith ( \"metadata.\" ): new_criteria [ \"metadata.\" + field ] = copy . copy ( criteria [ field ]) else : new_criteria [ field ] = copy . copy ( criteria [ field ]) return new_criteria","title":"transform_criteria()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.update","text":"Update documents into the Store Parameters: Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None additional_metadata Union[str, List[str]] field(s) to include in the gridfs metadata None Source code in maggma/stores/gridfs.py def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None , additional_metadata : Union [ str , List [ str ], None ] = None , ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used additional_metadata: field(s) to include in the gridfs metadata \"\"\" if not isinstance ( docs , list ): docs = [ docs ] if isinstance ( key , str ): key = [ key ] elif not key : key = [ self . key ] key = list ( set ( key ) - set ( files_collection_fields )) if additional_metadata is None : additional_metadata = [] elif isinstance ( additional_metadata , str ): additional_metadata = [ additional_metadata ] else : additional_metadata = list ( additional_metadata ) for d in docs : search_doc = { k : d [ k ] for k in key } metadata = { k : get ( d , k ) for k in [ self . last_updated_field ] + additional_metadata + self . searchable_fields if has ( d , k ) } metadata . update ( search_doc ) data = json . dumps ( jsanitize ( d )) . encode ( \"UTF-8\" ) if self . compression : data = zlib . compress ( data ) metadata [ \"compression\" ] = \"zlib\" self . _collection . put ( data , metadata = metadata ) search_doc = self . transform_criteria ( search_doc ) # Cleans up old gridfs entries for fdoc in ( self . _files_collection . find ( search_doc , [ \"_id\" ]) . sort ( \"uploadDate\" , - 1 ) . skip ( 1 ) ): self . _collection . delete ( fdoc [ \"_id\" ])","title":"update()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSURIStore","text":"A Store for GridFS backend, with connection via a mongo URI string. This is expected to be a special mongodb+srv:// URIs that include client parameters via TXT records Source code in maggma/stores/gridfs.py class GridFSURIStore ( GridFSStore ): \"\"\" A Store for GridFS backend, with connection via a mongo URI string. This is expected to be a special mongodb+srv:// URIs that include client parameters via TXT records \"\"\" def __init__ ( self , uri : str , collection_name : str , database : str = None , compression : bool = False , ensure_metadata : bool = False , searchable_fields : List [ str ] = None , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Initializes a GrdiFS Store for binary data Args: uri: MongoDB+SRV URI database: database to connect to collection_name: The collection name compression: compress the data as it goes into GridFS ensure_metadata: ensure returned documents have the metadata fields searchable_fields: fields to keep in the index store \"\"\" self . uri = uri # parse the dbname from the uri if database is None : d_uri = uri_parser . parse_uri ( uri ) if d_uri [ \"database\" ] is None : raise ConfigurationError ( \"If database name is not supplied, a database must be set in the uri\" ) self . database = d_uri [ \"database\" ] else : self . database = database self . collection_name = collection_name self . _coll = None # type: Any self . compression = compression self . ensure_metadata = ensure_metadata self . searchable_fields = [] if searchable_fields is None else searchable_fields self . kwargs = kwargs self . mongoclient_kwargs = mongoclient_kwargs or {} if \"key\" not in kwargs : kwargs [ \"key\" ] = \"_id\" super ( GridFSStore , self ) . __init__ ( ** kwargs ) # lgtm def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if not self . _coll or force_reset : # pragma: no cover conn = MongoClient ( self . uri , ** self . mongoclient_kwargs ) db = conn [ self . database ] self . _coll = gridfs . GridFS ( db , self . collection_name ) self . _files_collection = db [ \" {} .files\" . format ( self . collection_name )] self . _files_store = MongoStore . from_collection ( self . _files_collection ) self . _files_store . last_updated_field = f \"metadata. { self . last_updated_field } \" self . _files_store . key = self . key self . _chunks_collection = db [ \" {} .chunks\" . format ( self . collection_name )]","title":"GridFSURIStore"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSURIStore.__init__","text":"Initializes a GrdiFS Store for binary data Parameters: Name Type Description Default uri str MongoDB+SRV URI required database str database to connect to None collection_name str The collection name required compression bool compress the data as it goes into GridFS False ensure_metadata bool ensure returned documents have the metadata fields False searchable_fields List[str] fields to keep in the index store None Source code in maggma/stores/gridfs.py def __init__ ( self , uri : str , collection_name : str , database : str = None , compression : bool = False , ensure_metadata : bool = False , searchable_fields : List [ str ] = None , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Initializes a GrdiFS Store for binary data Args: uri: MongoDB+SRV URI database: database to connect to collection_name: The collection name compression: compress the data as it goes into GridFS ensure_metadata: ensure returned documents have the metadata fields searchable_fields: fields to keep in the index store \"\"\" self . uri = uri # parse the dbname from the uri if database is None : d_uri = uri_parser . parse_uri ( uri ) if d_uri [ \"database\" ] is None : raise ConfigurationError ( \"If database name is not supplied, a database must be set in the uri\" ) self . database = d_uri [ \"database\" ] else : self . database = database self . collection_name = collection_name self . _coll = None # type: Any self . compression = compression self . ensure_metadata = ensure_metadata self . searchable_fields = [] if searchable_fields is None else searchable_fields self . kwargs = kwargs self . mongoclient_kwargs = mongoclient_kwargs or {} if \"key\" not in kwargs : kwargs [ \"key\" ] = \"_id\" super ( GridFSStore , self ) . __init__ ( ** kwargs ) # lgtm","title":"__init__()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSURIStore.connect","text":"Connect to the source data Source code in maggma/stores/gridfs.py def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if not self . _coll or force_reset : # pragma: no cover conn = MongoClient ( self . uri , ** self . mongoclient_kwargs ) db = conn [ self . database ] self . _coll = gridfs . GridFS ( db , self . collection_name ) self . _files_collection = db [ \" {} .files\" . format ( self . collection_name )] self . _files_store = MongoStore . from_collection ( self . _files_collection ) self . _files_store . last_updated_field = f \"metadata. { self . last_updated_field } \" self . _files_store . key = self . key self . _chunks_collection = db [ \" {} .chunks\" . format ( self . collection_name )] Advanced Stores for connecting to AWS data","title":"connect()"},{"location":"reference/stores/#maggma.stores.aws.S3Store","text":"GridFS like storage using Amazon S3 and a regular store for indexing Assumes Amazon AWS key and secret key are set in environment or default config file Source code in maggma/stores/aws.py class S3Store ( Store ): \"\"\" GridFS like storage using Amazon S3 and a regular store for indexing Assumes Amazon AWS key and secret key are set in environment or default config file \"\"\" def __init__ ( self , index : Store , bucket : str , s3_profile : Optional [ Union [ str , dict ]] = None , compress : bool = False , endpoint_url : str = None , sub_dir : str = None , s3_workers : int = 1 , s3_resource_kwargs : Optional [ dict ] = None , key : str = \"fs_id\" , store_hash : bool = True , unpack_data : bool = True , searchable_fields : Optional [ List [ str ]] = None , ** kwargs , ): \"\"\" Initializes an S3 Store Args: index: a store to use to index the S3 Bucket bucket: name of the bucket s3_profile: name of aws profile containing credentials for role. Alternatively you can pass in a dictionary with the full credentials: aws_access_key_id (string) -- AWS access key ID aws_secret_access_key (string) -- AWS secret access key aws_session_token (string) -- AWS temporary session token region_name (string) -- Default region when creating new connections compress: compress files inserted into the store endpoint_url: endpoint_url to allow interface to minio service sub_dir: (optional) subdirectory of the s3 bucket to store the data s3_workers: number of concurrent S3 puts to run store_hash: store the sha1 hash right before insertion to the database. unpack_data: whether to decompress and unpack byte data when querying from the bucket. searchable_fields: fields to keep in the index store \"\"\" if boto3 is None : raise RuntimeError ( \"boto3 and botocore are required for S3Store\" ) self . index = index self . bucket = bucket self . s3_profile = s3_profile self . compress = compress self . endpoint_url = endpoint_url self . sub_dir = sub_dir . strip ( \"/\" ) + \"/\" if sub_dir else \"\" self . s3 = None # type: Any self . s3_bucket = None # type: Any self . s3_workers = s3_workers self . s3_resource_kwargs = ( s3_resource_kwargs if s3_resource_kwargs is not None else {} ) self . unpack_data = unpack_data self . searchable_fields = ( searchable_fields if searchable_fields is not None else [] ) self . store_hash = store_hash # Force the key to be the same as the index assert isinstance ( index . key , str ), \"Since we are using the key as a file name in S3, they key must be a string\" if key != index . key : warnings . warn ( f 'The desired S3Store key \" { key } \" does not match the index key \" { index . key } ,\"' \"the index key will be used\" , UserWarning , ) kwargs [ \"key\" ] = str ( index . key ) self . _thread_local = threading . local () super ( S3Store , self ) . __init__ ( ** kwargs ) @property def name ( self ) -> str : \"\"\" Returns: a string representing this data source \"\"\" return f \"s3:// { self . bucket } \" def connect ( self , * args , ** kwargs ): # lgtm[py/conflicting-attributes] \"\"\" Connect to the source data \"\"\" session = self . _get_session () resource = session . resource ( \"s3\" , endpoint_url = self . endpoint_url , ** self . s3_resource_kwargs ) if not self . s3 : self . s3 = resource try : self . s3 . meta . client . head_bucket ( Bucket = self . bucket ) except ClientError : raise RuntimeError ( \"Bucket not present on AWS: {} \" . format ( self . bucket )) self . s3_bucket = resource . Bucket ( self . bucket ) self . index . connect ( * args , ** kwargs ) def close ( self ): \"\"\" Closes any connections \"\"\" self . index . close () self . s3 = None self . s3_bucket = None @property def _collection ( self ): \"\"\" Returns: a handle to the pymongo collection object Important: Not guaranteed to exist in the future \"\"\" # For now returns the index collection since that is what we would \"search\" on return self . index . _collection def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" return self . index . count ( criteria ) def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" prop_keys = set () if isinstance ( properties , dict ): prop_keys = set ( properties . keys ()) elif isinstance ( properties , list ): prop_keys = set ( properties ) for doc in self . index . query ( criteria = criteria , sort = sort , limit = limit , skip = skip ): if properties is not None and prop_keys . issubset ( set ( doc . keys ())): yield { p : doc [ p ] for p in properties if p in doc } else : try : # TODO: THis is ugly and unsafe, do some real checking before pulling data data = ( self . s3_bucket . Object ( self . sub_dir + str ( doc [ self . key ])) . get ()[ \"Body\" ] . read () ) except botocore . exceptions . ClientError as e : # If a client error is thrown, then check that it was a 404 error. # If it was a 404 error, then the object does not exist. error_code = int ( e . response [ \"Error\" ][ \"Code\" ]) if error_code == 404 : self . logger . error ( \"Could not find S3 object {} \" . format ( doc [ self . key ]) ) break else : raise e if self . unpack_data : data = self . _unpack ( data = data , compressed = doc . get ( \"compression\" , \"\" ) == \"zlib\" ) if self . last_updated_field in doc : data [ self . last_updated_field ] = doc [ self . last_updated_field ] yield data @staticmethod def _unpack ( data : bytes , compressed : bool ): if compressed : data = zlib . decompress ( data ) # requires msgpack-python to be installed to fix string encoding problem # https://github.com/msgpack/msgpack/issues/121 # During recursion # msgpack.unpackb goes as deep as possible during reconstruction # MontyDecoder().process_decode only goes until it finds a from_dict # as such, we cannot just use msgpack.unpackb(data, object_hook=monty_object_hook, raw=False) # Should just return the unpacked object then let the user run process_decoded unpacked_data = msgpack . unpackb ( data , raw = False ) return unpacked_data def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" # Index is a store so it should have its own distinct function return self . index . distinct ( field , criteria = criteria ) def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" return self . index . groupby ( keys = keys , criteria = criteria , properties = properties , sort = sort , skip = skip , limit = limit , ) def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" return self . index . ensure_index ( key , unique = unique ) def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None , additional_metadata : Union [ str , List [ str ], None ] = None , ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used additional_metadata: field(s) to include in the s3 store's metadata \"\"\" if not isinstance ( docs , list ): docs = [ docs ] if isinstance ( key , str ): key = [ key ] elif not key : key = [ self . key ] if additional_metadata is None : additional_metadata = [] elif isinstance ( additional_metadata , str ): additional_metadata = [ additional_metadata ] else : additional_metadata = list ( additional_metadata ) with ThreadPoolExecutor ( max_workers = self . s3_workers ) as pool : fs = { pool . submit ( self . write_doc_to_s3 , doc = itr_doc , search_keys = key + additional_metadata + self . searchable_fields , ) for itr_doc in docs } fs , _ = wait ( fs ) search_docs = [ sdoc . result () for sdoc in fs ] # Use store's update to remove key clashes self . index . update ( search_docs , key = self . key ) def _get_session ( self ): if not hasattr ( self . _thread_local , \"s3_bucket\" ): if isinstance ( self . s3_profile , dict ): return Session ( ** self . s3_profile ) else : return Session ( profile_name = self . s3_profile ) def _get_bucket ( self ): \"\"\" If on the main thread return the bucket created above, else create a new bucket on each thread \"\"\" if threading . current_thread () . name == \"MainThread\" : return self . s3_bucket if not hasattr ( self . _thread_local , \"s3_bucket\" ): session = self . _get_session () resource = session . resource ( \"s3\" , endpoint_url = self . endpoint_url ) self . _thread_local . s3_bucket = resource . Bucket ( self . bucket ) return self . _thread_local . s3_bucket def write_doc_to_s3 ( self , doc : Dict , search_keys : List [ str ]): \"\"\" Write the data to s3 and return the metadata to be inserted into the index db Args: doc: the document search_keys: list of keys to pull from the docs and be inserted into the index db \"\"\" s3_bucket = self . _get_bucket () search_doc = { k : doc [ k ] for k in search_keys } search_doc [ self . key ] = doc [ self . key ] # Ensure key is in metadata if self . sub_dir != \"\" : search_doc [ \"sub_dir\" ] = self . sub_dir # Remove MongoDB _id from search if \"_id\" in search_doc : del search_doc [ \"_id\" ] # to make hashing more meaningful, make sure last updated field is removed lu_info = doc . pop ( self . last_updated_field , None ) data = msgpack . packb ( doc , default = monty_default ) if self . compress : # Compress with zlib if chosen search_doc [ \"compression\" ] = \"zlib\" data = zlib . compress ( data ) if self . last_updated_field in doc : # need this conversion for aws metadata insert search_doc [ self . last_updated_field ] = str ( to_isoformat_ceil_ms ( doc [ self . last_updated_field ]) ) # keep a record of original keys, in case these are important for the individual researcher # it is not expected that this information will be used except in disaster recovery s3_to_mongo_keys = { k : self . _sanitize_key ( k ) for k in search_doc . keys ()} s3_to_mongo_keys [ \"s3-to-mongo-keys\" ] = \"s3-to-mongo-keys\" # inception # encode dictionary since values have to be strings search_doc [ \"s3-to-mongo-keys\" ] = dumps ( s3_to_mongo_keys ) s3_bucket . put_object ( Key = self . sub_dir + str ( doc [ self . key ]), Body = data , Metadata = { s3_to_mongo_keys [ k ]: str ( v ) for k , v in search_doc . items ()}, ) if lu_info is not None : search_doc [ self . last_updated_field ] = lu_info if self . store_hash : hasher = sha1 () hasher . update ( data ) obj_hash = hasher . hexdigest () search_doc [ \"obj_hash\" ] = obj_hash return search_doc @staticmethod def _sanitize_key ( key ): \"\"\" Sanitize keys to store in S3/MinIO metadata. \"\"\" # Any underscores are encoded as double dashes in metadata, since keys with # underscores may be result in the corresponding HTTP header being stripped # by certain server configurations (e.g. default nginx), leading to: # `botocore.exceptions.ClientError: An error occurred (AccessDenied) when # calling the PutObject operation: There were headers present in the request # which were not signed` # Metadata stored in the MongoDB index (self.index) is stored unchanged. # Additionally, MinIO requires lowercase keys return str ( key ) . replace ( \"_\" , \"-\" ) . lower () def remove_docs ( self , criteria : Dict , remove_s3_object : bool = False ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match remove_s3_object: whether to remove the actual S3 Object or not \"\"\" if not remove_s3_object : self . index . remove_docs ( criteria = criteria ) else : to_remove = self . index . distinct ( self . key , criteria = criteria ) self . index . remove_docs ( criteria = criteria ) # Can remove up to 1000 items at a time via boto to_remove_chunks = list ( grouper ( to_remove , n = 1000 )) for chunk_to_remove in to_remove_chunks : objlist = [{ \"Key\" : f \" { self . sub_dir }{ obj } \" } for obj in chunk_to_remove ] self . s3_bucket . delete_objects ( Delete = { \"Objects\" : objlist }) @property def last_updated ( self ): return self . index . last_updated def newer_in ( self , target : Store , criteria : Optional [ Dict ] = None , exhaustive : bool = False ) -> List [ str ]: \"\"\" Returns the keys of documents that are newer in the target Store than this Store. Args: target: target Store criteria: PyMongo filter for documents to search in exhaustive: triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in \"\"\" if hasattr ( target , \"index\" ): return self . index . newer_in ( target = target . index , criteria = criteria , exhaustive = exhaustive ) else : return self . index . newer_in ( target = target , criteria = criteria , exhaustive = exhaustive ) def __hash__ ( self ): return hash (( self . index . __hash__ , self . bucket )) def rebuild_index_from_s3_data ( self , ** kwargs ): \"\"\" Rebuilds the index Store from the data in S3 Relies on the index document being stores as the metadata for the file This can help recover lost databases \"\"\" bucket = self . s3_bucket objects = bucket . objects . filter ( Prefix = self . sub_dir ) for obj in objects : key_ = self . sub_dir + obj . key data = self . s3_bucket . Object ( key_ ) . get ()[ \"Body\" ] . read () if self . compress : data = zlib . decompress ( data ) unpacked_data = msgpack . unpackb ( data , raw = False ) self . update ( unpacked_data , ** kwargs ) def rebuild_metadata_from_index ( self , index_query : dict = None ): \"\"\" Read data from the index store and populate the metadata of the S3 bucket Force all of the keys to be lower case to be Minio compatible Args: index_query: query on the index store \"\"\" qq = {} if index_query is None else index_query for index_doc in self . index . query ( qq ): key_ = self . sub_dir + index_doc [ self . key ] s3_object = self . s3_bucket . Object ( key_ ) new_meta = { self . _sanitize_key ( k ): v for k , v in s3_object . metadata . items ()} for k , v in index_doc . items (): new_meta [ str ( k ) . lower ()] = v new_meta . pop ( \"_id\" ) if self . last_updated_field in new_meta : new_meta [ self . last_updated_field ] = str ( to_isoformat_ceil_ms ( new_meta [ self . last_updated_field ]) ) # s3_object.metadata.update(new_meta) s3_object . copy_from ( CopySource = { \"Bucket\" : self . s3_bucket . name , \"Key\" : key_ }, Metadata = new_meta , MetadataDirective = \"REPLACE\" , ) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for S3Store other: other S3Store to compare with \"\"\" if not isinstance ( other , S3Store ): return False fields = [ \"index\" , \"bucket\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"S3Store"},{"location":"reference/stores/#maggma.stores.aws.S3Store.last_updated","text":"Provides the most recent last_updated date time stamp from the documents in this Store","title":"last_updated"},{"location":"reference/stores/#maggma.stores.aws.S3Store.name","text":"Returns: Type Description str a string representing this data source","title":"name"},{"location":"reference/stores/#maggma.stores.aws.S3Store.__eq__","text":"Check equality for S3Store other: other S3Store to compare with Source code in maggma/stores/aws.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for S3Store other: other S3Store to compare with \"\"\" if not isinstance ( other , S3Store ): return False fields = [ \"index\" , \"bucket\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.__init__","text":"Initializes an S3 Store Parameters: Name Type Description Default index Store a store to use to index the S3 Bucket required bucket str name of the bucket required s3_profile Union[str, dict] name of aws profile containing credentials for role. Alternatively you can pass in a dictionary with the full credentials: aws_access_key_id (string) -- AWS access key ID aws_secret_access_key (string) -- AWS secret access key aws_session_token (string) -- AWS temporary session token region_name (string) -- Default region when creating new connections None compress bool compress files inserted into the store False endpoint_url str endpoint_url to allow interface to minio service None sub_dir str (optional) subdirectory of the s3 bucket to store the data None s3_workers int number of concurrent S3 puts to run 1 store_hash bool store the sha1 hash right before insertion to the database. True unpack_data bool whether to decompress and unpack byte data when querying from the bucket. True searchable_fields Optional[List[str]] fields to keep in the index store None Source code in maggma/stores/aws.py def __init__ ( self , index : Store , bucket : str , s3_profile : Optional [ Union [ str , dict ]] = None , compress : bool = False , endpoint_url : str = None , sub_dir : str = None , s3_workers : int = 1 , s3_resource_kwargs : Optional [ dict ] = None , key : str = \"fs_id\" , store_hash : bool = True , unpack_data : bool = True , searchable_fields : Optional [ List [ str ]] = None , ** kwargs , ): \"\"\" Initializes an S3 Store Args: index: a store to use to index the S3 Bucket bucket: name of the bucket s3_profile: name of aws profile containing credentials for role. Alternatively you can pass in a dictionary with the full credentials: aws_access_key_id (string) -- AWS access key ID aws_secret_access_key (string) -- AWS secret access key aws_session_token (string) -- AWS temporary session token region_name (string) -- Default region when creating new connections compress: compress files inserted into the store endpoint_url: endpoint_url to allow interface to minio service sub_dir: (optional) subdirectory of the s3 bucket to store the data s3_workers: number of concurrent S3 puts to run store_hash: store the sha1 hash right before insertion to the database. unpack_data: whether to decompress and unpack byte data when querying from the bucket. searchable_fields: fields to keep in the index store \"\"\" if boto3 is None : raise RuntimeError ( \"boto3 and botocore are required for S3Store\" ) self . index = index self . bucket = bucket self . s3_profile = s3_profile self . compress = compress self . endpoint_url = endpoint_url self . sub_dir = sub_dir . strip ( \"/\" ) + \"/\" if sub_dir else \"\" self . s3 = None # type: Any self . s3_bucket = None # type: Any self . s3_workers = s3_workers self . s3_resource_kwargs = ( s3_resource_kwargs if s3_resource_kwargs is not None else {} ) self . unpack_data = unpack_data self . searchable_fields = ( searchable_fields if searchable_fields is not None else [] ) self . store_hash = store_hash # Force the key to be the same as the index assert isinstance ( index . key , str ), \"Since we are using the key as a file name in S3, they key must be a string\" if key != index . key : warnings . warn ( f 'The desired S3Store key \" { key } \" does not match the index key \" { index . key } ,\"' \"the index key will be used\" , UserWarning , ) kwargs [ \"key\" ] = str ( index . key ) self . _thread_local = threading . local () super ( S3Store , self ) . __init__ ( ** kwargs )","title":"__init__()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.close","text":"Closes any connections Source code in maggma/stores/aws.py def close ( self ): \"\"\" Closes any connections \"\"\" self . index . close () self . s3 = None self . s3_bucket = None","title":"close()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.connect","text":"Connect to the source data Source code in maggma/stores/aws.py def connect ( self , * args , ** kwargs ): # lgtm[py/conflicting-attributes] \"\"\" Connect to the source data \"\"\" session = self . _get_session () resource = session . resource ( \"s3\" , endpoint_url = self . endpoint_url , ** self . s3_resource_kwargs ) if not self . s3 : self . s3 = resource try : self . s3 . meta . client . head_bucket ( Bucket = self . bucket ) except ClientError : raise RuntimeError ( \"Bucket not present on AWS: {} \" . format ( self . bucket )) self . s3_bucket = resource . Bucket ( self . bucket ) self . index . connect ( * args , ** kwargs )","title":"connect()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.count","text":"Counts the number of documents matching the query criteria Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None Source code in maggma/stores/aws.py def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" return self . index . count ( criteria )","title":"count()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.distinct","text":"Get all distinct values for a field Parameters: Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None Source code in maggma/stores/aws.py def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" # Index is a store so it should have its own distinct function return self . index . distinct ( field , criteria = criteria )","title":"distinct()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.ensure_index","text":"Tries to create an index and return true if it suceeded Parameters: Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns: Type Description bool bool indicating if the index exists/was created Source code in maggma/stores/aws.py def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" return self . index . ensure_index ( key , unique = unique )","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.groupby","text":"Simple grouping function that will group documents by keys. Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) Source code in maggma/stores/aws.py def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" return self . index . groupby ( keys = keys , criteria = criteria , properties = properties , sort = sort , skip = skip , limit = limit , )","title":"groupby()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.newer_in","text":"Returns the keys of documents that are newer in the target Store than this Store. Parameters: Name Type Description Default target Store target Store required criteria Optional[Dict] PyMongo filter for documents to search in None exhaustive bool triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in False Source code in maggma/stores/aws.py def newer_in ( self , target : Store , criteria : Optional [ Dict ] = None , exhaustive : bool = False ) -> List [ str ]: \"\"\" Returns the keys of documents that are newer in the target Store than this Store. Args: target: target Store criteria: PyMongo filter for documents to search in exhaustive: triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in \"\"\" if hasattr ( target , \"index\" ): return self . index . newer_in ( target = target . index , criteria = criteria , exhaustive = exhaustive ) else : return self . index . newer_in ( target = target , criteria = criteria , exhaustive = exhaustive )","title":"newer_in()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.query","text":"Queries the Store for a set of documents Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Source code in maggma/stores/aws.py def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" prop_keys = set () if isinstance ( properties , dict ): prop_keys = set ( properties . keys ()) elif isinstance ( properties , list ): prop_keys = set ( properties ) for doc in self . index . query ( criteria = criteria , sort = sort , limit = limit , skip = skip ): if properties is not None and prop_keys . issubset ( set ( doc . keys ())): yield { p : doc [ p ] for p in properties if p in doc } else : try : # TODO: THis is ugly and unsafe, do some real checking before pulling data data = ( self . s3_bucket . Object ( self . sub_dir + str ( doc [ self . key ])) . get ()[ \"Body\" ] . read () ) except botocore . exceptions . ClientError as e : # If a client error is thrown, then check that it was a 404 error. # If it was a 404 error, then the object does not exist. error_code = int ( e . response [ \"Error\" ][ \"Code\" ]) if error_code == 404 : self . logger . error ( \"Could not find S3 object {} \" . format ( doc [ self . key ]) ) break else : raise e if self . unpack_data : data = self . _unpack ( data = data , compressed = doc . get ( \"compression\" , \"\" ) == \"zlib\" ) if self . last_updated_field in doc : data [ self . last_updated_field ] = doc [ self . last_updated_field ] yield data","title":"query()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.rebuild_index_from_s3_data","text":"Rebuilds the index Store from the data in S3 Relies on the index document being stores as the metadata for the file This can help recover lost databases Source code in maggma/stores/aws.py def rebuild_index_from_s3_data ( self , ** kwargs ): \"\"\" Rebuilds the index Store from the data in S3 Relies on the index document being stores as the metadata for the file This can help recover lost databases \"\"\" bucket = self . s3_bucket objects = bucket . objects . filter ( Prefix = self . sub_dir ) for obj in objects : key_ = self . sub_dir + obj . key data = self . s3_bucket . Object ( key_ ) . get ()[ \"Body\" ] . read () if self . compress : data = zlib . decompress ( data ) unpacked_data = msgpack . unpackb ( data , raw = False ) self . update ( unpacked_data , ** kwargs )","title":"rebuild_index_from_s3_data()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.rebuild_metadata_from_index","text":"Read data from the index store and populate the metadata of the S3 bucket Force all of the keys to be lower case to be Minio compatible Parameters: Name Type Description Default index_query dict query on the index store None Source code in maggma/stores/aws.py def rebuild_metadata_from_index ( self , index_query : dict = None ): \"\"\" Read data from the index store and populate the metadata of the S3 bucket Force all of the keys to be lower case to be Minio compatible Args: index_query: query on the index store \"\"\" qq = {} if index_query is None else index_query for index_doc in self . index . query ( qq ): key_ = self . sub_dir + index_doc [ self . key ] s3_object = self . s3_bucket . Object ( key_ ) new_meta = { self . _sanitize_key ( k ): v for k , v in s3_object . metadata . items ()} for k , v in index_doc . items (): new_meta [ str ( k ) . lower ()] = v new_meta . pop ( \"_id\" ) if self . last_updated_field in new_meta : new_meta [ self . last_updated_field ] = str ( to_isoformat_ceil_ms ( new_meta [ self . last_updated_field ]) ) # s3_object.metadata.update(new_meta) s3_object . copy_from ( CopySource = { \"Bucket\" : self . s3_bucket . name , \"Key\" : key_ }, Metadata = new_meta , MetadataDirective = \"REPLACE\" , )","title":"rebuild_metadata_from_index()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.remove_docs","text":"Remove docs matching the query dictionary Parameters: Name Type Description Default criteria Dict query dictionary to match required remove_s3_object bool whether to remove the actual S3 Object or not False Source code in maggma/stores/aws.py def remove_docs ( self , criteria : Dict , remove_s3_object : bool = False ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match remove_s3_object: whether to remove the actual S3 Object or not \"\"\" if not remove_s3_object : self . index . remove_docs ( criteria = criteria ) else : to_remove = self . index . distinct ( self . key , criteria = criteria ) self . index . remove_docs ( criteria = criteria ) # Can remove up to 1000 items at a time via boto to_remove_chunks = list ( grouper ( to_remove , n = 1000 )) for chunk_to_remove in to_remove_chunks : objlist = [{ \"Key\" : f \" { self . sub_dir }{ obj } \" } for obj in chunk_to_remove ] self . s3_bucket . delete_objects ( Delete = { \"Objects\" : objlist })","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.update","text":"Update documents into the Store Parameters: Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None additional_metadata Union[str, List[str]] field(s) to include in the s3 store's metadata None Source code in maggma/stores/aws.py def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None , additional_metadata : Union [ str , List [ str ], None ] = None , ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used additional_metadata: field(s) to include in the s3 store's metadata \"\"\" if not isinstance ( docs , list ): docs = [ docs ] if isinstance ( key , str ): key = [ key ] elif not key : key = [ self . key ] if additional_metadata is None : additional_metadata = [] elif isinstance ( additional_metadata , str ): additional_metadata = [ additional_metadata ] else : additional_metadata = list ( additional_metadata ) with ThreadPoolExecutor ( max_workers = self . s3_workers ) as pool : fs = { pool . submit ( self . write_doc_to_s3 , doc = itr_doc , search_keys = key + additional_metadata + self . searchable_fields , ) for itr_doc in docs } fs , _ = wait ( fs ) search_docs = [ sdoc . result () for sdoc in fs ] # Use store's update to remove key clashes self . index . update ( search_docs , key = self . key )","title":"update()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.write_doc_to_s3","text":"Write the data to s3 and return the metadata to be inserted into the index db Parameters: Name Type Description Default doc Dict the document required search_keys List[str] list of keys to pull from the docs and be inserted into the required Source code in maggma/stores/aws.py def write_doc_to_s3 ( self , doc : Dict , search_keys : List [ str ]): \"\"\" Write the data to s3 and return the metadata to be inserted into the index db Args: doc: the document search_keys: list of keys to pull from the docs and be inserted into the index db \"\"\" s3_bucket = self . _get_bucket () search_doc = { k : doc [ k ] for k in search_keys } search_doc [ self . key ] = doc [ self . key ] # Ensure key is in metadata if self . sub_dir != \"\" : search_doc [ \"sub_dir\" ] = self . sub_dir # Remove MongoDB _id from search if \"_id\" in search_doc : del search_doc [ \"_id\" ] # to make hashing more meaningful, make sure last updated field is removed lu_info = doc . pop ( self . last_updated_field , None ) data = msgpack . packb ( doc , default = monty_default ) if self . compress : # Compress with zlib if chosen search_doc [ \"compression\" ] = \"zlib\" data = zlib . compress ( data ) if self . last_updated_field in doc : # need this conversion for aws metadata insert search_doc [ self . last_updated_field ] = str ( to_isoformat_ceil_ms ( doc [ self . last_updated_field ]) ) # keep a record of original keys, in case these are important for the individual researcher # it is not expected that this information will be used except in disaster recovery s3_to_mongo_keys = { k : self . _sanitize_key ( k ) for k in search_doc . keys ()} s3_to_mongo_keys [ \"s3-to-mongo-keys\" ] = \"s3-to-mongo-keys\" # inception # encode dictionary since values have to be strings search_doc [ \"s3-to-mongo-keys\" ] = dumps ( s3_to_mongo_keys ) s3_bucket . put_object ( Key = self . sub_dir + str ( doc [ self . key ]), Body = data , Metadata = { s3_to_mongo_keys [ k ]: str ( v ) for k , v in search_doc . items ()}, ) if lu_info is not None : search_doc [ self . last_updated_field ] = lu_info if self . store_hash : hasher = sha1 () hasher . update ( data ) obj_hash = hasher . hexdigest () search_doc [ \"obj_hash\" ] = obj_hash return search_doc Advanced Stores for behavior outside normal access patterns","title":"write_doc_to_s3()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore","text":"Special Store that aliases for the primary accessors Source code in maggma/stores/advanced_stores.py class AliasingStore ( Store ): \"\"\" Special Store that aliases for the primary accessors \"\"\" def __init__ ( self , store : Store , aliases : Dict , ** kwargs ): \"\"\" Args: store: the store to wrap around aliases: dict of aliases of the form external key: internal key \"\"\" self . store = store # Given an external key tells what the internal key is self . aliases = aliases # Given the internal key tells us what the external key is self . reverse_aliases = { v : k for k , v in aliases . items ()} self . kwargs = kwargs kwargs . update ( { \"last_updated_field\" : store . last_updated_field , \"last_updated_type\" : store . last_updated_type , } ) super ( AliasingStore , self ) . __init__ ( ** kwargs ) @property def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" return self . store . name def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" criteria = criteria if criteria else {} lazy_substitute ( criteria , self . reverse_aliases ) return self . store . count ( criteria ) def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" criteria = criteria if criteria else {} if properties is not None : if isinstance ( properties , list ): properties = { p : 1 for p in properties } substitute ( properties , self . reverse_aliases ) lazy_substitute ( criteria , self . reverse_aliases ) for d in self . store . query ( properties = properties , criteria = criteria , sort = sort , limit = limit , skip = skip ): substitute ( d , self . aliases ) yield d def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria if criteria else {} lazy_substitute ( criteria , self . reverse_aliases ) # substitute forward return self . store . distinct ( self . aliases [ field ], criteria = criteria ) def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" # Convert to a list keys = keys if isinstance ( keys , list ) else [ keys ] # Make the aliasing transformations on keys keys = [ self . aliases [ k ] if k in self . aliases else k for k in keys ] # Update criteria and properties based on aliases criteria = criteria if criteria else {} if properties is not None : if isinstance ( properties , list ): properties = { p : 1 for p in properties } substitute ( properties , self . reverse_aliases ) lazy_substitute ( criteria , self . reverse_aliases ) return self . store . groupby ( keys = keys , properties = properties , criteria = criteria , skip = skip , limit = limit ) def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" key = key if key else self . key for d in docs : substitute ( d , self . reverse_aliases ) if key in self . aliases : key = self . aliases [ key ] self . store . update ( docs , key = key ) def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" # Update criteria and properties based on aliases lazy_substitute ( criteria , self . reverse_aliases ) self . store . remove_docs ( criteria ) def ensure_index ( self , key , unique = False , ** kwargs ): if key in self . aliases : key = self . aliases return self . store . ensure_index ( key , unique , ** kwargs ) def close ( self ): self . store . close () @property def _collection ( self ): return self . store . _collection def connect ( self , force_reset = False ): self . store . connect ( force_reset = force_reset ) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for AliasingStore Args: other: other AliasingStore to compare with \"\"\" if not isinstance ( other , AliasingStore ): return False fields = [ \"store\" , \"aliases\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"AliasingStore"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.name","text":"Return a string representing this data source","title":"name"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.__eq__","text":"Check equality for AliasingStore Parameters: Name Type Description Default other object other AliasingStore to compare with required Source code in maggma/stores/advanced_stores.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for AliasingStore Args: other: other AliasingStore to compare with \"\"\" if not isinstance ( other , AliasingStore ): return False fields = [ \"store\" , \"aliases\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.__init__","text":"Parameters: Name Type Description Default store Store the store to wrap around required aliases Dict dict of aliases of the form external key: internal key required Source code in maggma/stores/advanced_stores.py def __init__ ( self , store : Store , aliases : Dict , ** kwargs ): \"\"\" Args: store: the store to wrap around aliases: dict of aliases of the form external key: internal key \"\"\" self . store = store # Given an external key tells what the internal key is self . aliases = aliases # Given the internal key tells us what the external key is self . reverse_aliases = { v : k for k , v in aliases . items ()} self . kwargs = kwargs kwargs . update ( { \"last_updated_field\" : store . last_updated_field , \"last_updated_type\" : store . last_updated_type , } ) super ( AliasingStore , self ) . __init__ ( ** kwargs )","title":"__init__()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.close","text":"Closes any connections Source code in maggma/stores/advanced_stores.py def close ( self ): self . store . close ()","title":"close()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.connect","text":"Connect to the source data Parameters: Name Type Description Default force_reset whether to reset the connection or not False Source code in maggma/stores/advanced_stores.py def connect ( self , force_reset = False ): self . store . connect ( force_reset = force_reset )","title":"connect()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.count","text":"Counts the number of documents matching the query criteria Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None Source code in maggma/stores/advanced_stores.py def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" criteria = criteria if criteria else {} lazy_substitute ( criteria , self . reverse_aliases ) return self . store . count ( criteria )","title":"count()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.distinct","text":"Get all distinct values for a field Parameters: Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None Source code in maggma/stores/advanced_stores.py def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria if criteria else {} lazy_substitute ( criteria , self . reverse_aliases ) # substitute forward return self . store . distinct ( self . aliases [ field ], criteria = criteria )","title":"distinct()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.ensure_index","text":"Tries to create an index and return true if it suceeded Parameters: Name Type Description Default key single key to index required unique Whether or not this index contains only unique keys False Returns: Type Description bool indicating if the index exists/was created Source code in maggma/stores/advanced_stores.py def ensure_index ( self , key , unique = False , ** kwargs ): if key in self . aliases : key = self . aliases return self . store . ensure_index ( key , unique , ** kwargs )","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.groupby","text":"Simple grouping function that will group documents by keys. Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) Source code in maggma/stores/advanced_stores.py def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" # Convert to a list keys = keys if isinstance ( keys , list ) else [ keys ] # Make the aliasing transformations on keys keys = [ self . aliases [ k ] if k in self . aliases else k for k in keys ] # Update criteria and properties based on aliases criteria = criteria if criteria else {} if properties is not None : if isinstance ( properties , list ): properties = { p : 1 for p in properties } substitute ( properties , self . reverse_aliases ) lazy_substitute ( criteria , self . reverse_aliases ) return self . store . groupby ( keys = keys , properties = properties , criteria = criteria , skip = skip , limit = limit )","title":"groupby()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.query","text":"Queries the Store for a set of documents Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Source code in maggma/stores/advanced_stores.py def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" criteria = criteria if criteria else {} if properties is not None : if isinstance ( properties , list ): properties = { p : 1 for p in properties } substitute ( properties , self . reverse_aliases ) lazy_substitute ( criteria , self . reverse_aliases ) for d in self . store . query ( properties = properties , criteria = criteria , sort = sort , limit = limit , skip = skip ): substitute ( d , self . aliases ) yield d","title":"query()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.remove_docs","text":"Remove docs matching the query dictionary Parameters: Name Type Description Default criteria Dict query dictionary to match required Source code in maggma/stores/advanced_stores.py def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" # Update criteria and properties based on aliases lazy_substitute ( criteria , self . reverse_aliases ) self . store . remove_docs ( criteria )","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.update","text":"Update documents into the Store Parameters: Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Source code in maggma/stores/advanced_stores.py def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" key = key if key else self . key for d in docs : substitute ( d , self . reverse_aliases ) if key in self . aliases : key = self . aliases [ key ] self . store . update ( docs , key = key )","title":"update()"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore","text":"Initialize a Store with a mongogrant \" <role> : <host> / <db> .\" spec. Some class methods of MongoStore, e.g. from_db_file and from_collection, are not supported. mongogrant documentation: https://github.com/materialsproject/mongogrant Source code in maggma/stores/advanced_stores.py class MongograntStore ( MongoStore ): \"\"\"Initialize a Store with a mongogrant \"`<role>`:`<host>`/`<db>`.\" spec. Some class methods of MongoStore, e.g. from_db_file and from_collection, are not supported. mongogrant documentation: https://github.com/materialsproject/mongogrant \"\"\" def __init__ ( self , mongogrant_spec : str , collection_name : str , mgclient_config_path : Optional [ str ] = None , ** kwargs , ): \"\"\" Args: mongogrant_spec: of the form `<role>`:`<host>`/`<db>`, where role is one of {\"read\", \"readWrite\"} or aliases {\"ro\", \"rw\"}; host is a db host (w/ optional port) or alias; and db is a db on that host, or alias. See mongogrant documentation. collection_name: name of mongo collection mgclient_config_path: Path to mongogrant client config file, or None if default path (`mongogrant.client.path`). \"\"\" self . mongogrant_spec = mongogrant_spec self . collection_name = collection_name self . mgclient_config_path = mgclient_config_path self . _coll = None if self . mgclient_config_path : config = Config ( check = check , path = self . mgclient_config_path ) client = Client ( config ) else : client = Client () if set (( \"username\" , \"password\" , \"database\" , \"host\" )) & set ( kwargs ): raise StoreError ( \"MongograntStore does not accept \" \"username, password, database, or host \" \"arguments. Use `mongogrant_spec`.\" ) self . kwargs = kwargs _auth_info = client . get_db_auth_from_spec ( self . mongogrant_spec ) super ( MongograntStore , self ) . __init__ ( host = _auth_info [ \"host\" ], database = _auth_info [ \"authSource\" ], username = _auth_info [ \"username\" ], password = _auth_info [ \"password\" ], collection_name = self . collection_name , ** kwargs , ) @property def name ( self ): return f \"mgrant:// { self . mongogrant_spec } / { self . collection_name } \" def __hash__ ( self ): return hash ( ( self . mongogrant_spec , self . collection_name , self . last_updated_field ) ) @classmethod def from_db_file ( cls , file ): \"\"\" Raises ValueError since MongograntStores can't be initialized from a file \"\"\" raise ValueError ( \"MongograntStore doesn't implement from_db_file\" ) @classmethod def from_collection ( cls , collection ): \"\"\" Raises ValueError since MongograntStores can't be initialized from a PyMongo collection \"\"\" raise ValueError ( \"MongograntStore doesn't implement from_collection\" ) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MongograntStore Args: other: other MongograntStore to compare with \"\"\" if not isinstance ( other , MongograntStore ): return False fields = [ \"mongogrant_spec\" , \"collection_name\" , \"mgclient_config_path\" , \"last_updated_field\" , ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"MongograntStore"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.name","text":"Return a string representing this data source","title":"name"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.__eq__","text":"Check equality for MongograntStore Parameters: Name Type Description Default other object other MongograntStore to compare with required Source code in maggma/stores/advanced_stores.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MongograntStore Args: other: other MongograntStore to compare with \"\"\" if not isinstance ( other , MongograntStore ): return False fields = [ \"mongogrant_spec\" , \"collection_name\" , \"mgclient_config_path\" , \"last_updated_field\" , ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.__init__","text":"Parameters: Name Type Description Default mongogrant_spec str of the form <role> : <host> / <db> , where role is one of {\"read\", \"readWrite\"} or aliases {\"ro\", \"rw\"}; host is a db host (w/ optional port) or alias; and db is a db on that host, or alias. See mongogrant documentation. required collection_name str name of mongo collection required mgclient_config_path Optional[str] Path to mongogrant client config file, or None if default path ( mongogrant.client.path ). None Source code in maggma/stores/advanced_stores.py def __init__ ( self , mongogrant_spec : str , collection_name : str , mgclient_config_path : Optional [ str ] = None , ** kwargs , ): \"\"\" Args: mongogrant_spec: of the form `<role>`:`<host>`/`<db>`, where role is one of {\"read\", \"readWrite\"} or aliases {\"ro\", \"rw\"}; host is a db host (w/ optional port) or alias; and db is a db on that host, or alias. See mongogrant documentation. collection_name: name of mongo collection mgclient_config_path: Path to mongogrant client config file, or None if default path (`mongogrant.client.path`). \"\"\" self . mongogrant_spec = mongogrant_spec self . collection_name = collection_name self . mgclient_config_path = mgclient_config_path self . _coll = None if self . mgclient_config_path : config = Config ( check = check , path = self . mgclient_config_path ) client = Client ( config ) else : client = Client () if set (( \"username\" , \"password\" , \"database\" , \"host\" )) & set ( kwargs ): raise StoreError ( \"MongograntStore does not accept \" \"username, password, database, or host \" \"arguments. Use `mongogrant_spec`.\" ) self . kwargs = kwargs _auth_info = client . get_db_auth_from_spec ( self . mongogrant_spec ) super ( MongograntStore , self ) . __init__ ( host = _auth_info [ \"host\" ], database = _auth_info [ \"authSource\" ], username = _auth_info [ \"username\" ], password = _auth_info [ \"password\" ], collection_name = self . collection_name , ** kwargs , )","title":"__init__()"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.from_collection","text":"Raises ValueError since MongograntStores can't be initialized from a PyMongo collection Source code in maggma/stores/advanced_stores.py @classmethod def from_collection ( cls , collection ): \"\"\" Raises ValueError since MongograntStores can't be initialized from a PyMongo collection \"\"\" raise ValueError ( \"MongograntStore doesn't implement from_collection\" )","title":"from_collection()"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.from_db_file","text":"Raises ValueError since MongograntStores can't be initialized from a file Source code in maggma/stores/advanced_stores.py @classmethod def from_db_file ( cls , file ): \"\"\" Raises ValueError since MongograntStores can't be initialized from a file \"\"\" raise ValueError ( \"MongograntStore doesn't implement from_db_file\" )","title":"from_db_file()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore","text":"Provides a sandboxed view to another store Source code in maggma/stores/advanced_stores.py class SandboxStore ( Store ): \"\"\" Provides a sandboxed view to another store \"\"\" def __init__ ( self , store : Store , sandbox : str , exclusive : bool = False ): \"\"\" Args: store: store to wrap sandboxing around sandbox: the corresponding sandbox exclusive: whether to be exclusively in this sandbox or include global items \"\"\" self . store = store self . sandbox = sandbox self . exclusive = exclusive super () . __init__ ( key = self . store . key , last_updated_field = self . store . last_updated_field , last_updated_type = self . store . last_updated_type , validator = self . store . validator , ) @property def name ( self ) -> str : \"\"\" Returns: a string representing this data source \"\"\" return f \"Sandbox[ { self . store . name } ][ { self . sandbox } ]\" @property def sbx_criteria ( self ) -> Dict : \"\"\" Returns: the sandbox criteria dict used to filter the source store \"\"\" if self . exclusive : return { \"sbxn\" : self . sandbox } else : return { \"$or\" : [{ \"sbxn\" : { \"$in\" : [ self . sandbox ]}}, { \"sbxn\" : { \"$exists\" : False }}] } def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . count ( criteria = criteria ) def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . query ( properties = properties , criteria = criteria , sort = sort , limit = limit , skip = skip ) def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . groupby ( keys = keys , properties = properties , criteria = criteria , skip = skip , limit = limit ) def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" for d in docs : if \"sbxn\" in d : d [ \"sbxn\" ] = list ( set ( d [ \"sbxn\" ] + [ self . sandbox ])) else : d [ \"sbxn\" ] = [ self . sandbox ] self . store . update ( docs , key = key ) def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" # Update criteria and properties based on aliases criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) self . store . remove_docs ( criteria ) def ensure_index ( self , key , unique = False , ** kwargs ): return self . store . ensure_index ( key , unique , ** kwargs ) def close ( self ): self . store . close () @property def _collection ( self ): return self . store . _collection def connect ( self , force_reset = False ): self . store . connect ( force_reset = force_reset ) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for SandboxStore Args: other: other SandboxStore to compare with \"\"\" if not isinstance ( other , SandboxStore ): return False fields = [ \"store\" , \"sandbox\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"SandboxStore"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.name","text":"Returns: Type Description str a string representing this data source","title":"name"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.sbx_criteria","text":"Returns: Type Description Dict the sandbox criteria dict used to filter the source store","title":"sbx_criteria"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.__eq__","text":"Check equality for SandboxStore Parameters: Name Type Description Default other object other SandboxStore to compare with required Source code in maggma/stores/advanced_stores.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for SandboxStore Args: other: other SandboxStore to compare with \"\"\" if not isinstance ( other , SandboxStore ): return False fields = [ \"store\" , \"sandbox\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.__init__","text":"Parameters: Name Type Description Default store Store store to wrap sandboxing around required sandbox str the corresponding sandbox required exclusive bool whether to be exclusively in this sandbox or include global items False Source code in maggma/stores/advanced_stores.py def __init__ ( self , store : Store , sandbox : str , exclusive : bool = False ): \"\"\" Args: store: store to wrap sandboxing around sandbox: the corresponding sandbox exclusive: whether to be exclusively in this sandbox or include global items \"\"\" self . store = store self . sandbox = sandbox self . exclusive = exclusive super () . __init__ ( key = self . store . key , last_updated_field = self . store . last_updated_field , last_updated_type = self . store . last_updated_type , validator = self . store . validator , )","title":"__init__()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.close","text":"Closes any connections Source code in maggma/stores/advanced_stores.py def close ( self ): self . store . close ()","title":"close()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.connect","text":"Connect to the source data Parameters: Name Type Description Default force_reset whether to reset the connection or not False Source code in maggma/stores/advanced_stores.py def connect ( self , force_reset = False ): self . store . connect ( force_reset = force_reset )","title":"connect()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.count","text":"Counts the number of documents matching the query criteria Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None Source code in maggma/stores/advanced_stores.py def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . count ( criteria = criteria )","title":"count()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.ensure_index","text":"Tries to create an index and return true if it suceeded Parameters: Name Type Description Default key single key to index required unique Whether or not this index contains only unique keys False Returns: Type Description bool indicating if the index exists/was created Source code in maggma/stores/advanced_stores.py def ensure_index ( self , key , unique = False , ** kwargs ): return self . store . ensure_index ( key , unique , ** kwargs )","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.groupby","text":"Simple grouping function that will group documents by keys. Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) Source code in maggma/stores/advanced_stores.py def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . groupby ( keys = keys , properties = properties , criteria = criteria , skip = skip , limit = limit )","title":"groupby()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.query","text":"Queries the Store for a set of documents Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Source code in maggma/stores/advanced_stores.py def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . query ( properties = properties , criteria = criteria , sort = sort , limit = limit , skip = skip )","title":"query()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.remove_docs","text":"Remove docs matching the query dictionary Parameters: Name Type Description Default criteria Dict query dictionary to match required Source code in maggma/stores/advanced_stores.py def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" # Update criteria and properties based on aliases criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) self . store . remove_docs ( criteria )","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.update","text":"Update documents into the Store Parameters: Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Source code in maggma/stores/advanced_stores.py def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" for d in docs : if \"sbxn\" in d : d [ \"sbxn\" ] = list ( set ( d [ \"sbxn\" ] + [ self . sandbox ])) else : d [ \"sbxn\" ] = [ self . sandbox ] self . store . update ( docs , key = key )","title":"update()"},{"location":"reference/stores/#maggma.stores.advanced_stores.VaultStore","text":"Extends MongoStore to read credentials out of Vault server and uses these values to initialize MongoStore instance Source code in maggma/stores/advanced_stores.py class VaultStore ( MongoStore ): \"\"\" Extends MongoStore to read credentials out of Vault server and uses these values to initialize MongoStore instance \"\"\" @requires ( hvac is not None , \"hvac is required to use VaultStore\" ) def __init__ ( self , collection_name : str , vault_secret_path : str ): \"\"\" Args: collection_name: name of mongo collection vault_secret_path: path on vault server with mongo creds object Important: Environment variables that must be set prior to invocation VAULT_ADDR - URL of vault server (eg. https://matgen8.lbl.gov:8200) VAULT_TOKEN or GITHUB_TOKEN - token used to authenticate to vault \"\"\" self . collection_name = collection_name self . vault_secret_path = vault_secret_path # TODO: Switch this over to Pydantic ConfigSettings vault_addr = os . getenv ( \"VAULT_ADDR\" ) if not vault_addr : raise RuntimeError ( \"VAULT_ADDR not set\" ) client = hvac . Client ( vault_addr ) # If we have a vault token use this token = os . getenv ( \"VAULT_TOKEN\" ) # Look for a github token instead if not token : github_token = os . getenv ( \"GITHUB_TOKEN\" ) if github_token : client . auth_github ( github_token ) else : raise RuntimeError ( \"VAULT_TOKEN or GITHUB_TOKEN not set\" ) else : client . token = token if not client . is_authenticated (): raise RuntimeError ( \"Bad token\" ) # Read the vault secret json_db_creds = client . read ( vault_secret_path ) db_creds = json . loads ( json_db_creds [ \"data\" ][ \"value\" ]) database = db_creds . get ( \"db\" ) host = db_creds . get ( \"host\" , \"localhost\" ) port = db_creds . get ( \"port\" , 27017 ) username = db_creds . get ( \"username\" , \"\" ) password = db_creds . get ( \"password\" , \"\" ) super ( VaultStore , self ) . __init__ ( database , collection_name , host , port , username , password ) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for VaultStore Args: other: other VaultStore to compare with \"\"\" if not isinstance ( other , VaultStore ): return False fields = [ \"vault_secret_path\" , \"collection_name\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"VaultStore"},{"location":"reference/stores/#maggma.stores.advanced_stores.VaultStore.__eq__","text":"Check equality for VaultStore Parameters: Name Type Description Default other object other VaultStore to compare with required Source code in maggma/stores/advanced_stores.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for VaultStore Args: other: other VaultStore to compare with \"\"\" if not isinstance ( other , VaultStore ): return False fields = [ \"vault_secret_path\" , \"collection_name\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.advanced_stores.VaultStore.__init__","text":"Parameters: Name Type Description Default collection_name str name of mongo collection required vault_secret_path str path on vault server with mongo creds object required Important Environment variables that must be set prior to invocation VAULT_ADDR - URL of vault server (eg. https://matgen8.lbl.gov:8200) VAULT_TOKEN or GITHUB_TOKEN - token used to authenticate to vault Source code in maggma/stores/advanced_stores.py @requires ( hvac is not None , \"hvac is required to use VaultStore\" ) def __init__ ( self , collection_name : str , vault_secret_path : str ): \"\"\" Args: collection_name: name of mongo collection vault_secret_path: path on vault server with mongo creds object Important: Environment variables that must be set prior to invocation VAULT_ADDR - URL of vault server (eg. https://matgen8.lbl.gov:8200) VAULT_TOKEN or GITHUB_TOKEN - token used to authenticate to vault \"\"\" self . collection_name = collection_name self . vault_secret_path = vault_secret_path # TODO: Switch this over to Pydantic ConfigSettings vault_addr = os . getenv ( \"VAULT_ADDR\" ) if not vault_addr : raise RuntimeError ( \"VAULT_ADDR not set\" ) client = hvac . Client ( vault_addr ) # If we have a vault token use this token = os . getenv ( \"VAULT_TOKEN\" ) # Look for a github token instead if not token : github_token = os . getenv ( \"GITHUB_TOKEN\" ) if github_token : client . auth_github ( github_token ) else : raise RuntimeError ( \"VAULT_TOKEN or GITHUB_TOKEN not set\" ) else : client . token = token if not client . is_authenticated (): raise RuntimeError ( \"Bad token\" ) # Read the vault secret json_db_creds = client . read ( vault_secret_path ) db_creds = json . loads ( json_db_creds [ \"data\" ][ \"value\" ]) database = db_creds . get ( \"db\" ) host = db_creds . get ( \"host\" , \"localhost\" ) port = db_creds . get ( \"port\" , 27017 ) username = db_creds . get ( \"username\" , \"\" ) password = db_creds . get ( \"password\" , \"\" ) super ( VaultStore , self ) . __init__ ( database , collection_name , host , port , username , password ) Special stores that combine underlying Stores together","title":"__init__()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore","text":"Store concatting multiple stores Source code in maggma/stores/compound_stores.py class ConcatStore ( Store ): \"\"\"Store concatting multiple stores\"\"\" def __init__ ( self , stores : List [ Store ], ** kwargs ): \"\"\" Initialize a ConcatStore that concatenates multiple stores together to appear as one store Args: stores: list of stores to concatenate together \"\"\" self . stores = stores self . kwargs = kwargs super ( ConcatStore , self ) . __init__ ( ** kwargs ) @property def name ( self ) -> str : \"\"\" A string representing this data source \"\"\" compound_name = \",\" . join ([ store . name for store in self . stores ]) return f \"Concat[ { compound_name } ]\" def connect ( self , force_reset : bool = False ): \"\"\" Connect all stores in this ConcatStore Args: force_reset: Whether to forcibly reset the connection for all stores \"\"\" for store in self . stores : store . connect ( force_reset ) def close ( self ): \"\"\" Close all connections in this ConcatStore \"\"\" for store in self . stores : store . close () @property def _collection ( self ): raise NotImplementedError ( \"No collection property for ConcatStore\" ) @property def last_updated ( self ) -> datetime : \"\"\" Finds the most recent last_updated across all the stores. This might not be the most usefull way to do this for this type of Store since it could very easily over-estimate the last_updated based on what stores are used \"\"\" lus = [] for store in self . stores : lu = store . last_updated lus . append ( lu ) return max ( lus ) def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Not implemented in ConcatStore Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" raise NotImplementedError ( \"No update method for ConcatStore\" ) def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" distincts = [] for store in self . stores : distincts . extend ( store . distinct ( field = field , criteria = criteria )) return list ( set ( distincts )) def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Ensure an index is properly set. Returns whether all stores support this index or not Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created on all stores \"\"\" return all ([ store . ensure_index ( key , unique ) for store in self . stores ]) def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" counts = [ store . count ( criteria ) for store in self . stores ] return sum ( counts ) def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries across all Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" # TODO: skip, sort and limit are broken. implement properly for store in self . stores : for d in store . query ( criteria = criteria , properties = properties ): yield d def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" if isinstance ( keys , str ): keys = [ keys ] docs = [] for store in self . stores : temp_docs = list ( store . groupby ( keys = keys , criteria = criteria , properties = properties , sort = sort , skip = skip , limit = limit , ) ) for key , group in temp_docs : docs . extend ( group ) def key_set ( d : Dict ) -> Tuple : \"index function based on passed in keys\" test_d = tuple ( d . get ( k , None ) for k in keys ) return test_d sorted_docs = sorted ( docs , key = key_set ) for vals , group_iter in groupby ( sorted_docs , key = key_set ): id_dict = { key : val for key , val in zip ( keys , vals )} yield id_dict , list ( group_iter ) def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" raise NotImplementedError ( \"No remove_docs method for JointStore\" ) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for ConcatStore Args: other: other JointStore to compare with \"\"\" if not isinstance ( other , ConcatStore ): return False fields = [ \"stores\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"ConcatStore"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.last_updated","text":"Finds the most recent last_updated across all the stores. This might not be the most usefull way to do this for this type of Store since it could very easily over-estimate the last_updated based on what stores are used","title":"last_updated"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.name","text":"A string representing this data source","title":"name"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.__eq__","text":"Check equality for ConcatStore Parameters: Name Type Description Default other object other JointStore to compare with required Source code in maggma/stores/compound_stores.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for ConcatStore Args: other: other JointStore to compare with \"\"\" if not isinstance ( other , ConcatStore ): return False fields = [ \"stores\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.__init__","text":"Initialize a ConcatStore that concatenates multiple stores together to appear as one store Parameters: Name Type Description Default stores List[maggma.core.store.Store] list of stores to concatenate together required Source code in maggma/stores/compound_stores.py def __init__ ( self , stores : List [ Store ], ** kwargs ): \"\"\" Initialize a ConcatStore that concatenates multiple stores together to appear as one store Args: stores: list of stores to concatenate together \"\"\" self . stores = stores self . kwargs = kwargs super ( ConcatStore , self ) . __init__ ( ** kwargs )","title":"__init__()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.close","text":"Close all connections in this ConcatStore Source code in maggma/stores/compound_stores.py def close ( self ): \"\"\" Close all connections in this ConcatStore \"\"\" for store in self . stores : store . close ()","title":"close()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.connect","text":"Connect all stores in this ConcatStore Parameters: Name Type Description Default force_reset bool Whether to forcibly reset the connection for all stores False Source code in maggma/stores/compound_stores.py def connect ( self , force_reset : bool = False ): \"\"\" Connect all stores in this ConcatStore Args: force_reset: Whether to forcibly reset the connection for all stores \"\"\" for store in self . stores : store . connect ( force_reset )","title":"connect()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.count","text":"Counts the number of documents matching the query criteria Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None Source code in maggma/stores/compound_stores.py def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" counts = [ store . count ( criteria ) for store in self . stores ] return sum ( counts )","title":"count()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.distinct","text":"Get all distinct values for a field Parameters: Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None Source code in maggma/stores/compound_stores.py def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" distincts = [] for store in self . stores : distincts . extend ( store . distinct ( field = field , criteria = criteria )) return list ( set ( distincts ))","title":"distinct()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.ensure_index","text":"Ensure an index is properly set. Returns whether all stores support this index or not Parameters: Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns: Type Description bool bool indicating if the index exists/was created on all stores Source code in maggma/stores/compound_stores.py def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Ensure an index is properly set. Returns whether all stores support this index or not Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created on all stores \"\"\" return all ([ store . ensure_index ( key , unique ) for store in self . stores ])","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.groupby","text":"Simple grouping function that will group documents by keys. Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) Source code in maggma/stores/compound_stores.py def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" if isinstance ( keys , str ): keys = [ keys ] docs = [] for store in self . stores : temp_docs = list ( store . groupby ( keys = keys , criteria = criteria , properties = properties , sort = sort , skip = skip , limit = limit , ) ) for key , group in temp_docs : docs . extend ( group ) def key_set ( d : Dict ) -> Tuple : \"index function based on passed in keys\" test_d = tuple ( d . get ( k , None ) for k in keys ) return test_d sorted_docs = sorted ( docs , key = key_set ) for vals , group_iter in groupby ( sorted_docs , key = key_set ): id_dict = { key : val for key , val in zip ( keys , vals )} yield id_dict , list ( group_iter )","title":"groupby()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.query","text":"Queries across all Store for a set of documents Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Source code in maggma/stores/compound_stores.py def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries across all Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. skip: number documents to skip limit: limit on total number of documents returned \"\"\" # TODO: skip, sort and limit are broken. implement properly for store in self . stores : for d in store . query ( criteria = criteria , properties = properties ): yield d","title":"query()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.remove_docs","text":"Remove docs matching the query dictionary Parameters: Name Type Description Default criteria Dict query dictionary to match required Source code in maggma/stores/compound_stores.py def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" raise NotImplementedError ( \"No remove_docs method for JointStore\" )","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.update","text":"Update documents into the Store Not implemented in ConcatStore Parameters: Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Source code in maggma/stores/compound_stores.py def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Not implemented in ConcatStore Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" raise NotImplementedError ( \"No update method for ConcatStore\" )","title":"update()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore","text":"Store that implements a on-the-fly join across multiple collections all in the same MongoDB database. This is a Read-Only Store designed to combine data from multiple collections. Source code in maggma/stores/compound_stores.py class JointStore ( Store ): \"\"\" Store that implements a on-the-fly join across multiple collections all in the same MongoDB database. This is a Read-Only Store designed to combine data from multiple collections. \"\"\" def __init__ ( self , database : str , collection_names : List [ str ], host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , main : Optional [ str ] = None , merge_at_root : bool = False , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Args: database: The database name collection_names: list of all collections to join host: Hostname for the database port: TCP port to connect to username: Username for the collection password: Password to connect with main: name for the main collection if not specified this defaults to the first in collection_names list \"\"\" self . database = database self . collection_names = collection_names self . host = host self . port = port self . username = username self . password = password self . _coll = None # type: Any self . main = main or collection_names [ 0 ] self . merge_at_root = merge_at_root self . mongoclient_kwargs = mongoclient_kwargs or {} self . kwargs = kwargs super ( JointStore , self ) . __init__ ( ** kwargs ) @property def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" compound_name = \",\" . join ( self . collection_names ) return f \"Compound[ { self . host } / { self . database } ][ { compound_name } ]\" def connect ( self , force_reset : bool = False ): \"\"\" Connects the underlying Mongo database and all collection connections Args: force_reset: whether to forcibly reset the connection \"\"\" conn = ( MongoClient ( host = self . host , port = self . port , username = self . username , password = self . password , ** self . mongoclient_kwargs , ) if self . username != \"\" else MongoClient ( self . host , self . port , ** self . mongoclient_kwargs ) ) db = conn [ self . database ] self . _coll = db [ self . main ] self . _has_merge_objects = ( self . _collection . database . client . server_info ()[ \"version\" ] > \"3.6\" ) def close ( self ): \"\"\" Closes underlying database connections \"\"\" self . _collection . database . client . close () @property def _collection ( self ): \"\"\"Property referring to the root pymongo collection\"\"\" if self . _coll is None : raise StoreError ( \"Must connect Mongo-like store before attemping to use it\" ) return self . _coll @property def nonmain_names ( self ) -> List : \"\"\" alll non-main collection names \"\"\" return list ( set ( self . collection_names ) - { self . main }) @property def last_updated ( self ) -> datetime : \"\"\" Special last_updated for this JointStore that checks all underlying collections \"\"\" lus = [] for cname in self . collection_names : store = MongoStore . from_collection ( self . _collection . database [ cname ]) store . last_updated_field = self . last_updated_field lu = store . last_updated lus . append ( lu ) return max ( lus ) # TODO: implement update? def update ( self , docs , update_lu = True , key = None , ** kwargs ): \"\"\" Update documents into the underlying collections Not Implemented for JointStore \"\"\" raise NotImplementedError ( \"JointStore is a read-only store\" ) def _get_store_by_name ( self , name ) -> MongoStore : \"\"\" Gets an underlying collection as a mongoStore \"\"\" if name not in self . collection_names : raise ValueError ( \"Asking for collection not referenced in this Store\" ) return MongoStore . from_collection ( self . _collection . database [ name ]) def ensure_index ( self , key , unique = False , ** kwargs ): \"\"\" Can't ensure index for JointStore \"\"\" raise NotImplementedError ( \"No ensure_index method for JointStore\" ) def _get_pipeline ( self , criteria = None , properties = None , skip = 0 , limit = 0 ): \"\"\" Gets the aggregation pipeline for query and query_one Args: properties: properties to be returned criteria: criteria to filter by skip: docs to skip limit: limit results to N docs Returns: list of aggregation operators \"\"\" pipeline = [] collection_names = list ( set ( self . collection_names ) - set ( self . main )) for cname in collection_names : pipeline . append ( { \"$lookup\" : { \"from\" : cname , \"localField\" : self . key , \"foreignField\" : self . key , \"as\" : cname , } } ) if self . merge_at_root : if not self . _has_merge_objects : raise Exception ( \"MongoDB server version too low to use $mergeObjects.\" ) pipeline . append ( { \"$replaceRoot\" : { \"newRoot\" : { \"$mergeObjects\" : [ { \"$arrayElemAt\" : [ \"$ {} \" . format ( cname ), 0 ]}, \"$$ROOT\" , ] } } } ) else : pipeline . append ( { \"$unwind\" : { \"path\" : \"$ {} \" . format ( cname ), \"preserveNullAndEmptyArrays\" : True , } } ) # Do projection for max last_updated lu_max_fields = [ \"$ {} \" . format ( self . last_updated_field )] lu_max_fields . extend ( [ \"$ {} . {} \" . format ( cname , self . last_updated_field ) for cname in self . collection_names ] ) lu_proj = { self . last_updated_field : { \"$max\" : lu_max_fields }} pipeline . append ({ \"$addFields\" : lu_proj }) if criteria : pipeline . append ({ \"$match\" : criteria }) if isinstance ( properties , list ): properties = { k : 1 for k in properties } if properties : pipeline . append ({ \"$project\" : properties }) if skip > 0 : pipeline . append ({ \"$skip\" : skip }) if limit > 0 : pipeline . append ({ \"$limit\" : limit }) return pipeline def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" pipeline = self . _get_pipeline ( criteria = criteria ) pipeline . append ({ \"$count\" : \"count\" }) agg = list ( self . _collection . aggregate ( pipeline )) return agg [ 0 ] . get ( \"count\" , 0 ) if len ( agg ) > 0 else 0 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: pipeline = self . _get_pipeline ( criteria = criteria , properties = properties , skip = skip , limit = limit ) agg = self . _collection . aggregate ( pipeline ) for d in agg : yield d def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: pipeline = self . _get_pipeline ( criteria = criteria , properties = properties , skip = skip , limit = limit ) if not isinstance ( keys , list ): keys = [ keys ] group_id = {} # type: Dict[str,Any] for key in keys : set_ ( group_id , key , \"$ {} \" . format ( key )) pipeline . append ({ \"$group\" : { \"_id\" : group_id , \"docs\" : { \"$push\" : \"$$ROOT\" }}}) agg = self . _collection . aggregate ( pipeline ) for d in agg : yield d [ \"_id\" ], d [ \"docs\" ] def query_one ( self , criteria = None , properties = None , ** kwargs ): \"\"\" Get one document Args: properties: properties to return in query criteria: filter for matching kwargs: kwargs for collection.aggregate Returns: single document \"\"\" # TODO: maybe adding explicit limit in agg pipeline is better as below? # pipeline = self._get_pipeline(properties, criteria) # pipeline.append({\"$limit\": 1}) query = self . query ( criteria = criteria , properties = properties , ** kwargs ) try : doc = next ( query ) return doc except StopIteration : return None def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" raise NotImplementedError ( \"No remove_docs method for JointStore\" ) def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for JointStore Args: other: other JointStore to compare with \"\"\" if not isinstance ( other , JointStore ): return False fields = [ \"database\" , \"collection_names\" , \"host\" , \"port\" , \"main\" , \"merge_at_root\" , ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"JointStore"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.last_updated","text":"Special last_updated for this JointStore that checks all underlying collections","title":"last_updated"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.name","text":"Return a string representing this data source","title":"name"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.nonmain_names","text":"alll non-main collection names","title":"nonmain_names"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.__eq__","text":"Check equality for JointStore Parameters: Name Type Description Default other object other JointStore to compare with required Source code in maggma/stores/compound_stores.py def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for JointStore Args: other: other JointStore to compare with \"\"\" if not isinstance ( other , JointStore ): return False fields = [ \"database\" , \"collection_names\" , \"host\" , \"port\" , \"main\" , \"merge_at_root\" , ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields )","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.__init__","text":"Parameters: Name Type Description Default database str The database name required collection_names List[str] list of all collections to join required host str Hostname for the database 'localhost' port int TCP port to connect to 27017 username str Username for the collection '' password str Password to connect with '' main Optional[str] name for the main collection if not specified this defaults to the first in collection_names list None Source code in maggma/stores/compound_stores.py def __init__ ( self , database : str , collection_names : List [ str ], host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , main : Optional [ str ] = None , merge_at_root : bool = False , mongoclient_kwargs : Optional [ Dict ] = None , ** kwargs , ): \"\"\" Args: database: The database name collection_names: list of all collections to join host: Hostname for the database port: TCP port to connect to username: Username for the collection password: Password to connect with main: name for the main collection if not specified this defaults to the first in collection_names list \"\"\" self . database = database self . collection_names = collection_names self . host = host self . port = port self . username = username self . password = password self . _coll = None # type: Any self . main = main or collection_names [ 0 ] self . merge_at_root = merge_at_root self . mongoclient_kwargs = mongoclient_kwargs or {} self . kwargs = kwargs super ( JointStore , self ) . __init__ ( ** kwargs )","title":"__init__()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.close","text":"Closes underlying database connections Source code in maggma/stores/compound_stores.py def close ( self ): \"\"\" Closes underlying database connections \"\"\" self . _collection . database . client . close ()","title":"close()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.connect","text":"Connects the underlying Mongo database and all collection connections Parameters: Name Type Description Default force_reset bool whether to forcibly reset the connection False Source code in maggma/stores/compound_stores.py def connect ( self , force_reset : bool = False ): \"\"\" Connects the underlying Mongo database and all collection connections Args: force_reset: whether to forcibly reset the connection \"\"\" conn = ( MongoClient ( host = self . host , port = self . port , username = self . username , password = self . password , ** self . mongoclient_kwargs , ) if self . username != \"\" else MongoClient ( self . host , self . port , ** self . mongoclient_kwargs ) ) db = conn [ self . database ] self . _coll = db [ self . main ] self . _has_merge_objects = ( self . _collection . database . client . server_info ()[ \"version\" ] > \"3.6\" )","title":"connect()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.count","text":"Counts the number of documents matching the query criteria Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None Source code in maggma/stores/compound_stores.py def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" pipeline = self . _get_pipeline ( criteria = criteria ) pipeline . append ({ \"$count\" : \"count\" }) agg = list ( self . _collection . aggregate ( pipeline )) return agg [ 0 ] . get ( \"count\" , 0 ) if len ( agg ) > 0 else 0","title":"count()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.ensure_index","text":"Can't ensure index for JointStore Source code in maggma/stores/compound_stores.py def ensure_index ( self , key , unique = False , ** kwargs ): \"\"\" Can't ensure index for JointStore \"\"\" raise NotImplementedError ( \"No ensure_index method for JointStore\" )","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.groupby","text":"Simple grouping function that will group documents by keys. Parameters: Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns: Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) Source code in maggma/stores/compound_stores.py def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: pipeline = self . _get_pipeline ( criteria = criteria , properties = properties , skip = skip , limit = limit ) if not isinstance ( keys , list ): keys = [ keys ] group_id = {} # type: Dict[str,Any] for key in keys : set_ ( group_id , key , \"$ {} \" . format ( key )) pipeline . append ({ \"$group\" : { \"_id\" : group_id , \"docs\" : { \"$push\" : \"$$ROOT\" }}}) agg = self . _collection . aggregate ( pipeline ) for d in agg : yield d [ \"_id\" ], d [ \"docs\" ]","title":"groupby()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.query","text":"Queries the Store for a set of documents Parameters: Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List] properties to return in grouped documents None sort Optional[Dict[str, Union[maggma.core.store.Sort, int]]] Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending. None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Source code in maggma/stores/compound_stores.py def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Union [ Sort , int ]]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: pipeline = self . _get_pipeline ( criteria = criteria , properties = properties , skip = skip , limit = limit ) agg = self . _collection . aggregate ( pipeline ) for d in agg : yield d","title":"query()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.query_one","text":"Get one document Parameters: Name Type Description Default properties properties to return in query None criteria filter for matching None kwargs kwargs for collection.aggregate {} Returns: Type Description single document Source code in maggma/stores/compound_stores.py def query_one ( self , criteria = None , properties = None , ** kwargs ): \"\"\" Get one document Args: properties: properties to return in query criteria: filter for matching kwargs: kwargs for collection.aggregate Returns: single document \"\"\" # TODO: maybe adding explicit limit in agg pipeline is better as below? # pipeline = self._get_pipeline(properties, criteria) # pipeline.append({\"$limit\": 1}) query = self . query ( criteria = criteria , properties = properties , ** kwargs ) try : doc = next ( query ) return doc except StopIteration : return None","title":"query_one()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.remove_docs","text":"Remove docs matching the query dictionary Parameters: Name Type Description Default criteria Dict query dictionary to match required Source code in maggma/stores/compound_stores.py def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" raise NotImplementedError ( \"No remove_docs method for JointStore\" )","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.update","text":"Update documents into the underlying collections Not Implemented for JointStore Source code in maggma/stores/compound_stores.py def update ( self , docs , update_lu = True , key = None , ** kwargs ): \"\"\" Update documents into the underlying collections Not Implemented for JointStore \"\"\" raise NotImplementedError ( \"JointStore is a read-only store\" )","title":"update()"}]}
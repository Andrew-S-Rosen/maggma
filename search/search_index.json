{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Maggma \u00b6 What is Maggma \u00b6 Maggma is a framework to build data pipelines from files on disk all the way to a REST API in scientific environments. Maggma has been developed by the Materials Project (MP) team at Lawrence Berkeley Labs. Maggma is written in Python and supports Python 3.+. Installation from PyPI \u00b6 Maggma is published on the Python Package Index . The preferred tool for installing packages from PyPi is pip . This tool is provided with all modern versions of Python. Open your terminal and run the following command. pip install -U maggma Installation from source \u00b6 You can install Maggma directly from a clone of the Git repository . This can be done either by cloning the repo and installing from the local clone, or simply installing directly via git . Local Clone git clone https://github.com//materialsproject/maggma python setup.py install Direct Git pip install git+https://github.com/materialsproject/maggma","title":"Home"},{"location":"#maggma","text":"","title":"Maggma"},{"location":"#what-is-maggma","text":"Maggma is a framework to build data pipelines from files on disk all the way to a REST API in scientific environments. Maggma has been developed by the Materials Project (MP) team at Lawrence Berkeley Labs. Maggma is written in Python and supports Python 3.+.","title":"What is Maggma"},{"location":"#installation-from-pypi","text":"Maggma is published on the Python Package Index . The preferred tool for installing packages from PyPi is pip . This tool is provided with all modern versions of Python. Open your terminal and run the following command. pip install -U maggma","title":"Installation from PyPI"},{"location":"#installation-from-source","text":"You can install Maggma directly from a clone of the Git repository . This can be done either by cloning the repo and installing from the local clone, or simply installing directly via git . Local Clone git clone https://github.com//materialsproject/maggma python setup.py install Direct Git pip install git+https://github.com/materialsproject/maggma","title":"Installation from source"},{"location":"concepts/","text":"MSONable \u00b6 Maggma objects implement the MSONable pattern which enables these objects to serialize and deserialize to python dictionaries or even JSON. The MSONable encoder injects in @module and @class info so that the object can be deserialized without the manual. This enables much of Maggma to operate like a plugin system. Store \u00b6 Stores are document-based data sources and data sinks. They are modeled around the MongoDB collection although they can represent more complex data sources as well. Stores implement methods to connect , query , find distinct values, groupby fields, update documents, and remove documents. Stores also implement a number of critical fields for Maggma: the key and the last_updated_field . key is the field that is used to index the underlying data source. last_updated_field is the timestamp of when that document. Builder \u00b6 Builders represent a data transformation step. Builders break down each transformation into 3 key steps: get_items , proc es s_item , and update_targets . Both get_items and update_targets can perform IO to the data stores. proc es s_item is expected to not perform any IO so that it can be parallelized by Maggma. Builders can be chained together into a array and then saved as a JSON file to be run on a production system.","title":"Core Concepts"},{"location":"concepts/#msonable","text":"Maggma objects implement the MSONable pattern which enables these objects to serialize and deserialize to python dictionaries or even JSON. The MSONable encoder injects in @module and @class info so that the object can be deserialized without the manual. This enables much of Maggma to operate like a plugin system.","title":"MSONable"},{"location":"concepts/#store","text":"Stores are document-based data sources and data sinks. They are modeled around the MongoDB collection although they can represent more complex data sources as well. Stores implement methods to connect , query , find distinct values, groupby fields, update documents, and remove documents. Stores also implement a number of critical fields for Maggma: the key and the last_updated_field . key is the field that is used to index the underlying data source. last_updated_field is the timestamp of when that document.","title":"Store"},{"location":"concepts/#builder","text":"Builders represent a data transformation step. Builders break down each transformation into 3 key steps: get_items , proc es s_item , and update_targets . Both get_items and update_targets can perform IO to the data stores. proc es s_item is expected to not perform any IO so that it can be parallelized by Maggma. Builders can be chained together into a array and then saved as a JSON file to be run on a production system.","title":"Builder"},{"location":"getting_started/advanced_builder/","text":"Advanced Builder Concepts \u00b6 There are a number of features in maggma designed to assist with advanced features: Logging \u00b6 maggma builders have a python logger object that is already setup to output to the correct level. You can directly use it to output info , debug , and error messages. def get_items ( self ) -> Iterable : ... self . logger . info ( f \"Got {len(to_process_ids)} to process\" ) ... Querying for Updated Documents \u00b6 One of the most important features in a builder is incremental building which allows the builder to just process new documents. One of the parameters for a maggma store is the last_updated_field and the last_updated_type which tell maggma how to deal with dates in the source and target documents. This allows us to get the id of any documents that are newer in the target than the newest document in the source: new_ids = self . target . newer_in ( self . source ) Speeding up IO \u00b6 Since maggma is designed around Mongo style data sources and sinks, building indexes or in-memory copies of fields you want to search on is critical to get the fastest possible IO. Since this is very builder and document style dependent, maggma provides a direct interface to ensure_indexes on a Store. A common paradigm is to do this in the beginning of get_items : def ensure_indexes ( self ): self . source . ensure_index ( \"some_search_fields\" ) self . target . ensure_index ( self . target . key ) def get_items ( self ) -> Iterable : self . ensure_indexes () ... Getting Advanced Features for Free \u00b6 maggma implements standard builders that implement many of these advanced features: MapBuilder","title":"Advanced Builders"},{"location":"getting_started/advanced_builder/#advanced-builder-concepts","text":"There are a number of features in maggma designed to assist with advanced features:","title":"Advanced Builder Concepts"},{"location":"getting_started/advanced_builder/#logging","text":"maggma builders have a python logger object that is already setup to output to the correct level. You can directly use it to output info , debug , and error messages. def get_items ( self ) -> Iterable : ... self . logger . info ( f \"Got {len(to_process_ids)} to process\" ) ...","title":"Logging"},{"location":"getting_started/advanced_builder/#querying-for-updated-documents","text":"One of the most important features in a builder is incremental building which allows the builder to just process new documents. One of the parameters for a maggma store is the last_updated_field and the last_updated_type which tell maggma how to deal with dates in the source and target documents. This allows us to get the id of any documents that are newer in the target than the newest document in the source: new_ids = self . target . newer_in ( self . source )","title":"Querying for Updated Documents"},{"location":"getting_started/advanced_builder/#speeding-up-io","text":"Since maggma is designed around Mongo style data sources and sinks, building indexes or in-memory copies of fields you want to search on is critical to get the fastest possible IO. Since this is very builder and document style dependent, maggma provides a direct interface to ensure_indexes on a Store. A common paradigm is to do this in the beginning of get_items : def ensure_indexes ( self ): self . source . ensure_index ( \"some_search_fields\" ) self . target . ensure_index ( self . target . key ) def get_items ( self ) -> Iterable : self . ensure_indexes () ...","title":"Speeding up IO"},{"location":"getting_started/advanced_builder/#getting-advanced-features-for-free","text":"maggma implements standard builders that implement many of these advanced features: MapBuilder","title":"Getting Advanced Features for Free"},{"location":"getting_started/map_builder/","text":"Map Builder \u00b6 maggma has a built in builder called the MapBuilder which handles a number of tedious tasks in writing a builder. This class is designed to be used similar to a map operator in any other framework in even the map function in python. MapBuilder will take each document in the source store, apply the function you give it, and then store that in the target store. It handles incremental building, keeping track of errors, getting only the data you need, managing timeouts, and deleting orphaned documents through configurational options. Let's rebuild the MultiplierBuilder we wrote earlier using MapBuilder : from maggma.builders import MapBuilder from maggma.core import Store class MultiplyBuilder ( MapBuilder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" Just like before we define a new class, but this time it should inherit from MapBuilder . def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs super () . __init__ ( source = source , target = target , projection = [ \"a\" ], delete_orphans = False , timeout = 10 , store_process_timeout = True , retry_failed = True , ** kwargs ) MapBuilder has a number of configurational options that you can hardcode as above or expose as properties for the user through **kwargs: projection: list of the fields you want to project. This can reduce the IO load if you only need certain keys from the source documents delete_orphans: this will delete documents in the target which don't have a corresponding document in the source timeout: optional timeout on the process function store_process_timeout: adds the process time into the target document for profiling retry_failed: retries running the process function on previously failed documents Finally let's get to the hard part which is running our function. We do this by defining unary_function def unary_function ( self , item ): return { \"a\" : item [ \"a\" ] * self . mulitplier }","title":"Working with MapBuilder"},{"location":"getting_started/map_builder/#map-builder","text":"maggma has a built in builder called the MapBuilder which handles a number of tedious tasks in writing a builder. This class is designed to be used similar to a map operator in any other framework in even the map function in python. MapBuilder will take each document in the source store, apply the function you give it, and then store that in the target store. It handles incremental building, keeping track of errors, getting only the data you need, managing timeouts, and deleting orphaned documents through configurational options. Let's rebuild the MultiplierBuilder we wrote earlier using MapBuilder : from maggma.builders import MapBuilder from maggma.core import Store class MultiplyBuilder ( MapBuilder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" Just like before we define a new class, but this time it should inherit from MapBuilder . def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs super () . __init__ ( source = source , target = target , projection = [ \"a\" ], delete_orphans = False , timeout = 10 , store_process_timeout = True , retry_failed = True , ** kwargs ) MapBuilder has a number of configurational options that you can hardcode as above or expose as properties for the user through **kwargs: projection: list of the fields you want to project. This can reduce the IO load if you only need certain keys from the source documents delete_orphans: this will delete documents in the target which don't have a corresponding document in the source timeout: optional timeout on the process function store_process_timeout: adds the process time into the target document for profiling retry_failed: retries running the process function on previously failed documents Finally let's get to the hard part which is running our function. We do this by defining unary_function def unary_function ( self , item ): return { \"a\" : item [ \"a\" ] * self . mulitplier }","title":"Map Builder"},{"location":"getting_started/running_builders/","text":"Running Builders \u00b6 maggma is designed to run build-pipelines in a production environment. Builders can be run directly in a python environment, but this gives you none of the performance features such as multiprocessing. The base Builder class implements a simple run method that can be used to run that builder: class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" ... my_builder = MultiplyBuilder ( source_store , target_store , multiplier = 3 ) my_builder . run () A better way to run this builder would be to use the mrun command line tool. Since evrything in maggma is MSONable, we can use monty to dump the builders into a JSON file: from monty.serialization import dumpfn dumpfn ( my_builder , \"my_builder.json\" ) Then we can run the builder using mrun : mrun my_builder.json mrun has a number of usefull options: mrun --help Usage: mrun [ OPTIONS ] [ BUILDERS ] ... Options: -v, --verbose Controls logging level per number of v ' s -n, --num-workers INTEGER RANGE Number of worker processes. Defaults to single processing --help Show this message and exit. We can use the -n option to control how many workers run proc es s_items in parallel. Similarly, -v controls the logging verbosity from just WARNINGs to INFO to DEBUG output. The result will be something that looks like this: 2020 -01-08 14 :33:17,187 - Builder - INFO - Starting Builder Builder 2020 -01-08 14 :33:17,217 - Builder - INFO - Processing 100 items Get: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 15366 .00it/s ] 2020 -01-08 14 :33:17,235 - MultiProcessor - INFO - Processing batch of 1000 items Update Targets: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 584 .51it/s ] Process Items: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 567 .39it/s ] There are progress bars for each of the three steps, which lets you understand what the slowest step is and the overall progress of the system.","title":"Running a Builder Pipeline"},{"location":"getting_started/running_builders/#running-builders","text":"maggma is designed to run build-pipelines in a production environment. Builders can be run directly in a python environment, but this gives you none of the performance features such as multiprocessing. The base Builder class implements a simple run method that can be used to run that builder: class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" ... my_builder = MultiplyBuilder ( source_store , target_store , multiplier = 3 ) my_builder . run () A better way to run this builder would be to use the mrun command line tool. Since evrything in maggma is MSONable, we can use monty to dump the builders into a JSON file: from monty.serialization import dumpfn dumpfn ( my_builder , \"my_builder.json\" ) Then we can run the builder using mrun : mrun my_builder.json mrun has a number of usefull options: mrun --help Usage: mrun [ OPTIONS ] [ BUILDERS ] ... Options: -v, --verbose Controls logging level per number of v ' s -n, --num-workers INTEGER RANGE Number of worker processes. Defaults to single processing --help Show this message and exit. We can use the -n option to control how many workers run proc es s_items in parallel. Similarly, -v controls the logging verbosity from just WARNINGs to INFO to DEBUG output. The result will be something that looks like this: 2020 -01-08 14 :33:17,187 - Builder - INFO - Starting Builder Builder 2020 -01-08 14 :33:17,217 - Builder - INFO - Processing 100 items Get: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 15366 .00it/s ] 2020 -01-08 14 :33:17,235 - MultiProcessor - INFO - Processing batch of 1000 items Update Targets: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 584 .51it/s ] Process Items: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 567 .39it/s ] There are progress bars for each of the three steps, which lets you understand what the slowest step is and the overall progress of the system.","title":"Running Builders"},{"location":"getting_started/simple_builder/","text":"Writing a Builder \u00b6 Builder Architecture \u00b6 A Builder is a class that inherits from maggma.core.Builder and implement 3 methods: get_items : This method should return some iterable of items to run through proc es s_items proc es s_item : This method should take a single item, process it, and return the processed item update_targets : This method should take a list of processed items and update the target stores. To make this less abstract, we will write a builder that multiplies the \"a\" sub-document by a pre-configured multiplier . Let's assume we have some source collection in MongoDB with documents that look like this: { \"id\" : 1 , \"a\" : 3 , \"last_updated\" : \"2019-11-3\" } Class definition and __init__ \u00b6 A simple class definition for a Maggma-based builder looks like this: from maggma.core import Builder from maggma.core import Store class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" The __init__ for a builder can have any set of parameters. Generally, you want a source Store and a target Store along with any parameters that configure the builder. Due to the MSONable pattern, any parameters to __init__ have to be stored as attributes. A simple __init__ would look like this: def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) Python type annotations provide a really nice way of documenting the types we expect and being able to later type check using mypy . We defined the type for source and target as Store since we only care that implements that pattern. How exactly these Store s operate doesn't concern us here. Note that the __init__ arguments: source , target , multiplier , and kwargs get saved as attributess: self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs Finally, we want to call the base Builder 's __init__ to tell it our sources and targets for this builder. In addition, we pass along any extra parameters that might configured the base builder class. super () . __init__ ( sources = source , targets = target , ** kwargs ) Calling the parent class __init__ is a good practice as sub-classing builders is a good way to encapsulate complex logic. get_items \u00b6 get_items is conceptually a simple method to implement, but in practice can easily be more code than the rest of the builder. All of the logic for getting data from the sources has to happen here, which requires some planning. get_items should also sort all of the data into induvidual items to process. This simple builder has a very easy get_items : def get_items ( self ) -> Iterator : \"\"\" Gets induvidual documents to multiply \"\"\" return self . source . query () Here, get items just returns the results of query() from the store. It could also have been written as a generator: def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" for doc in self . source . query (): yield doc We could have also returned a list of items: def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" docs = list ( self . source . query ()) proc es s_item \u00b6 proc es s_item just has to do the parallelizable work on each item. Since the item is whatever comes out of get_items , you know exactly what it should be. It may be a single document, a list of documents, a mapping, a set, etc. Our simple process item just has to multiply one field by self.mulitplier : def process_items ( self , item : Dict ) -> Dict : \"\"\" Multiplies the \"a\" sub-document by self.multiplier \"\"\" new_item = dict ( ** item ) new_item [ \"a\" ] *= self . multiplier return new_item update_targets \u00b6 Finally, we have to put the processed item in to the target store: def update_targets ( self , items : List [ Dict ]): \"\"\" Adds the processed items into the target store \"\"\" self . target . update ( items ) Putting it all together we get: from typing import Dict , Iterable , List from maggma.core import Builder from maggma.core import Store class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" docs = list ( self . source . query ()) def process_items ( self , item : Dict ) -> Dict : \"\"\" Multiplies the \"a\" sub-document by self.multiplier \"\"\" new_item = dict ( ** item ) new_item [ \"a\" ] *= self . multiplier return new_item def update_targets ( self , items : List [ Dict ]): \"\"\" Adds the processed items into the target store \"\"\" self . target . update ( items )","title":"Writing a Builder"},{"location":"getting_started/simple_builder/#writing-a-builder","text":"","title":"Writing a Builder"},{"location":"getting_started/simple_builder/#builder-architecture","text":"A Builder is a class that inherits from maggma.core.Builder and implement 3 methods: get_items : This method should return some iterable of items to run through proc es s_items proc es s_item : This method should take a single item, process it, and return the processed item update_targets : This method should take a list of processed items and update the target stores. To make this less abstract, we will write a builder that multiplies the \"a\" sub-document by a pre-configured multiplier . Let's assume we have some source collection in MongoDB with documents that look like this: { \"id\" : 1 , \"a\" : 3 , \"last_updated\" : \"2019-11-3\" }","title":"Builder Architecture"},{"location":"getting_started/simple_builder/#class-definition-and-__init__","text":"A simple class definition for a Maggma-based builder looks like this: from maggma.core import Builder from maggma.core import Store class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" The __init__ for a builder can have any set of parameters. Generally, you want a source Store and a target Store along with any parameters that configure the builder. Due to the MSONable pattern, any parameters to __init__ have to be stored as attributes. A simple __init__ would look like this: def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) Python type annotations provide a really nice way of documenting the types we expect and being able to later type check using mypy . We defined the type for source and target as Store since we only care that implements that pattern. How exactly these Store s operate doesn't concern us here. Note that the __init__ arguments: source , target , multiplier , and kwargs get saved as attributess: self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs Finally, we want to call the base Builder 's __init__ to tell it our sources and targets for this builder. In addition, we pass along any extra parameters that might configured the base builder class. super () . __init__ ( sources = source , targets = target , ** kwargs ) Calling the parent class __init__ is a good practice as sub-classing builders is a good way to encapsulate complex logic.","title":"Class definition and __init__"},{"location":"getting_started/simple_builder/#get_items","text":"get_items is conceptually a simple method to implement, but in practice can easily be more code than the rest of the builder. All of the logic for getting data from the sources has to happen here, which requires some planning. get_items should also sort all of the data into induvidual items to process. This simple builder has a very easy get_items : def get_items ( self ) -> Iterator : \"\"\" Gets induvidual documents to multiply \"\"\" return self . source . query () Here, get items just returns the results of query() from the store. It could also have been written as a generator: def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" for doc in self . source . query (): yield doc We could have also returned a list of items: def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" docs = list ( self . source . query ())","title":"get_items"},{"location":"getting_started/simple_builder/#process_item","text":"proc es s_item just has to do the parallelizable work on each item. Since the item is whatever comes out of get_items , you know exactly what it should be. It may be a single document, a list of documents, a mapping, a set, etc. Our simple process item just has to multiply one field by self.mulitplier : def process_items ( self , item : Dict ) -> Dict : \"\"\" Multiplies the \"a\" sub-document by self.multiplier \"\"\" new_item = dict ( ** item ) new_item [ \"a\" ] *= self . multiplier return new_item","title":"process_item"},{"location":"getting_started/simple_builder/#update_targets","text":"Finally, we have to put the processed item in to the target store: def update_targets ( self , items : List [ Dict ]): \"\"\" Adds the processed items into the target store \"\"\" self . target . update ( items ) Putting it all together we get: from typing import Dict , Iterable , List from maggma.core import Builder from maggma.core import Store class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" docs = list ( self . source . query ()) def process_items ( self , item : Dict ) -> Dict : \"\"\" Multiplies the \"a\" sub-document by self.multiplier \"\"\" new_item = dict ( ** item ) new_item [ \"a\" ] *= self . multiplier return new_item def update_targets ( self , items : List [ Dict ]): \"\"\" Adds the processed items into the target store \"\"\" self . target . update ( items )","title":"update_targets"},{"location":"getting_started/stores/","text":"Using Store \u00b6 A Store is just a wrapper to access data from somewhere. That somewhere is typically a MongoDB collection, but it could also be GridFS which lets you keep large binary objects. maggma makes GridFS and MongoDB collections feel the same. Beyond that it adds in something that looks like GridFS but is actually using AWS S3 as the storage space. Finally, Store can actually perform logic, concatenating two or more Stores together to make them look like one data source for instance. This means you only have to write a Builder for one scenario and the choice of Store lets you control how and where the data goes. List of Stores \u00b6 Current working and tested Stores include: MongoStore: interfaces to a MongoDB Collection MemoryStore: just a Store that exists temporarily in memory JSONStore: buids a MemoryStore and then populates it with the contents of the given JSON files GridFSStore: interfaces to GridFS collection in MongoDB MongograntStore: uses Mongogrant to get credentials for MongoDB database VaulStore: uses Vault to get credentials for a MongoDB database AliasingStore: aliases keys from the underlying store to new names SandboxStore: provides permission control to documents via a _sbxn sandbox key AmazonS3Store: provides an interface to an S3 Bucket JointStore: joins several MongoDB collections together, merging documents with the same key , so they look like one collection ConcatStore: concatenates several MongoDB collections in series so they look like one collection The Store interface \u00b6 Initializing a Store \u00b6 All Store s have a few basic arguments that are critical to understand. Any Store has two special fields: key and last_updated_field . The key defines how the Store tells documents part. Typically this is _id in MongoDB, but you could use your own field. last_updated_field tells Store how to order the documents by a date field. Store s can also take a Validator object to make sure the data going into obeys some schema. Using a Store \u00b6 Stores provide a number of basic methods that make easy to use: query: Standard mongo style find method that lets you search the store. query_one: Same as above but limits to the first document. update: Update the documents into the collection. This will override documents if the key field matches. You can temporarily provide extra fields to key these documents if you don't to maintain duplicates, for instance keying on both key and last_udpated_field . ensure_index: This creates an index the underlying data-source for fast querying. distinct: Gets distinct values of a field. groupby: Similar to query but performs a grouping operation and returns sets of documents. remove_docs: Removes documents from the underlying data source. last_updated: Finds the most recently updated last_updated_field value and returns that. Usefull for knowing how old a data-source is. newer_in: Finds all documents that are newer in the target collection and returns their key s. This is a very useful way of performing incremental processing.","title":"Using Stores"},{"location":"getting_started/stores/#using-store","text":"A Store is just a wrapper to access data from somewhere. That somewhere is typically a MongoDB collection, but it could also be GridFS which lets you keep large binary objects. maggma makes GridFS and MongoDB collections feel the same. Beyond that it adds in something that looks like GridFS but is actually using AWS S3 as the storage space. Finally, Store can actually perform logic, concatenating two or more Stores together to make them look like one data source for instance. This means you only have to write a Builder for one scenario and the choice of Store lets you control how and where the data goes.","title":"Using Store"},{"location":"getting_started/stores/#list-of-stores","text":"Current working and tested Stores include: MongoStore: interfaces to a MongoDB Collection MemoryStore: just a Store that exists temporarily in memory JSONStore: buids a MemoryStore and then populates it with the contents of the given JSON files GridFSStore: interfaces to GridFS collection in MongoDB MongograntStore: uses Mongogrant to get credentials for MongoDB database VaulStore: uses Vault to get credentials for a MongoDB database AliasingStore: aliases keys from the underlying store to new names SandboxStore: provides permission control to documents via a _sbxn sandbox key AmazonS3Store: provides an interface to an S3 Bucket JointStore: joins several MongoDB collections together, merging documents with the same key , so they look like one collection ConcatStore: concatenates several MongoDB collections in series so they look like one collection","title":"List of Stores"},{"location":"getting_started/stores/#the-store-interface","text":"","title":"The Store interface"},{"location":"getting_started/stores/#initializing-a-store","text":"All Store s have a few basic arguments that are critical to understand. Any Store has two special fields: key and last_updated_field . The key defines how the Store tells documents part. Typically this is _id in MongoDB, but you could use your own field. last_updated_field tells Store how to order the documents by a date field. Store s can also take a Validator object to make sure the data going into obeys some schema.","title":"Initializing a Store"},{"location":"getting_started/stores/#using-a-store","text":"Stores provide a number of basic methods that make easy to use: query: Standard mongo style find method that lets you search the store. query_one: Same as above but limits to the first document. update: Update the documents into the collection. This will override documents if the key field matches. You can temporarily provide extra fields to key these documents if you don't to maintain duplicates, for instance keying on both key and last_udpated_field . ensure_index: This creates an index the underlying data-source for fast querying. distinct: Gets distinct values of a field. groupby: Similar to query but performs a grouping operation and returns sets of documents. remove_docs: Removes documents from the underlying data source. last_updated: Finds the most recently updated last_updated_field value and returns that. Usefull for knowing how old a data-source is. newer_in: Finds all documents that are newer in the target collection and returns their key s. This is a very useful way of performing incremental processing.","title":"Using a Store"},{"location":"reference/builders/","text":"Base Builder class to define how builders need to be defined CopyBuilder \u00b6 Sync a source store with a target store. unary_function ( self , item ) \u00b6 Show source code in maggma/builders.py 297 298 299 300 301 302 303 def unary_function ( self , item ): \"\"\" Identity function for copy builder map operation \"\"\" if \"_id\" in item : del item [ \"_id\" ] return item Identity function for copy builder map operation GroupBuilder \u00b6 Group source docs and produce one target doc from each group. Supports incremental building, where a source group gets (re)built only if it has a newer (by last_updated_field) doc than the corresponding (by key) target doc. docs_to_groups ( docs ) (staticmethod) \u00b6 Show source code in maggma/builders.py 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 @staticmethod @abstractmethod def docs_to_groups ( docs : Iterator [ Dict ]) -> List : \"\"\" Yield groups from (minimally-projected) documents. This could be as simple as returning a set of unique document keys. Args: docs: documents with minimal projections needed to determine groups. Returns: iterable: one group at a time \"\"\" Yield groups from (minimally-projected) documents. This could be as simple as returning a set of unique document keys. Parameters Name Type Description Default docs Iterator [ Dict ] documents with minimal projections needed to determine groups. required Returns Type Description List iterable: one group at a time get_items ( self ) \u00b6 Show source code in maggma/builders.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def get_items ( self ) -> Iterator [ Dict ]: \"\"\" Rebuilt get_items for GroupBuilder \"\"\" criteria = { self . source . key : { \"$in\" : self . target . newer_in ( self . source , criteria = self . query ) } } properties = self . grouping_properties () groups = self . docs_to_groups ( self . source . query ( criteria = criteria , properties = properties ) ) self . total = len ( groups ) if hasattr ( self , \"n_items_per_group\" ): n = self . n_items_per_group if isinstance ( n , int ) and n >= 1 : self . total *= n for group in groups : for item in self . group_to_items ( group ): yield item Rebuilt get_items for GroupBuilder group_to_items ( self , group ) \u00b6 Show source code in maggma/builders.py 279 280 281 282 283 284 285 286 287 288 289 290 291 @abstractmethod def group_to_items ( self , group : Dict ) -> Iterator : \"\"\" Given a group, yield items for this builder's process_item method. This method does the work of fetching data needed for processing. Args: group: sufficient as or to produce a source filter Returns: iterable: one or more items per group for process_item. \"\"\" Given a group, yield items for this builder's process_item method. This method does the work of fetching data needed for processing. Parameters Name Type Description Default group Dict sufficient as or to produce a source filter required Returns Type Description Iterator iterable: one or more items per group for process_item. grouping_properties () (staticmethod) \u00b6 Show source code in maggma/builders.py 249 250 251 252 253 254 255 256 257 258 259 260 261 @staticmethod @abstractmethod def grouping_properties () -> Union [ List , Dict ]: \"\"\" Needed projection for docs_to_groups (passed to source.query). Returns: list or dict: of the same form as projection param passed to pymongo.collection.Collection.find. If a list, it is converted to dict form with {\"_id\": 0} unless \"_id\" is explicitly included in the list. This is to ease use of index-covered queries in docs_to_groups. \"\"\" Needed projection for docs_to_groups (passed to source.query). Returns Type Description Union[List, Dict] list or dict: of the same form as projection param passed to pymongo.collection.Collection.find. If a list, it is converted to dict form with MapBuilder \u00b6 Apply a unary function to yield a target document for each source document. Supports incremental building, where a source document gets built only if it has newer (by last_updated_field) data than the corresponding (by key) target document. __init__ ( self , source , target , query = None , projection = None , delete_orphans = False , timeout = 0 , store_process_time = True , retry_failed = False , ** kwargs ) \u00b6 Show source code in maggma/builders.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , source : Store , target : Store , query : Optional [ Dict ] = None , projection : Optional [ List ] = None , delete_orphans : bool = False , timeout : int = 0 , store_process_time : bool = True , retry_failed : bool = False , ** kwargs ): \"\"\" Apply a unary function to each source document. Args: source: source store target: target store query: optional query to filter source store projection: list of keys to project from the source for processing. Limits data transfer to improve efficiency. delete_orphans: Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. timeout: maximum running time per item in seconds store_process_time: If True, add \"_process_time\" key to document for profiling purposes retry_failed: If True, will retry building documents that previously failed \"\"\" self . source = source self . target = target self . query = query self . projection = projection self . delete_orphans = delete_orphans self . kwargs = kwargs self . timeout = timeout self . store_process_time = store_process_time self . retry_failed = retry_failed super () . __init__ ( sources = [ source ], targets = [ target ], ** kwargs ) Apply a unary function to each source document. Parameters Name Type Description Default source Store source store required target Store target store required query Optional [ Dict ] optional query to filter source store None projection Optional [ List ] list of keys to project from the source for processing. Limits data transfer to improve efficiency. None delete_orphans bool Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. False timeout int maximum running time per item in seconds 0 store_process_time bool If True, add \"_process_time\" key to document for profiling purposes True retry_failed bool If True, will retry building documents that previously failed False ensure_indexes ( self ) \u00b6 Show source code in maggma/builders.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def ensure_indexes ( self ): \"\"\" Ensures indicies on critical fields for MapBuilder \"\"\" index_checks = [ self . source . ensure_index ( self . source . key ), self . source . ensure_index ( self . source . last_updated_field ), self . target . ensure_index ( self . target . key ), self . target . ensure_index ( self . target . last_updated_field ), self . target . ensure_index ( \"state\" ), ] if not all ( index_checks ): self . logger . warning ( \"Missing one or more important indices on stores. \" \"Performance for large stores may be severely degraded. \" \"Ensure indices on target.key and \" \"[(store.last_updated_field, -1), (store.key, 1)] \" \"for each of source and target.\" ) Ensures indicies on critical fields for MapBuilder finalize ( self ) \u00b6 Show source code in maggma/builders.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def finalize ( self ): \"\"\" Finalize MapBuilder operations including removing orphaned documents \"\"\" if self . delete_orphans : source_keyvals = set ( self . source . distinct ( self . source . key )) target_keyvals = set ( self . target . distinct ( self . target . key )) to_delete = list ( target_keyvals - source_keyvals ) if len ( to_delete ): self . logger . info ( \"Finalize: Deleting {} orphans.\" . format ( len ( to_delete )) ) self . target . remove_docs ({ self . target . key : { \"$in\" : to_delete }}) super () . finalize () Finalize MapBuilder operations including removing orphaned documents get_items ( self ) \u00b6 Show source code in maggma/builders.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def get_items ( self ): \"\"\" Generic get items for Map Builder designed to perform incremental building \"\"\" self . logger . info ( \"Starting {} Builder\" . format ( self . __class__ . __name__ )) self . ensure_indexes () temp_query = dict ( ** self . query ) if self . query else {} if self . retry_failed : temp_query . pop ( \"state\" , None ) else : temp_query [ \"state\" ] = { \"$ne\" : \"failed\" } keys = self . target . newer_in ( self . source , criteria = self . query , exhaustive = True ) self . logger . info ( \"Processing {} items\" . format ( len ( keys ))) if self . projection : projection = list ( set ( self . projection + [ self . source . key , self . source . last_updated_field ]) ) else : projection = None self . total = len ( keys ) for chunked_keys in grouper ( keys , self . chunk_size , None ): chunked_keys = list ( filter ( None . __ne__ , chunked_keys )) for doc in list ( self . source . query ( criteria = { self . source . key : { \"$in\" : chunked_keys }}, properties = projection , ) ): yield doc Generic get items for Map Builder designed to perform incremental building prechunk ( self , number_splits ) \u00b6 Show source code in maggma/builders.py 87 88 89 90 91 92 93 94 95 96 97 def prechunk ( self , number_splits : int ) -> Iterator [ Dict ]: \"\"\" Generic prechunk for map builder to perform domain-decompostion by the key field \"\"\" self . ensure_indexes () keys = self . target . newer_in ( self . source , criteria = self . query , exhaustive = True ) N = ceil ( len ( keys ) / number_splits ) for split in grouper ( keys , N ): yield { self . source . key : { \"$in\" : list ( filter ( None . __ne__ , split ))}} Generic prechunk for map builder to perform domain-decompostion by the key field process_item ( self , item ) \u00b6 Show source code in maggma/builders.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def process_item ( self , item : Dict ): \"\"\" Generic process items to process a dictionary using a map function \"\"\" self . logger . debug ( \"Processing: {} \" . format ( item [ self . source . key ])) time_start = time () try : with Timeout ( seconds = self . timeout ): processed = self . unary_function ( item ) processed . update ({ \"state\" : \"successful\" }) except Exception as e : self . logger . error ( traceback . format_exc ()) processed = { \"error\" : str ( e ), \"state\" : \"failed\" } time_end = time () key , last_updated_field = self . source . key , self . source . last_updated_field out = { self . target . key : item [ key ], self . target . last_updated_field : self . source . _lu_func [ 0 ]( item [ last_updated_field ] ), } if self . store_process_time : out [ \"_process_time\" ] = time_end - time_start out . update ( processed ) return out Generic process items to process a dictionary using a map function unary_function ( self , item ) \u00b6 Show source code in maggma/builders.py 205 206 207 208 209 210 211 212 213 214 215 @abstractmethod def unary_function ( self , item ): \"\"\" ufn: Unary function to process item You do not need to provide values for source.key and source.last_updated_field in the output. Any uncaught exceptions will be caught by process_item and logged to the \"error\" field in the target document. \"\"\" pass ufn: Unary function to process item You do not need to provide values for source.key and source.last_updated_field in the output. Any uncaught exceptions will be caught by process_item and logged to the \"error\" field in the target document. update_targets ( self , items ) \u00b6 Show source code in maggma/builders.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def update_targets ( self , items : List [ Dict ]): \"\"\" Generic update targets for Map Builder \"\"\" source , target = self . source , self . target for item in items : # Use source last-updated value, ensuring `datetime` type. item [ target . last_updated_field ] = source . _lu_func [ 0 ]( item [ source . last_updated_field ] ) if source . last_updated_field != target . last_updated_field : del item [ source . last_updated_field ] item [ \"_bt\" ] = datetime . utcnow () if \"_id\" in item : del item [ \"_id\" ] if len ( items ) > 0 : target . update ( items ) Generic update targets for Map Builder","title":"Builders"},{"location":"reference/builders/#maggma.builders.CopyBuilder","text":"Sync a source store with a target store.","title":"CopyBuilder"},{"location":"reference/builders/#maggma.builders.CopyBuilder.unary_function","text":"Show source code in maggma/builders.py 297 298 299 300 301 302 303 def unary_function ( self , item ): \"\"\" Identity function for copy builder map operation \"\"\" if \"_id\" in item : del item [ \"_id\" ] return item Identity function for copy builder map operation","title":"unary_function()"},{"location":"reference/builders/#maggma.builders.GroupBuilder","text":"Group source docs and produce one target doc from each group. Supports incremental building, where a source group gets (re)built only if it has a newer (by last_updated_field) doc than the corresponding (by key) target doc.","title":"GroupBuilder"},{"location":"reference/builders/#maggma.builders.GroupBuilder.docs_to_groups","text":"Show source code in maggma/builders.py 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 @staticmethod @abstractmethod def docs_to_groups ( docs : Iterator [ Dict ]) -> List : \"\"\" Yield groups from (minimally-projected) documents. This could be as simple as returning a set of unique document keys. Args: docs: documents with minimal projections needed to determine groups. Returns: iterable: one group at a time \"\"\" Yield groups from (minimally-projected) documents. This could be as simple as returning a set of unique document keys. Parameters Name Type Description Default docs Iterator [ Dict ] documents with minimal projections needed to determine groups. required Returns Type Description List iterable: one group at a time","title":"docs_to_groups()"},{"location":"reference/builders/#maggma.builders.GroupBuilder.get_items","text":"Show source code in maggma/builders.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def get_items ( self ) -> Iterator [ Dict ]: \"\"\" Rebuilt get_items for GroupBuilder \"\"\" criteria = { self . source . key : { \"$in\" : self . target . newer_in ( self . source , criteria = self . query ) } } properties = self . grouping_properties () groups = self . docs_to_groups ( self . source . query ( criteria = criteria , properties = properties ) ) self . total = len ( groups ) if hasattr ( self , \"n_items_per_group\" ): n = self . n_items_per_group if isinstance ( n , int ) and n >= 1 : self . total *= n for group in groups : for item in self . group_to_items ( group ): yield item Rebuilt get_items for GroupBuilder","title":"get_items()"},{"location":"reference/builders/#maggma.builders.GroupBuilder.group_to_items","text":"Show source code in maggma/builders.py 279 280 281 282 283 284 285 286 287 288 289 290 291 @abstractmethod def group_to_items ( self , group : Dict ) -> Iterator : \"\"\" Given a group, yield items for this builder's process_item method. This method does the work of fetching data needed for processing. Args: group: sufficient as or to produce a source filter Returns: iterable: one or more items per group for process_item. \"\"\" Given a group, yield items for this builder's process_item method. This method does the work of fetching data needed for processing. Parameters Name Type Description Default group Dict sufficient as or to produce a source filter required Returns Type Description Iterator iterable: one or more items per group for process_item.","title":"group_to_items()"},{"location":"reference/builders/#maggma.builders.GroupBuilder.grouping_properties","text":"Show source code in maggma/builders.py 249 250 251 252 253 254 255 256 257 258 259 260 261 @staticmethod @abstractmethod def grouping_properties () -> Union [ List , Dict ]: \"\"\" Needed projection for docs_to_groups (passed to source.query). Returns: list or dict: of the same form as projection param passed to pymongo.collection.Collection.find. If a list, it is converted to dict form with {\"_id\": 0} unless \"_id\" is explicitly included in the list. This is to ease use of index-covered queries in docs_to_groups. \"\"\" Needed projection for docs_to_groups (passed to source.query). Returns Type Description Union[List, Dict] list or dict: of the same form as projection param passed to pymongo.collection.Collection.find. If a list, it is converted to dict form with","title":"grouping_properties()"},{"location":"reference/builders/#maggma.builders.MapBuilder","text":"Apply a unary function to yield a target document for each source document. Supports incremental building, where a source document gets built only if it has newer (by last_updated_field) data than the corresponding (by key) target document.","title":"MapBuilder"},{"location":"reference/builders/#maggma.builders.MapBuilder.__init__","text":"Show source code in maggma/builders.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , source : Store , target : Store , query : Optional [ Dict ] = None , projection : Optional [ List ] = None , delete_orphans : bool = False , timeout : int = 0 , store_process_time : bool = True , retry_failed : bool = False , ** kwargs ): \"\"\" Apply a unary function to each source document. Args: source: source store target: target store query: optional query to filter source store projection: list of keys to project from the source for processing. Limits data transfer to improve efficiency. delete_orphans: Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. timeout: maximum running time per item in seconds store_process_time: If True, add \"_process_time\" key to document for profiling purposes retry_failed: If True, will retry building documents that previously failed \"\"\" self . source = source self . target = target self . query = query self . projection = projection self . delete_orphans = delete_orphans self . kwargs = kwargs self . timeout = timeout self . store_process_time = store_process_time self . retry_failed = retry_failed super () . __init__ ( sources = [ source ], targets = [ target ], ** kwargs ) Apply a unary function to each source document. Parameters Name Type Description Default source Store source store required target Store target store required query Optional [ Dict ] optional query to filter source store None projection Optional [ List ] list of keys to project from the source for processing. Limits data transfer to improve efficiency. None delete_orphans bool Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. False timeout int maximum running time per item in seconds 0 store_process_time bool If True, add \"_process_time\" key to document for profiling purposes True retry_failed bool If True, will retry building documents that previously failed False","title":"__init__()"},{"location":"reference/builders/#maggma.builders.MapBuilder.ensure_indexes","text":"Show source code in maggma/builders.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def ensure_indexes ( self ): \"\"\" Ensures indicies on critical fields for MapBuilder \"\"\" index_checks = [ self . source . ensure_index ( self . source . key ), self . source . ensure_index ( self . source . last_updated_field ), self . target . ensure_index ( self . target . key ), self . target . ensure_index ( self . target . last_updated_field ), self . target . ensure_index ( \"state\" ), ] if not all ( index_checks ): self . logger . warning ( \"Missing one or more important indices on stores. \" \"Performance for large stores may be severely degraded. \" \"Ensure indices on target.key and \" \"[(store.last_updated_field, -1), (store.key, 1)] \" \"for each of source and target.\" ) Ensures indicies on critical fields for MapBuilder","title":"ensure_indexes()"},{"location":"reference/builders/#maggma.builders.MapBuilder.finalize","text":"Show source code in maggma/builders.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def finalize ( self ): \"\"\" Finalize MapBuilder operations including removing orphaned documents \"\"\" if self . delete_orphans : source_keyvals = set ( self . source . distinct ( self . source . key )) target_keyvals = set ( self . target . distinct ( self . target . key )) to_delete = list ( target_keyvals - source_keyvals ) if len ( to_delete ): self . logger . info ( \"Finalize: Deleting {} orphans.\" . format ( len ( to_delete )) ) self . target . remove_docs ({ self . target . key : { \"$in\" : to_delete }}) super () . finalize () Finalize MapBuilder operations including removing orphaned documents","title":"finalize()"},{"location":"reference/builders/#maggma.builders.MapBuilder.get_items","text":"Show source code in maggma/builders.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def get_items ( self ): \"\"\" Generic get items for Map Builder designed to perform incremental building \"\"\" self . logger . info ( \"Starting {} Builder\" . format ( self . __class__ . __name__ )) self . ensure_indexes () temp_query = dict ( ** self . query ) if self . query else {} if self . retry_failed : temp_query . pop ( \"state\" , None ) else : temp_query [ \"state\" ] = { \"$ne\" : \"failed\" } keys = self . target . newer_in ( self . source , criteria = self . query , exhaustive = True ) self . logger . info ( \"Processing {} items\" . format ( len ( keys ))) if self . projection : projection = list ( set ( self . projection + [ self . source . key , self . source . last_updated_field ]) ) else : projection = None self . total = len ( keys ) for chunked_keys in grouper ( keys , self . chunk_size , None ): chunked_keys = list ( filter ( None . __ne__ , chunked_keys )) for doc in list ( self . source . query ( criteria = { self . source . key : { \"$in\" : chunked_keys }}, properties = projection , ) ): yield doc Generic get items for Map Builder designed to perform incremental building","title":"get_items()"},{"location":"reference/builders/#maggma.builders.MapBuilder.prechunk","text":"Show source code in maggma/builders.py 87 88 89 90 91 92 93 94 95 96 97 def prechunk ( self , number_splits : int ) -> Iterator [ Dict ]: \"\"\" Generic prechunk for map builder to perform domain-decompostion by the key field \"\"\" self . ensure_indexes () keys = self . target . newer_in ( self . source , criteria = self . query , exhaustive = True ) N = ceil ( len ( keys ) / number_splits ) for split in grouper ( keys , N ): yield { self . source . key : { \"$in\" : list ( filter ( None . __ne__ , split ))}} Generic prechunk for map builder to perform domain-decompostion by the key field","title":"prechunk()"},{"location":"reference/builders/#maggma.builders.MapBuilder.process_item","text":"Show source code in maggma/builders.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def process_item ( self , item : Dict ): \"\"\" Generic process items to process a dictionary using a map function \"\"\" self . logger . debug ( \"Processing: {} \" . format ( item [ self . source . key ])) time_start = time () try : with Timeout ( seconds = self . timeout ): processed = self . unary_function ( item ) processed . update ({ \"state\" : \"successful\" }) except Exception as e : self . logger . error ( traceback . format_exc ()) processed = { \"error\" : str ( e ), \"state\" : \"failed\" } time_end = time () key , last_updated_field = self . source . key , self . source . last_updated_field out = { self . target . key : item [ key ], self . target . last_updated_field : self . source . _lu_func [ 0 ]( item [ last_updated_field ] ), } if self . store_process_time : out [ \"_process_time\" ] = time_end - time_start out . update ( processed ) return out Generic process items to process a dictionary using a map function","title":"process_item()"},{"location":"reference/builders/#maggma.builders.MapBuilder.unary_function","text":"Show source code in maggma/builders.py 205 206 207 208 209 210 211 212 213 214 215 @abstractmethod def unary_function ( self , item ): \"\"\" ufn: Unary function to process item You do not need to provide values for source.key and source.last_updated_field in the output. Any uncaught exceptions will be caught by process_item and logged to the \"error\" field in the target document. \"\"\" pass ufn: Unary function to process item You do not need to provide values for source.key and source.last_updated_field in the output. Any uncaught exceptions will be caught by process_item and logged to the \"error\" field in the target document.","title":"unary_function()"},{"location":"reference/builders/#maggma.builders.MapBuilder.update_targets","text":"Show source code in maggma/builders.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def update_targets ( self , items : List [ Dict ]): \"\"\" Generic update targets for Map Builder \"\"\" source , target = self . source , self . target for item in items : # Use source last-updated value, ensuring `datetime` type. item [ target . last_updated_field ] = source . _lu_func [ 0 ]( item [ source . last_updated_field ] ) if source . last_updated_field != target . last_updated_field : del item [ source . last_updated_field ] item [ \"_bt\" ] = datetime . utcnow () if \"_id\" in item : del item [ \"_id\" ] if len ( items ) > 0 : target . update ( items ) Generic update targets for Map Builder","title":"update_targets()"},{"location":"reference/core_builder/","text":"Module containing the core builder definition Builder \u00b6 Base Builder class At minimum this class should implement: get_items - Get items from the sources update_targets - Updates the sources with results Multiprocessing and MPI processing can be used if all the data processing is limited to process_items __init__ ( self , sources , targets , chunk_size = 1000 , query = None ) \u00b6 Show source code in maggma/core/builder.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , sources : Union [ List [ Store ], Store ], targets : Union [ List [ Store ], Store ], chunk_size : int = 1000 , query : Optional [ Dict ] = None , ): \"\"\" Initialize the builder the framework. Arguments: sources: source Store(s) targets: target Store(s) chunk_size: chunk size for processing query: dictionary of options to utilize on a source; Each builder has internal logic on which souce this will apply to \"\"\" self . sources = sources if isinstance ( sources , list ) else [ sources ] self . targets = targets if isinstance ( targets , list ) else [ targets ] self . chunk_size = chunk_size self . query = query self . total = None # type: Optional[int] self . logger = logging . getLogger ( type ( self ) . __name__ ) self . logger . addHandler ( logging . NullHandler ()) Initialize the builder the framework. Parameters Name Type Description Default sources Union [ List[Store ] , Store ] source Store(s) required targets Union [ List[Store ] , Store ] target Store(s) required chunk_size int chunk size for processing 1000 query Optional [ Dict ] dictionary of options to utilize on a source; Each builder has internal logic on which souce this will apply to None connect ( self ) \u00b6 Show source code in maggma/core/builder.py 52 53 54 55 56 57 def connect ( self ): \"\"\" Connect to the builder sources and targets. \"\"\" for s in self . sources + self . targets : s . connect () Connect to the builder sources and targets. finalize ( self ) \u00b6 Show source code in maggma/core/builder.py 110 111 112 113 114 115 116 117 118 119 def finalize ( self ): \"\"\" Perform any final clean up. \"\"\" # Close any Mongo connections. for store in self . sources + self . targets : try : store . close () except AttributeError : continue Perform any final clean up. get_items ( self ) \u00b6 Show source code in maggma/core/builder.py 74 75 76 77 78 79 80 81 82 @abstractmethod def get_items ( self ) -> Iterable : \"\"\" Returns all the items to process. Returns: generator or list of items to process \"\"\" pass Returns all the items to process. Returns Type Description Iterable generator or list of items to process prechunk ( self , number_splits ) \u00b6 Show source code in maggma/core/builder.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def prechunk ( self , number_splits : int ) -> Iterable [ Dict ]: \"\"\" Part of a domain-decomposition paradigm to allow the builder to operate on multiple nodes by divinding up the IO as well as the compute This function should return an iterator of dictionaries that can be distributed to multiple instances of the builder to get/process/udpate on Arguments: number_splits: The number of groups to split the documents to work on \"\"\" if self . query : return [ self . query ] else : return [] Part of a domain-decomposition paradigm to allow the builder to operate on multiple nodes by divinding up the IO as well as the compute This function should return an iterator of dictionaries that can be distributed to multiple instances of the builder to get/process/udpate on Parameters Name Type Description Default number_splits int The number of groups to split the documents to work on required process_item ( self , item ) \u00b6 Show source code in maggma/core/builder.py 84 85 86 87 88 89 90 91 92 93 94 def process_item ( self , item : Any ) -> Any : \"\"\" Process an item. There should be no database operations in this method. Default behavior is to return the item. Arguments: item: Returns: item: an item to update \"\"\" return item Process an item. There should be no database operations in this method. Default behavior is to return the item. Parameters Name Type Description Default item Any required Returns Type Description Any item: an item to update run ( self ) \u00b6 Show source code in maggma/core/builder.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def run ( self ): \"\"\" Run the builder serially This is only intended for diagnostic purposes \"\"\" self . connect () cursor = self . get_items () for chunk in grouper ( cursor , self . chunk_size ): self . logger . info ( \"Processing batch of {} items\" . format ( self . chunk_size )) processed_items = [ self . process_item ( item ) for item in chunk if item is not None ] self . update_targets ( processed_items ) self . finalize () Run the builder serially This is only intended for diagnostic purposes update_targets ( self , items ) \u00b6 Show source code in maggma/core/builder.py 96 97 98 99 100 101 102 103 104 105 106 107 108 @abstractmethod def update_targets ( self , items : List ): \"\"\" Takes a list of items from process item and updates the targets with them. Can also perform other book keeping in the process such as storing gridfs oids, etc. Arguments: items: Returns: \"\"\" pass Takes a list of items from process item and updates the targets with them. Can also perform other book keeping in the process such as storing gridfs oids, etc. Parameters Name Type Description Default items List required Returns Type Description _empty","title":"Builder"},{"location":"reference/core_builder/#maggma.core.builder.Builder","text":"Base Builder class At minimum this class should implement: get_items - Get items from the sources update_targets - Updates the sources with results Multiprocessing and MPI processing can be used if all the data processing is limited to process_items","title":"Builder"},{"location":"reference/core_builder/#maggma.core.builder.Builder.__init__","text":"Show source code in maggma/core/builder.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , sources : Union [ List [ Store ], Store ], targets : Union [ List [ Store ], Store ], chunk_size : int = 1000 , query : Optional [ Dict ] = None , ): \"\"\" Initialize the builder the framework. Arguments: sources: source Store(s) targets: target Store(s) chunk_size: chunk size for processing query: dictionary of options to utilize on a source; Each builder has internal logic on which souce this will apply to \"\"\" self . sources = sources if isinstance ( sources , list ) else [ sources ] self . targets = targets if isinstance ( targets , list ) else [ targets ] self . chunk_size = chunk_size self . query = query self . total = None # type: Optional[int] self . logger = logging . getLogger ( type ( self ) . __name__ ) self . logger . addHandler ( logging . NullHandler ()) Initialize the builder the framework. Parameters Name Type Description Default sources Union [ List[Store ] , Store ] source Store(s) required targets Union [ List[Store ] , Store ] target Store(s) required chunk_size int chunk size for processing 1000 query Optional [ Dict ] dictionary of options to utilize on a source; Each builder has internal logic on which souce this will apply to None","title":"__init__()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.connect","text":"Show source code in maggma/core/builder.py 52 53 54 55 56 57 def connect ( self ): \"\"\" Connect to the builder sources and targets. \"\"\" for s in self . sources + self . targets : s . connect () Connect to the builder sources and targets.","title":"connect()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.finalize","text":"Show source code in maggma/core/builder.py 110 111 112 113 114 115 116 117 118 119 def finalize ( self ): \"\"\" Perform any final clean up. \"\"\" # Close any Mongo connections. for store in self . sources + self . targets : try : store . close () except AttributeError : continue Perform any final clean up.","title":"finalize()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.get_items","text":"Show source code in maggma/core/builder.py 74 75 76 77 78 79 80 81 82 @abstractmethod def get_items ( self ) -> Iterable : \"\"\" Returns all the items to process. Returns: generator or list of items to process \"\"\" pass Returns all the items to process. Returns Type Description Iterable generator or list of items to process","title":"get_items()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.prechunk","text":"Show source code in maggma/core/builder.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def prechunk ( self , number_splits : int ) -> Iterable [ Dict ]: \"\"\" Part of a domain-decomposition paradigm to allow the builder to operate on multiple nodes by divinding up the IO as well as the compute This function should return an iterator of dictionaries that can be distributed to multiple instances of the builder to get/process/udpate on Arguments: number_splits: The number of groups to split the documents to work on \"\"\" if self . query : return [ self . query ] else : return [] Part of a domain-decomposition paradigm to allow the builder to operate on multiple nodes by divinding up the IO as well as the compute This function should return an iterator of dictionaries that can be distributed to multiple instances of the builder to get/process/udpate on Parameters Name Type Description Default number_splits int The number of groups to split the documents to work on required","title":"prechunk()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.process_item","text":"Show source code in maggma/core/builder.py 84 85 86 87 88 89 90 91 92 93 94 def process_item ( self , item : Any ) -> Any : \"\"\" Process an item. There should be no database operations in this method. Default behavior is to return the item. Arguments: item: Returns: item: an item to update \"\"\" return item Process an item. There should be no database operations in this method. Default behavior is to return the item. Parameters Name Type Description Default item Any required Returns Type Description Any item: an item to update","title":"process_item()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.run","text":"Show source code in maggma/core/builder.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def run ( self ): \"\"\" Run the builder serially This is only intended for diagnostic purposes \"\"\" self . connect () cursor = self . get_items () for chunk in grouper ( cursor , self . chunk_size ): self . logger . info ( \"Processing batch of {} items\" . format ( self . chunk_size )) processed_items = [ self . process_item ( item ) for item in chunk if item is not None ] self . update_targets ( processed_items ) self . finalize () Run the builder serially This is only intended for diagnostic purposes","title":"run()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.update_targets","text":"Show source code in maggma/core/builder.py 96 97 98 99 100 101 102 103 104 105 106 107 108 @abstractmethod def update_targets ( self , items : List ): \"\"\" Takes a list of items from process item and updates the targets with them. Can also perform other book keeping in the process such as storing gridfs oids, etc. Arguments: items: Returns: \"\"\" pass Takes a list of items from process item and updates the targets with them. Can also perform other book keeping in the process such as storing gridfs oids, etc. Parameters Name Type Description Default items List required Returns Type Description _empty","title":"update_targets()"},{"location":"reference/core_store/","text":"Module containing the core Store definition DateTimeFormat \u00b6 Datetime format in store document Sort \u00b6 Enumeration for sorting order Store \u00b6 Abstract class for a data Store Defines the interface for all data going in and out of a Builder last_updated : datetime (property, readonly) \u00b6 Provides the most recent last_updated date time stamp from the documents in this Store name : str (property, readonly) \u00b6 Return a string representing this data source __init__ ( self , key = 'task_id' , last_updated_field = 'last_updated' , last_updated_type =< DateTimeFormat . DateTime : 'datetime' > , validator = None ) \u00b6 Show source code in maggma/core/store.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , key : str = \"task_id\" , last_updated_field : str = \"last_updated\" , last_updated_type : DateTimeFormat = DateTimeFormat ( \"datetime\" ), validator : Optional [ Validator ] = None , ): \"\"\" Args: key: master key to index on last_updated_field: field for date/time stamping the data last_updated_type: the date/time format for the last_updated_field. Can be \"datetime\" or \"isoformat\" validator: Validator to validate documents going into the store \"\"\" self . key = key self . last_updated_field = last_updated_field self . last_updated_type = last_updated_type self . _lu_func = ( LU_KEY_ISOFORMAT if last_updated_type == DateTimeFormat . IsoFormat else ( identity , identity ) ) # type: Tuple[Callable, Callable] self . validator = validator self . logger = logging . getLogger ( type ( self ) . __name__ ) self . logger . addHandler ( logging . NullHandler ()) Parameters Name Type Description Default key str master key to index on task_id last_updated_field str field for date/time stamping the data last_updated last_updated_type DateTimeFormat the date/time format for the last_updated_field. Can be \"datetime\" or \"isoformat\" DateTimeFormat.DateTime validator Optional [ Validator ] Validator to validate documents going into the store None close ( self ) \u00b6 Show source code in maggma/core/store.py 93 94 95 96 97 @abstractmethod def close ( self ): \"\"\" Closes any connections \"\"\" Closes any connections connect ( self , force_reset = False ) \u00b6 Show source code in maggma/core/store.py 84 85 86 87 88 89 90 91 @abstractmethod def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data Args: force_reset: whether to reset the connection or not \"\"\" Connect to the source data Parameters Name Type Description Default force_reset bool whether to reset the connection or not False distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Show source code in maggma/core/store.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def distinct ( self , field : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , all_exist : bool = False , ) -> List : \"\"\" Get all distinct values for a field(s) For a single field, this returns a list of values For multiple fields, this return a list of of dictionaries for each unique combination Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in all_exist: ensure all fields exist for the distinct set \"\"\" field = field if isinstance ( field , list ) else [ field ] criteria = criteria or {} if all_exist : criteria . update ({ f : { \"$exists\" : 1 } for f in field if f not in criteria }) results = [ key for key , _ in self . groupby ( field , properties = field , criteria = criteria ) ] # Flatten out results if searching for a single field if len ( field ) == 1 : results = [ get ( r , field [ 0 ]) for r in results ] return results Get all distinct values for a field(s) For a single field, this returns a list of values For multiple fields, this return a list of of dictionaries for each unique combination Parameters Name Type Description Default field Union [ List[str ] , str ] the field(s) to get distinct values for required criteria Optional [ Dict ] PyMongo filter for documents to search in None all_exist bool ensure all fields exist for the distinct set False ensure_index ( self , key , unique = False ) \u00b6 Show source code in maggma/core/store.py 132 133 134 135 136 137 138 139 140 141 142 143 @abstractmethod def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" Tries to create an index and return true if it suceeded Parameters Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/core/store.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 @abstractmethod def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (dict, list of docs) newer_in ( self , target , criteria = None , exhaustive = False ) \u00b6 Show source code in maggma/core/store.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 def newer_in ( self , target : Store , criteria : Optional [ Dict ] = None , exhaustive : bool = False ) -> List [ str ]: \"\"\" Returns the keys of documents that are newer in the target Store than this Store. Args: target: target Store to criteria: PyMongo filter for documents to search in exhaustive: triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in \"\"\" self . ensure_index ( self . key ) self . ensure_index ( self . last_updated_field ) if exhaustive : # Get our current last_updated dates for each key value props = { self . key : 1 , self . last_updated_field : 1 , \"_id\" : 0 } dates = { d [ self . key ]: self . _lu_func [ 0 ]( d [ self . last_updated_field ]) for d in self . query ( properties = props ) } # Get the last_updated for the store we're comparing with props = { target . key : 1 , target . last_updated_field : 1 , \"_id\" : 0 } target_dates = { d [ target . key ]: target . _lu_func [ 0 ]( d [ target . last_updated_field ]) for d in target . query ( criteria = criteria , properties = props ) } new_keys = set ( target_dates . keys ()) - set ( dates . keys ()) updated_keys = { key for key , date in dates . items () if target_dates . get ( key , datetime . min ) > date } return list ( new_keys | updated_keys ) else : criteria = { self . last_updated_field : { \"$gt\" : self . _lu_func [ 1 ]( self . last_updated )} } return target . distinct ( field = self . key , criteria = criteria ) Returns the keys of documents that are newer in the target Store than this Store. Parameters Name Type Description Default target Store target Store to required criteria Optional [ Dict ] PyMongo filter for documents to search in None exhaustive bool triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in False query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/core/store.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 @abstractmethod def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 query_one ( self , criteria = None , properties = None , sort = None ) \u00b6 Show source code in maggma/core/store.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def query_one ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , ): \"\"\" Queries the Store for a single document Args: criteria: PyMongo filter for documents to search properties: properties to return in the document sort: Dictionary of sort order for fields \"\"\" return next ( self . query ( criteria = criteria , properties = properties , sort = sort ), None ) Queries the Store for a single document Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search None properties Union[Dict, List, None] properties to return in the document None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None remove_docs ( self , criteria ) \u00b6 Show source code in maggma/core/store.py 171 172 173 174 175 176 177 178 @abstractmethod def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required update ( self , docs , key = None ) \u00b6 Show source code in maggma/core/store.py 119 120 121 122 123 124 125 126 127 128 129 130 @abstractmethod def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" Update documents into the Store Parameters Name Type Description Default docs Union [ List[Dict ] , Dict ] the document or list of documents to update required key Union[List, str, None] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None StoreError \u00b6 General Store-related error","title":"Store"},{"location":"reference/core_store/#maggma.core.store.DateTimeFormat","text":"Datetime format in store document","title":"DateTimeFormat"},{"location":"reference/core_store/#maggma.core.store.Sort","text":"Enumeration for sorting order","title":"Sort"},{"location":"reference/core_store/#maggma.core.store.Store","text":"Abstract class for a data Store Defines the interface for all data going in and out of a Builder","title":"Store"},{"location":"reference/core_store/#maggma.core.store.Store.last_updated","text":"Provides the most recent last_updated date time stamp from the documents in this Store","title":"last_updated"},{"location":"reference/core_store/#maggma.core.store.Store.name","text":"Return a string representing this data source","title":"name"},{"location":"reference/core_store/#maggma.core.store.Store.__init__","text":"Show source code in maggma/core/store.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , key : str = \"task_id\" , last_updated_field : str = \"last_updated\" , last_updated_type : DateTimeFormat = DateTimeFormat ( \"datetime\" ), validator : Optional [ Validator ] = None , ): \"\"\" Args: key: master key to index on last_updated_field: field for date/time stamping the data last_updated_type: the date/time format for the last_updated_field. Can be \"datetime\" or \"isoformat\" validator: Validator to validate documents going into the store \"\"\" self . key = key self . last_updated_field = last_updated_field self . last_updated_type = last_updated_type self . _lu_func = ( LU_KEY_ISOFORMAT if last_updated_type == DateTimeFormat . IsoFormat else ( identity , identity ) ) # type: Tuple[Callable, Callable] self . validator = validator self . logger = logging . getLogger ( type ( self ) . __name__ ) self . logger . addHandler ( logging . NullHandler ()) Parameters Name Type Description Default key str master key to index on task_id last_updated_field str field for date/time stamping the data last_updated last_updated_type DateTimeFormat the date/time format for the last_updated_field. Can be \"datetime\" or \"isoformat\" DateTimeFormat.DateTime validator Optional [ Validator ] Validator to validate documents going into the store None","title":"__init__()"},{"location":"reference/core_store/#maggma.core.store.Store.close","text":"Show source code in maggma/core/store.py 93 94 95 96 97 @abstractmethod def close ( self ): \"\"\" Closes any connections \"\"\" Closes any connections","title":"close()"},{"location":"reference/core_store/#maggma.core.store.Store.connect","text":"Show source code in maggma/core/store.py 84 85 86 87 88 89 90 91 @abstractmethod def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data Args: force_reset: whether to reset the connection or not \"\"\" Connect to the source data Parameters Name Type Description Default force_reset bool whether to reset the connection or not False","title":"connect()"},{"location":"reference/core_store/#maggma.core.store.Store.distinct","text":"Show source code in maggma/core/store.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def distinct ( self , field : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , all_exist : bool = False , ) -> List : \"\"\" Get all distinct values for a field(s) For a single field, this returns a list of values For multiple fields, this return a list of of dictionaries for each unique combination Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in all_exist: ensure all fields exist for the distinct set \"\"\" field = field if isinstance ( field , list ) else [ field ] criteria = criteria or {} if all_exist : criteria . update ({ f : { \"$exists\" : 1 } for f in field if f not in criteria }) results = [ key for key , _ in self . groupby ( field , properties = field , criteria = criteria ) ] # Flatten out results if searching for a single field if len ( field ) == 1 : results = [ get ( r , field [ 0 ]) for r in results ] return results Get all distinct values for a field(s) For a single field, this returns a list of values For multiple fields, this return a list of of dictionaries for each unique combination Parameters Name Type Description Default field Union [ List[str ] , str ] the field(s) to get distinct values for required criteria Optional [ Dict ] PyMongo filter for documents to search in None all_exist bool ensure all fields exist for the distinct set False","title":"distinct()"},{"location":"reference/core_store/#maggma.core.store.Store.ensure_index","text":"Show source code in maggma/core/store.py 132 133 134 135 136 137 138 139 140 141 142 143 @abstractmethod def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" Tries to create an index and return true if it suceeded Parameters Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created","title":"ensure_index()"},{"location":"reference/core_store/#maggma.core.store.Store.groupby","text":"Show source code in maggma/core/store.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 @abstractmethod def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (dict, list of docs)","title":"groupby()"},{"location":"reference/core_store/#maggma.core.store.Store.newer_in","text":"Show source code in maggma/core/store.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 def newer_in ( self , target : Store , criteria : Optional [ Dict ] = None , exhaustive : bool = False ) -> List [ str ]: \"\"\" Returns the keys of documents that are newer in the target Store than this Store. Args: target: target Store to criteria: PyMongo filter for documents to search in exhaustive: triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in \"\"\" self . ensure_index ( self . key ) self . ensure_index ( self . last_updated_field ) if exhaustive : # Get our current last_updated dates for each key value props = { self . key : 1 , self . last_updated_field : 1 , \"_id\" : 0 } dates = { d [ self . key ]: self . _lu_func [ 0 ]( d [ self . last_updated_field ]) for d in self . query ( properties = props ) } # Get the last_updated for the store we're comparing with props = { target . key : 1 , target . last_updated_field : 1 , \"_id\" : 0 } target_dates = { d [ target . key ]: target . _lu_func [ 0 ]( d [ target . last_updated_field ]) for d in target . query ( criteria = criteria , properties = props ) } new_keys = set ( target_dates . keys ()) - set ( dates . keys ()) updated_keys = { key for key , date in dates . items () if target_dates . get ( key , datetime . min ) > date } return list ( new_keys | updated_keys ) else : criteria = { self . last_updated_field : { \"$gt\" : self . _lu_func [ 1 ]( self . last_updated )} } return target . distinct ( field = self . key , criteria = criteria ) Returns the keys of documents that are newer in the target Store than this Store. Parameters Name Type Description Default target Store target Store to required criteria Optional [ Dict ] PyMongo filter for documents to search in None exhaustive bool triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in False","title":"newer_in()"},{"location":"reference/core_store/#maggma.core.store.Store.query","text":"Show source code in maggma/core/store.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 @abstractmethod def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0","title":"query()"},{"location":"reference/core_store/#maggma.core.store.Store.query_one","text":"Show source code in maggma/core/store.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def query_one ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , ): \"\"\" Queries the Store for a single document Args: criteria: PyMongo filter for documents to search properties: properties to return in the document sort: Dictionary of sort order for fields \"\"\" return next ( self . query ( criteria = criteria , properties = properties , sort = sort ), None ) Queries the Store for a single document Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search None properties Union[Dict, List, None] properties to return in the document None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None","title":"query_one()"},{"location":"reference/core_store/#maggma.core.store.Store.remove_docs","text":"Show source code in maggma/core/store.py 171 172 173 174 175 176 177 178 @abstractmethod def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required","title":"remove_docs()"},{"location":"reference/core_store/#maggma.core.store.Store.update","text":"Show source code in maggma/core/store.py 119 120 121 122 123 124 125 126 127 128 129 130 @abstractmethod def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" Update documents into the Store Parameters Name Type Description Default docs Union [ List[Dict ] , Dict ] the document or list of documents to update required key Union[List, str, None] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None","title":"update()"},{"location":"reference/core_store/#maggma.core.store.StoreError","text":"General Store-related error","title":"StoreError"},{"location":"reference/core_validator/","text":"Validator class for document-level validation on Stores. Attach an instance of a Validator subclass to a Store .schema variable to enable validation on that Store. Validator \u00b6 A generic class to perform document-level validation on Stores. Attach a Validator to a Store during initialization, any all documents added to the Store will call .validate_doc() before being added. is_valid ( self , doc ) \u00b6 Show source code in maggma/core/validator.py 20 21 22 23 24 25 26 27 @abstractmethod def is_valid ( self , doc : Dict ) -> bool : \"\"\" Determines if the document is valid Args: doc: document to check \"\"\" Determines if the document is valid Parameters Name Type Description Default doc Dict document to check required validation_errors ( self , doc ) \u00b6 Show source code in maggma/core/validator.py 29 30 31 32 33 34 35 36 37 38 39 @abstractmethod def validation_errors ( self , doc : Dict ) -> List [ str ]: \"\"\" If document is not valid, provides a list of strings to display for why validation has failed Returns empty list if the document is valid Args: doc: document to check \"\"\" If document is not valid, provides a list of strings to display for why validation has failed Returns empty list if the document is valid Parameters Name Type Description Default doc Dict document to check required","title":"Validator"},{"location":"reference/core_validator/#maggma.core.validator.Validator","text":"A generic class to perform document-level validation on Stores. Attach a Validator to a Store during initialization, any all documents added to the Store will call .validate_doc() before being added.","title":"Validator"},{"location":"reference/core_validator/#maggma.core.validator.Validator.is_valid","text":"Show source code in maggma/core/validator.py 20 21 22 23 24 25 26 27 @abstractmethod def is_valid ( self , doc : Dict ) -> bool : \"\"\" Determines if the document is valid Args: doc: document to check \"\"\" Determines if the document is valid Parameters Name Type Description Default doc Dict document to check required","title":"is_valid()"},{"location":"reference/core_validator/#maggma.core.validator.Validator.validation_errors","text":"Show source code in maggma/core/validator.py 29 30 31 32 33 34 35 36 37 38 39 @abstractmethod def validation_errors ( self , doc : Dict ) -> List [ str ]: \"\"\" If document is not valid, provides a list of strings to display for why validation has failed Returns empty list if the document is valid Args: doc: document to check \"\"\" If document is not valid, provides a list of strings to display for why validation has failed Returns empty list if the document is valid Parameters Name Type Description Default doc Dict document to check required","title":"validation_errors()"},{"location":"reference/stores/","text":"Module containing various definitions of Stores. Stores are a default access pattern to data and provide various utillities JSONStore \u00b6 A Store for access to a single or multiple JSON files __eq__ ( self , other ) \u00b6 Show source code in maggma/stores/mongolike.py 409 410 411 412 413 414 415 416 417 418 419 420 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for JSONStore Args: other: other JSONStore to compare with \"\"\" if not isinstance ( other , JSONStore ): return False fields = [ \"paths\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for JSONStore Parameters Name Type Description Default other object other JSONStore to compare with required __init__ ( self , paths , ** kwargs ) \u00b6 Show source code in maggma/stores/mongolike.py 383 384 385 386 387 388 389 390 391 def __init__ ( self , paths : Union [ str , List [ str ]], ** kwargs ): \"\"\" Args: paths: paths for json files to turn into a Store \"\"\" paths = paths if isinstance ( paths , ( list , tuple )) else [ paths ] self . paths = paths self . kwargs = kwargs super () . __init__ ( collection_name = \"collection\" , ** kwargs ) Parameters Name Type Description Default paths Union [ str, List[str ] ] paths for json files to turn into a Store required connect ( self , force_reset = False ) \u00b6 Show source code in maggma/stores/mongolike.py 393 394 395 396 397 398 399 400 401 402 403 404 def connect ( self , force_reset = False ): \"\"\" Loads the files into the collection in memory \"\"\" super () . connect ( force_reset = force_reset ) for path in self . paths : with zopen ( path ) as f : data = f . read () data = data . decode () if isinstance ( data , bytes ) else data objects = json . loads ( data ) objects = [ objects ] if not isinstance ( objects , list ) else objects self . update ( objects ) Loads the files into the collection in memory MemoryStore \u00b6 An in-memory Store that functions similarly to a MongoStore name : _empty (property, readonly) \u00b6 Name for the store __eq__ ( self , other ) \u00b6 Show source code in maggma/stores/mongolike.py 366 367 368 369 370 371 372 373 374 375 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MemoryStore other: other MemoryStore to compare with \"\"\" if not isinstance ( other , MemoryStore ): return False fields = [ \"collection_name\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for MemoryStore other: other MemoryStore to compare with __hash__ ( self ) \u00b6 Show source code in maggma/stores/mongolike.py 322 323 324 def __hash__ ( self ): \"\"\" Hash for the store \"\"\" return hash (( self . name , self . last_updated_field )) Hash for the store __init__ ( self , collection_name = 'memory_db' , ** kwargs ) \u00b6 Show source code in maggma/stores/mongolike.py 299 300 301 302 303 304 305 306 307 308 def __init__ ( self , collection_name : str = \"memory_db\" , ** kwargs ): \"\"\" Initializes the Memory Store Args: collection_name: name for the collection in memory \"\"\" self . collection_name = collection_name self . _collection = None self . kwargs = kwargs super ( MongoStore , self ) . __init__ ( ** kwargs ) # noqa Initializes the Memory Store Parameters Name Type Description Default collection_name str name for the collection in memory memory_db connect ( self , force_reset = False ) \u00b6 Show source code in maggma/stores/mongolike.py 310 311 312 313 314 315 def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if not self . _collection or force_reset : self . _collection = mongomock . MongoClient () . db [ self . name ] Connect to the source data groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/stores/mongolike.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (key, list of elemnts) \"\"\" keys = keys if isinstance ( keys , list ) else [ keys ] data = [ doc for doc in self . query ( properties = keys , criteria = criteria ) if all ( has ( doc , k ) for k in keys ) ] def grouper ( doc ): return tuple ( get ( doc , k ) for k in keys ) for vals , group in groupby ( sorted ( data , key = grouper ), grouper ): doc = {} # type: Dict[Any,Any] for k , v in zip ( keys , vals ): set_ ( doc , k , v ) yield doc , list ( group ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (key, list of elemnts) MongoStore \u00b6 A Store that connects to a Mongo collection name : str (property, readonly) \u00b6 Return a string representing this data source __eq__ ( self , other ) \u00b6 Show source code in maggma/stores/mongolike.py 281 282 283 284 285 286 287 288 289 290 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MongoStore other: other mongostore to compare with \"\"\" if not isinstance ( other , MongoStore ): return False fields = [ \"database\" , \"collection_name\" , \"host\" , \"port\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for MongoStore other: other mongostore to compare with __hash__ ( self ) \u00b6 Show source code in maggma/stores/mongolike.py 82 83 84 def __hash__ ( self ) -> int : \"\"\" Hash for MongoStore \"\"\" return hash (( self . database , self . collection_name , self . last_updated_field )) Hash for MongoStore __init__ ( self , database , collection_name , host = 'localhost' , port = 27017 , username = '' , password = '' , ** kwargs ) \u00b6 Show source code in maggma/stores/mongolike.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __init__ ( self , database : str , collection_name : str , host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , ** kwargs , ): \"\"\" Args: database: The database name collection_name: The collection name host: Hostname for the database port: TCP port to connect to username: Username for the collection password: Password to connect with \"\"\" self . database = database self . collection_name = collection_name self . host = host self . port = port self . username = username self . password = password self . _collection = None # type: Any self . kwargs = kwargs super () . __init__ ( ** kwargs ) Parameters Name Type Description Default database str The database name required collection_name str The collection name required host str Hostname for the database localhost port int TCP port to connect to 27017 username str Username for the collection required password str Password to connect with required close ( self ) \u00b6 Show source code in maggma/stores/mongolike.py 277 278 279 def close ( self ): \"\"\" Close up all collections \"\"\" self . _collection . database . client . close () Close up all collections connect ( self , force_reset = False ) \u00b6 Show source code in maggma/stores/mongolike.py 71 72 73 74 75 76 77 78 79 80 def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if not self . _collection or force_reset : conn = MongoClient ( self . host , self . port ) db = conn [ self . database ] if self . username != \"\" : db . authenticate ( self . username , self . password ) self . _collection = db [ self . collection_name ] Connect to the source data ensure_index ( self , key , unique = False ) \u00b6 Show source code in maggma/stores/mongolike.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 def ensure_index ( self , key : str , unique : Optional [ bool ] = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" if confirm_field_index ( self . _collection , key ): return True else : try : self . _collection . create_index ( key , unique = unique , background = True ) return True except Exception : return False Tries to create an index and return true if it suceeded Parameters Name Type Description Default key str single key to index required unique Optional [ bool ] Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created from_collection ( collection ) (classmethod) \u00b6 Show source code in maggma/stores/mongolike.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 @classmethod def from_collection ( cls , collection ): \"\"\" Generates a MongoStore from a pymongo collection object This is not a fully safe operation as it gives dummy information to the MongoStore As a result, this will not serialize and can not reset its connection Args: collection: the PyMongo collection to create a MongoStore around \"\"\" # TODO: How do we make this safer? coll_name = collection . name db_name = collection . database . name store = cls ( db_name , coll_name ) store . _collection = collection return store Generates a MongoStore from a pymongo collection object This is not a fully safe operation as it gives dummy information to the MongoStore As a result, this will not serialize and can not reset its connection Parameters Name Type Description Default collection _empty the PyMongo collection to create a MongoStore around required from_db_file ( filename ) (classmethod) \u00b6 Show source code in maggma/stores/mongolike.py 86 87 88 89 90 91 92 93 94 95 96 97 @classmethod def from_db_file ( cls , filename : str ): \"\"\" Convenience method to construct MongoStore from db_file from old QueryEngine format \"\"\" kwargs = loadfn ( filename ) if \"collection\" in kwargs : kwargs [ \"collection_name\" ] = kwargs . pop ( \"collection\" ) # Get rid of aliases from traditional query engine db docs kwargs . pop ( \"aliases\" , None ) return cls ( ** kwargs ) Convenience method to construct MongoStore from db_file from old QueryEngine format groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/stores/mongolike.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (key, list of docs) \"\"\" pipeline = [] if isinstance ( keys , str ): keys = [ keys ] if properties is None : properties = [] if isinstance ( properties , dict ): properties = list ( properties . keys ()) if criteria is not None : pipeline . append ({ \"$match\" : criteria }) if len ( properties ) > 0 : pipeline . append ({ \"$project\" : { p : 1 for p in properties + keys }}) alpha = \"abcdefghijklmnopqrstuvwxyz\" group_id = { letter : f \"$ {key} \" for letter , key in zip ( alpha , keys )} pipeline . append ({ \"$group\" : { \"_id\" : group_id , \"docs\" : { \"$push\" : \"$$ROOT\" }}}) for d in self . _collection . aggregate ( pipeline , allowDiskUse = True ): id_doc = {} # type: Dict[str,Any] for letter , key in group_id . items (): if has ( d [ \"_id\" ], letter ): set_ ( id_doc , key [ 1 :], d [ \"_id\" ][ letter ]) yield ( id_doc , d [ \"docs\" ]) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (key, list of docs) query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/stores/mongolike.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( properties , list ): properties = { p : 1 for p in properties } sort_list = [( k , v . value ) for k , v in sort . items ()] if sort else None for d in self . _collection . find ( filter = criteria , projection = properties , skip = skip , limit = limit , sort = sort_list , ): yield d Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 remove_docs ( self , criteria ) \u00b6 Show source code in maggma/stores/mongolike.py 268 269 270 271 272 273 274 275 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" self . _collection . delete_many ( filter = criteria ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required update ( self , docs , key = None ) \u00b6 Show source code in maggma/stores/mongolike.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" requests = [] if not isinstance ( docs , list ): docs = [ docs ] for d in docs : d = jsanitize ( d , allow_bson = True ) # document-level validation is optional validates = True if self . validator : validates = self . validator . is_valid ( d ) if not validates : if self . validator . strict : raise ValueError ( self . validator . validation_errors ( d )) else : self . logger . error ( self . validator . validation_errors ( d )) if validates : key = key or self . key if isinstance ( key , list ): search_doc = { k : d [ k ] for k in key } else : search_doc = { key : d [ key ]} requests . append ( ReplaceOne ( search_doc , d , upsert = True )) if len ( requests ) > 0 : self . _collection . bulk_write ( requests , ordered = False ) Update documents into the Store Parameters Name Type Description Default docs Union [ List[Dict ] , Dict ] the document or list of documents to update required key Union[List, str, None] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Module containing various definitions of Stores. Stores are a default access pattern to data and provide various utillities GridFSStore \u00b6 A Store for GrdiFS backend. Provides a common access method consistent with other stores last_updated : datetime (property, readonly) \u00b6 Provides the most recent last_updated date time stamp from the documents in this Store __eq__ ( self , other ) \u00b6 Show source code in maggma/stores/gridfs.py 346 347 348 349 350 351 352 353 354 355 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for GridFSStore other: other GridFSStore to compare with \"\"\" if not isinstance ( other , GridFSStore ): return False fields = [ \"database\" , \"collection_name\" , \"host\" , \"port\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for GridFSStore other: other GridFSStore to compare with __init__ ( self , database , collection_name , host = 'localhost' , port = 27017 , username = '' , password = '' , compression = False , ** kwargs ) \u00b6 Show source code in maggma/stores/gridfs.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , database : str , collection_name : str , host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , compression : bool = False , ** kwargs , ): \"\"\" Initializes a GrdiFS Store for binary data Args: database: database name collection_name: The name of the collection. This is the string portion before the GridFS extensions host: hostname for the database port: port to connec to username: username to connect as password: password to authenticate as \"\"\" self . database = database self . collection_name = collection_name self . host = host self . port = port self . username = username self . password = password self . _collection = None # type: Any self . compression = compression self . kwargs = kwargs self . meta_keys = set () # type: Set[str] if \"key\" not in kwargs : kwargs [ \"key\" ] = \"_id\" super () . __init__ ( ** kwargs ) Initializes a GrdiFS Store for binary data Parameters Name Type Description Default database str database name required collection_name str The name of the collection. This is the string portion before the GridFS extensions required host str hostname for the database localhost port int port to connec to 27017 username str username to connect as required password str password to authenticate as required close ( self ) \u00b6 Show source code in maggma/stores/gridfs.py 343 344 def close ( self ): self . _collection . database . client . close () Closes any connections connect ( self , force_reset = False ) \u00b6 Show source code in maggma/stores/gridfs.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" conn = MongoClient ( self . host , self . port ) if not self . _collection or force_reset : db = conn [ self . database ] if self . username != \"\" : db . authenticate ( self . username , self . password ) self . _collection = gridfs . GridFS ( db , self . collection_name ) self . _files_collection = db [ \" {} .files\" . format ( self . collection_name )] self . _files_store = MongoStore . from_collection ( self . _files_collection ) self . _files_store . last_updated_field = f \"metadata. {self.last_updated_field} \" self . _chunks_collection = db [ \" {} .chunks\" . format ( self . collection_name )] Connect to the source data distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Show source code in maggma/stores/gridfs.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def distinct ( self , field : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , all_exist : bool = False , ) -> Union [ List [ Dict ], List ]: \"\"\" Function get to get all distinct values of a certain key in a GridFs store. Args: field: key or keys for which to find distinct values or sets of values. criteria: criteria for filter all_exist: whether to ensure all keys in list exist in each document, defaults to False \"\"\" criteria = ( self . transform_criteria ( criteria ) if isinstance ( criteria , dict ) else criteria ) field = [ field ] if not isinstance ( field , list ) else field field = [ f \"metadata. {k} \" if k not in self . files_collection_fields and not k . startswith ( \"metadata.\" ) else k for k in field ] return self . _files_store . distinct ( field = field , criteria = criteria , all_exist = all_exist ) Function get to get all distinct values of a certain key in a GridFs store. Parameters Name Type Description Default field Union [ List[str ] , str ] key or keys for which to find distinct values or sets of values. required criteria Optional [ Dict ] criteria for filter None all_exist bool whether to ensure all keys in list exist in each document, defaults to False False ensure_index ( self , key , unique = False ) \u00b6 Show source code in maggma/stores/gridfs.py 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 def ensure_index ( self , key : str , unique : Optional [ bool ] = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Currently operators on the GridFS files collection Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" # Transform key for gridfs first if key not in self . files_collection_fields : key = \"metadata. {} \" . format ( key ) if confirm_field_index ( self . collection , key ): return True else : try : self . _collection . create_index ( key , unique = unique , background = True ) return True except Exception : return False Tries to create an index and return true if it suceeded Currently operators on the GridFS files collection Parameters Name Type Description Default key str single key to index required unique Optional [ bool ] Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/stores/gridfs.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Will only work if the keys are included in the files collection for GridFS Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" criteria = ( self . transform_criteria ( criteria ) if isinstance ( criteria , dict ) else criteria ) keys = [ keys ] if not isinstance ( keys , list ) else keys keys = [ f \"metadata. {k} \" if k not in self . files_collection_fields and not k . startswith ( \"metadata.\" ) else k for k in keys ] for group , ids in self . _files_store . groupby ( keys , criteria = criteria , properties = [ f \"metadata. {self.key} \" ] ): ids = [ get ( doc , f \"metadata. {self.key} \" ) for doc in ids if has ( doc , f \"metadata. {self.key} \" ) ] group = { k . replace ( \"metadata.\" , \"\" ): get ( group , k ) for k in keys if has ( group , k ) } yield group , list ( self . query ( criteria = { self . key : { \"$in\" : ids }})) Simple grouping function that will group documents by keys. Will only work if the keys are included in the files collection for GridFS Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (dict, list of docs) name ( self ) \u00b6 Show source code in maggma/stores/gridfs.py 84 85 86 87 88 def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" return self . collection_name Return a string representing this data source query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/stores/gridfs.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the GridFS Store for a set of documents Currently ignores properties TODO: If properties wholy in metadata, just query that Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) for f in self . _collection . find ( filter = criteria , skip = skip , limit = limit , sort = sort ): data = f . read () metadata = f . metadata if metadata . get ( \"compression\" , \"\" ) == \"zlib\" : data = zlib . decompress ( data ) . decode ( \"UTF-8\" ) try : data = json . loads ( data ) except Exception : pass yield data Queries the GridFS Store for a set of documents Currently ignores properties TODO: If properties wholy in metadata, just query that Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 remove_docs ( self , criteria ) \u00b6 Show source code in maggma/stores/gridfs.py 329 330 331 332 333 334 335 336 337 338 339 340 341 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) ids = [ cursor . _id for cursor in self . _collection . find ( criteria )] for id in ids : self . _collection . delete ( id ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required transform_criteria ( criteria ) (classmethod) \u00b6 Show source code in maggma/stores/gridfs.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 @classmethod def transform_criteria ( cls , criteria : Dict ) -> Dict : \"\"\" Allow client to not need to prepend 'metadata.' to query fields. Args: criteria: Query criteria \"\"\" new_criteria = dict () for field in criteria : if field not in cls . files_collection_fields and not field . startswith ( \"metadata.\" ): new_criteria [ \"metadata.\" + field ] = copy . copy ( criteria [ field ]) else : new_criteria [ field ] = copy . copy ( criteria [ field ]) return new_criteria Allow client to not need to prepend 'metadata.' to query fields. Parameters Name Type Description Default criteria Dict Query criteria required update ( self , docs , key = None ) \u00b6 Show source code in maggma/stores/gridfs.py 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" if not isinstance ( docs , list ): docs = [ docs ] if isinstance ( key , str ): key = [ key ] elif not key : key = [ self . key ] key = list ( set ( key ) | self . meta_keys - set ( self . files_collection_fields )) for d in docs : search_doc = { k : d [ k ] for k in key } metadata = { k : d [ k ] for k in [ self . last_updated_field ] if k in d } metadata . update ( search_doc ) data = json . dumps ( jsanitize ( d )) . encode ( \"UTF-8\" ) if self . compression : data = zlib . compress ( data ) metadata [ \"compression\" ] = \"zlib\" self . _collection . put ( data , metadata = metadata ) search_doc = self . transform_criteria ( search_doc ) # Cleans up old gridfs entries for fdoc in ( self . _files_collection . find ( search_doc , [ \"_id\" ]) . sort ( \"uploadDate\" , - 1 ) . skip ( 1 ) ): self . _collection . delete ( fdoc [ \"_id\" ]) Update documents into the Store Parameters Name Type Description Default docs Union [ List[Dict ] , Dict ] the document or list of documents to update required key Union[List, str, None] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Advanced Stores for connecting to AWS data AmazonS3Store \u00b6 GridFS like storage using Amazon S3 and a regular store for indexing Assumes Amazon AWS key and secret key are set in environment or default config file last_updated : _empty (property, readonly) \u00b6 Provides the most recent last_updated date time stamp from the documents in this Store __eq__ ( self , other ) \u00b6 Show source code in maggma/stores/aws.py 295 296 297 298 299 300 301 302 303 304 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for AmazonS3Store other: other AmazonS3Store to compare with \"\"\" if not isinstance ( other , AmazonS3Store ): return False fields = [ \"index\" , \"bucket\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for AmazonS3Store other: other AmazonS3Store to compare with __init__ ( self , index , bucket , compress = False , ** kwargs ) \u00b6 Show source code in maggma/stores/aws.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , index : Store , bucket : str , compress : bool = False , ** kwargs ): \"\"\" Initializes an S3 Store Args: index: a store to use to index the S3 Bucket bucket: name of the bucket compress: compress files inserted into the store \"\"\" if boto3 is None : raise RuntimeError ( \"boto3 and botocore are required for AmazonS3Store\" ) self . index = index self . bucket = bucket self . compress = compress self . s3 = None # type: Any self . s3_bucket = None # type: Any # Force the key to be the same as the index kwargs [ \"key\" ] = index . key super ( AmazonS3Store , self ) . __init__ ( ** kwargs ) Initializes an S3 Store Parameters Name Type Description Default index Store a store to use to index the S3 Bucket required bucket str name of the bucket required compress bool compress files inserted into the store False close ( self ) \u00b6 Show source code in maggma/stores/aws.py 70 71 72 73 74 75 76 def close ( self ): \"\"\" Closes any connections \"\"\" self . index . close () self . s3 = None self . s3_bucket = None Closes any connections connect ( self , force_reset = False ) \u00b6 Show source code in maggma/stores/aws.py 57 58 59 60 61 62 63 64 65 66 67 68 def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" self . index . connect ( force_reset = force_reset ) if not self . s3 : self . s3 = boto3 . resource ( \"s3\" ) if self . bucket not in [ bucket . name for bucket in self . s3 . buckets . all ()]: raise Exception ( \"Bucket not present on AWS: {} \" . format ( self . bucket )) self . s3_bucket = self . s3 . Bucket ( self . bucket ) Connect to the source data distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Show source code in maggma/stores/aws.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def distinct ( self , field : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , all_exist : bool = False , ) -> Union [ List [ Dict ], List ]: \"\"\" Get all distinct values for a field(s) For a single field, this returns a list of values For multiple fields, this return a list of of dictionaries for each unique combination Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in all_exist: ensure all fields exist for the distinct set \"\"\" # Index is a store so it should have its own distinct function return self . index . distinct ( field , criteria = criteria , all_exist = all_exist ) Get all distinct values for a field(s) For a single field, this returns a list of values For multiple fields, this return a list of of dictionaries for each unique combination Parameters Name Type Description Default field Union [ List[str ] , str ] the field(s) to get distinct values for required criteria Optional [ Dict ] PyMongo filter for documents to search in None all_exist bool ensure all fields exist for the distinct set False ensure_index ( self , key , unique = False ) \u00b6 Show source code in maggma/stores/aws.py 183 184 185 186 187 188 189 190 191 192 193 194 def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" return self . index . ensure_index ( key , unique = unique ) Tries to create an index and return true if it suceeded Parameters Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/stores/aws.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" return self . index . groupby ( keys = keys , criteria = criteria , properties = properties , sort = sort , skip = skip , limit = limit , ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (dict, list of docs) name ( self ) \u00b6 Show source code in maggma/stores/aws.py 50 51 52 53 54 55 def name ( self ) -> str : \"\"\" Returns: a string representing this data source \"\"\" return self . bucket Returns Type Description str a string representing this data source newer_in ( self , target , criteria = None , exhaustive = False ) \u00b6 Show source code in maggma/stores/aws.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def newer_in ( self , target : Store , criteria : Optional [ Dict ] = None , exhaustive : bool = False ) -> List [ str ]: \"\"\" Returns the keys of documents that are newer in the target Store than this Store. Args: target: target Store criteria: PyMongo filter for documents to search in exhaustive: triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in \"\"\" return self . index . newer_in ( target = target , criteria = criteria , exhaustive = exhaustive ) Returns the keys of documents that are newer in the target Store than this Store. Parameters Name Type Description Default target Store target Store required criteria Optional [ Dict ] PyMongo filter for documents to search in None exhaustive bool triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in False query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/stores/aws.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" for doc in self . index . query ( criteria = criteria , sort = sort , limit = limit , skip = skip ): try : # TODO: THis is ugly and unsafe, do some real checking before pulling data data = self . s3_bucket . Object ( doc [ self . key ]) . get ()[ \"Body\" ] . read () except botocore . exceptions . ClientError as e : # If a client error is thrown, then check that it was a 404 error. # If it was a 404 error, then the object does not exist. error_code = int ( e . response [ \"Error\" ][ \"Code\" ]) if error_code == 404 : self . logger . error ( \"Could not find S3 object {} \" . format ( doc [ self . key ]) ) break else : raise e if doc . get ( \"compression\" , \"\" ) == \"zlib\" : data = zlib . decompress ( data ) yield json . loads ( data ) Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 rebuild_index_from_s3_data ( self ) \u00b6 Show source code in maggma/stores/aws.py 283 284 285 286 287 288 289 290 291 292 293 def rebuild_index_from_s3_data ( self ): \"\"\" Rebuilds the index Store from the data in S3 Relies on the index document being stores as the metadata for the file \"\"\" index_docs = [] for file in self . s3_bucket . objects . all (): # TODO: Transform the data back from strings and remove AWS S3 specific keys index_docs . append ( file . metadata ) self . index . update ( index_docs ) Rebuilds the index Store from the data in S3 Relies on the index document being stores as the metadata for the file remove_docs ( self , criteria , remove_s3_object = False ) \u00b6 Show source code in maggma/stores/aws.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 def remove_docs ( self , criteria : Dict , remove_s3_object : bool = False ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match remove_s3_object: whether to remove the actual S3 Object or not \"\"\" if not remove_s3_object : self . index . remove_docs ( criteria = criteria ) else : to_remove = self . index . distinct ( self . key , criteria = criteria ) self . index . remove_docs ( criteria = criteria ) # Can remove up to 1000 items at a time via boto to_remove_chunks = list ( grouper ( to_remove , n = 1000 )) for chunk_to_remove in to_remove_chunks : objlist = [{ \"Key\" : obj } for obj in chunk_to_remove ] self . s3_bucket . delete_objects ( Delete = { \"Objects\" : objlist }) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required remove_s3_object bool whether to remove the actual S3 Object or not False update ( self , docs , key = None ) \u00b6 Show source code in maggma/stores/aws.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" search_docs = [] search_keys = [] if isinstance ( key , list ): search_keys = key elif key : search_keys = [ key ] else : search_keys = [ self . key ] for d in docs : search_doc = { k : d [ k ] for k in search_keys } search_doc [ self . key ] = d [ self . key ] # Ensure key is in metadata # Remove MongoDB _id from search if \"_id\" in search_doc : del search_doc [ \"_id\" ] data = json . dumps ( jsanitize ( d )) . encode () # Compress with zlib if chosen if self . compress : search_doc [ \"compression\" ] = \"zlib\" data = zlib . compress ( data ) self . s3_bucket . put_object ( Key = d [ self . key ], Body = data , Metadata = search_doc ) search_docs . append ( search_doc ) # Use store's update to remove key clashes self . index . update ( search_docs ) Update documents into the Store Parameters Name Type Description Default docs Union [ List[Dict ] , Dict ] the document or list of documents to update required key Optional[Union[List, str]] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Advanced Stores for behavior outside normal access patterns AliasingStore \u00b6 Special Store that aliases for the primary accessors __eq__ ( self , other ) \u00b6 Show source code in maggma/stores/advanced_stores.py 360 361 362 363 364 365 366 367 368 369 370 371 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for AliasingStore Args: other: other AliasingStore to compare with \"\"\" if not isinstance ( other , AliasingStore ): return False fields = [ \"store\" , \"aliases\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for AliasingStore Parameters Name Type Description Default other object other AliasingStore to compare with required __init__ ( self , store , aliases , ** kwargs ) \u00b6 Show source code in maggma/stores/advanced_stores.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def __init__ ( self , store : Store , aliases : Dict , ** kwargs ): \"\"\" Args: store: the store to wrap around aliases: dict of aliases of the form external key: internal key \"\"\" self . store = store # Given an external key tells what the internal key is self . aliases = aliases # Given the internal key tells us what the external key is self . reverse_aliases = { v : k for k , v in aliases . items ()} self . kwargs = kwargs kwargs . update ( { \"last_updated_field\" : store . last_updated_field , \"last_updated_type\" : store . last_updated_type , } ) super ( AliasingStore , self ) . __init__ ( ** kwargs ) Parameters Name Type Description Default store Store the store to wrap around required aliases Dict dict of aliases of the form external key: internal key required close ( self ) \u00b6 Show source code in maggma/stores/advanced_stores.py 350 351 def close ( self ): self . store . close () Closes any connections connect ( self , force_reset = False ) \u00b6 Show source code in maggma/stores/advanced_stores.py 357 358 def connect ( self , force_reset = False ): self . store . connect ( force_reset = force_reset ) Connect to the source data Parameters Name Type Description Default force_reset _empty whether to reset the connection or not False distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Show source code in maggma/stores/advanced_stores.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 def distinct ( self , field : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , all_exist : bool = False , ) -> List : \"\"\" Get all distinct values for a key Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in all_exist: ensure all fields exist for the distinct set \"\"\" criteria = criteria if criteria else {} lazy_substitute ( criteria , self . reverse_aliases ) field = field if isinstance ( field , list ) else [ field ] # substitute forward field = [ self . aliases [ f ] for f in field ] return self . store . distinct ( field , criteria = criteria ) Get all distinct values for a key Parameters Name Type Description Default field Union [ List[str ] , str ] the field(s) to get distinct values for required criteria Optional [ Dict ] PyMongo filter for documents to search in None all_exist bool ensure all fields exist for the distinct set False ensure_index ( self , key , unique = False , ** kwargs ) \u00b6 Show source code in maggma/stores/advanced_stores.py 345 346 347 348 def ensure_index ( self , key , unique = False , ** kwargs ): if key in self . aliases : key = self . aliases return self . store . ensure_index ( key , unique , ** kwargs ) Tries to create an index and return true if it suceeded Parameters Name Type Description Default key _empty single key to index required unique _empty Whether or not this index contains only unique keys False Returns Type Description _empty bool indicating if the index exists/was created groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/stores/advanced_stores.py 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" # Convert to a list keys = keys if isinstance ( keys , list ) else [ keys ] # Make the aliasing transformations on keys keys = [ self . aliases [ k ] if k in self . aliases else k for k in keys ] # Update criteria and properties based on aliases criteria = criteria if criteria else {} substitute ( properties , self . reverse_aliases ) lazy_substitute ( criteria , self . reverse_aliases ) return self . store . groupby ( keys = keys , properties = properties , criteria = criteria , skip = skip , limit = limit ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (dict, list of docs) name ( self ) \u00b6 Show source code in maggma/stores/advanced_stores.py 216 217 218 219 220 def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" return self . store . name Return a string representing this data source query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/stores/advanced_stores.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( properties , list ): properties = { p : 1 for p in properties } criteria = criteria if criteria else {} substitute ( properties , self . reverse_aliases ) lazy_substitute ( criteria , self . reverse_aliases ) for d in self . store . query ( properties = properties , criteria = criteria , sort = sort , limit = limit , skip = skip ): substitute ( d , self . aliases ) yield d Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 remove_docs ( self , criteria ) \u00b6 Show source code in maggma/stores/advanced_stores.py 334 335 336 337 338 339 340 341 342 343 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" # Update criteria and properties based on aliases lazy_substitute ( criteria , self . reverse_aliases ) self . store . remove_docs ( criteria ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required update ( self , docs , key = None ) \u00b6 Show source code in maggma/stores/advanced_stores.py 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" key = key if key else self . key for d in docs : substitute ( d , self . reverse_aliases ) if key in self . aliases : key = self . aliases [ key ] self . store . update ( docs , key = key ) Update documents into the Store Parameters Name Type Description Default docs Union [ List[Dict ] , Dict ] the document or list of documents to update required key Optional[Union[List, str]] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None MongograntStore \u00b6 Initialize a Store with a mongogrant \" : / .\" spec. Some class methods of MongoStore , e . g . from_db_file and from_collection , are not supported . mongogrant documentation : https : // github . com / materialsproject / mongogrant __eq__ ( self , other ) \u00b6 Show source code in maggma/stores/advanced_stores.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MongograntStore Args: other: other MongograntStore to compare with \"\"\" if not isinstance ( other , MongograntStore ): return False fields = [ \"mongogrant_spec\" , \"collection_name\" , \"mgclient_config_path\" , \"last_updated_field\" , ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for MongograntStore Parameters Name Type Description Default other object other MongograntStore to compare with required __init__ ( self , mongogrant_spec , collection_name , mgclient_config_path = None , ** kwargs ) \u00b6 Show source code in maggma/stores/advanced_stores.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , mongogrant_spec : str , collection_name : str , mgclient_config_path : Optional [ str ] = None , ** kwargs ): \"\"\" Args: mongogrant_spec: of the form <role>:<host>/<db>, where role is one of {\"read\", \"readWrite\"} or aliases {\"ro\", \"rw\"}; host is a db host (w/ optional port) or alias; and db is a db on that host, or alias. See mongogrant documentation. collection_name: name of mongo collection mgclient_config_path: Path to mongogrant client config file, or None if default path (`mongogrant.client.path`). \"\"\" self . mongogrant_spec = mongogrant_spec self . collection_name = collection_name self . mgclient_config_path = mgclient_config_path self . _collection = None if set (( \"username\" , \"password\" , \"database\" , \"host\" )) & set ( kwargs ): raise StoreError ( \"MongograntStore does not accept \" \"username, password, database, or host \" \"arguments. Use `mongogrant_spec`.\" ) self . kwargs = kwargs super ( MongoStore , self ) . __init__ ( ** kwargs ) # noqa Parameters Name Type Description Default mongogrant_spec str of the form : / , where role is one of required collection_name str name of mongo collection required mgclient_config_path Optional [ str ] Path to mongogrant client config file, or None if default path ( mongogrant.client.path ). None connect ( self , force_reset = False ) \u00b6 Show source code in maggma/stores/advanced_stores.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def connect ( self , force_reset : bool = False ): \"\"\" Connect to the mongogrant source Args: force_reset: forces the connection to reset rather than just ensuring the connection is present \"\"\" if not self . _collection or force_reset : if self . mgclient_config_path : config = Config ( check = check , path = self . mgclient_config_path ) client = Client ( config ) else : client = Client () db = client . db ( self . mongogrant_spec ) self . _collection = db [ self . collection_name ] Connect to the mongogrant source Parameters Name Type Description Default force_reset bool forces the connection to reset rather than just ensuring the connection is present False from_collection ( collection ) (classmethod) \u00b6 Show source code in maggma/stores/advanced_stores.py 91 92 93 94 95 96 @classmethod def from_collection ( cls , collection ): \"\"\" Raises ValueError since MongograntStores can't be initialized from a PyMongo collection \"\"\" raise ValueError ( \"MongograntStore doesn't implement from_collection\" ) Raises ValueError since MongograntStores can't be initialized from a PyMongo collection from_db_file ( file ) (classmethod) \u00b6 Show source code in maggma/stores/advanced_stores.py 84 85 86 87 88 89 @classmethod def from_db_file ( cls , file ): \"\"\" Raises ValueError since MongograntStores can't be initialized from a file \"\"\" raise ValueError ( \"MongograntStore doesn't implement from_db_file\" ) Raises ValueError since MongograntStores can't be initialized from a file SandboxStore \u00b6 Provides a sandboxed view to another store sbx_criteria : Dict (property, readonly) \u00b6 Returns Type Description Dict the sandbox criteria dict used to filter the source store __eq__ ( self , other ) \u00b6 Show source code in maggma/stores/advanced_stores.py 518 519 520 521 522 523 524 525 526 527 528 529 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for SandboxStore Args: other: other SandboxStore to compare with \"\"\" if not isinstance ( other , SandboxStore ): return False fields = [ \"store\" , \"sandbox\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for SandboxStore Parameters Name Type Description Default other object other SandboxStore to compare with required __init__ ( self , store , sandbox , exclusive = False ) \u00b6 Show source code in maggma/stores/advanced_stores.py 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 def __init__ ( self , store : Store , sandbox : str , exclusive : bool = False ): \"\"\" Args: store: store to wrap sandboxing around sandbox: the corresponding sandbox exclusive: whether to be exclusively in this sandbox or include global items \"\"\" self . store = store self . sandbox = sandbox self . exclusive = exclusive super () . __init__ ( key = self . store . key , last_updated_field = self . store . last_updated_field , last_updated_type = self . store . last_updated_type , validator = self . store . validator , ) Parameters Name Type Description Default store Store store to wrap sandboxing around required sandbox str the corresponding sandbox required exclusive bool whether to be exclusively in this sandbox or include global items False close ( self ) \u00b6 Show source code in maggma/stores/advanced_stores.py 508 509 def close ( self ): self . store . close () Closes any connections connect ( self , force_reset = False ) \u00b6 Show source code in maggma/stores/advanced_stores.py 515 516 def connect ( self , force_reset = False ): self . store . connect ( force_reset = force_reset ) Connect to the source data Parameters Name Type Description Default force_reset _empty whether to reset the connection or not False ensure_index ( self , key , unique = False , ** kwargs ) \u00b6 Show source code in maggma/stores/advanced_stores.py 505 506 def ensure_index ( self , key , unique = False , ** kwargs ): return self . store . ensure_index ( key , unique , ** kwargs ) Tries to create an index and return true if it suceeded Parameters Name Type Description Default key _empty single key to index required unique _empty Whether or not this index contains only unique keys False Returns Type Description _empty bool indicating if the index exists/was created groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/stores/advanced_stores.py 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . groupby ( keys = keys , properties = properties , criteria = criteria , skip = skip , limit = limit ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (dict, list of docs) name ( self ) \u00b6 Show source code in maggma/stores/advanced_stores.py 396 397 398 399 400 401 def name ( self ) -> str : \"\"\" Returns: a string representing this data source \"\"\" return self . store . name Returns Type Description str a string representing this data source query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/stores/advanced_stores.py 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . query ( properties = properties , criteria = criteria , sort = sort , limit = limit , skip = skip ) Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 remove_docs ( self , criteria ) \u00b6 Show source code in maggma/stores/advanced_stores.py 492 493 494 495 496 497 498 499 500 501 502 503 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" # Update criteria and properties based on aliases criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) self . store . remove_docs ( criteria ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required update ( self , docs , key = None ) \u00b6 Show source code in maggma/stores/advanced_stores.py 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" for d in docs : if \"sbxn\" in d : d [ \"sbxn\" ] = list ( set ( d [ \"sbxn\" ] + [ self . sandbox ])) else : d [ \"sbxn\" ] = [ self . sandbox ] self . store . update ( docs , key = key ) Update documents into the Store Parameters Name Type Description Default docs Union [ List[Dict ] , Dict ] the document or list of documents to update required key Optional[Union[List, str]] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None VaultStore \u00b6 Extends MongoStore to read credentials out of Vault server and uses these values to initialize MongoStore instance __eq__ ( self , other ) \u00b6 Show source code in maggma/stores/advanced_stores.py 176 177 178 179 180 181 182 183 184 185 186 187 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for VaultStore Args: other: other VaultStore to compare with \"\"\" if not isinstance ( other , VaultStore ): return False fields = [ \"vault_secret_path\" , \"collection_name\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for VaultStore Parameters Name Type Description Default other object other VaultStore to compare with required __init__ ( self , collection_name , vault_secret_path ) \u00b6 Show source code in maggma/stores/advanced_stores.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 @requires ( hvac is not None , \"hvac is required to use VaultStore\" ) def __init__ ( self , collection_name : str , vault_secret_path : str ): \"\"\" Args: collection_name: name of mongo collection vault_secret_path: path on vault server with mongo creds object Important: Environment variables that must be set prior to invocation VAULT_ADDR - URL of vault server (eg. https://matgen8.lbl.gov:8200) VAULT_TOKEN or GITHUB_TOKEN - token used to authenticate to vault \"\"\" self . collection_name = collection_name self . vault_secret_path = vault_secret_path # TODO: Switch this over to Pydantic ConfigSettings vault_addr = os . getenv ( \"VAULT_ADDR\" ) if not vault_addr : raise RuntimeError ( \"VAULT_ADDR not set\" ) client = hvac . Client ( vault_addr ) # If we have a vault token use this token = os . getenv ( \"VAULT_TOKEN\" ) # Look for a github token instead if not token : github_token = os . getenv ( \"GITHUB_TOKEN\" ) if github_token : client . auth_github ( github_token ) else : raise RuntimeError ( \"VAULT_TOKEN or GITHUB_TOKEN not set\" ) else : client . token = token if not client . is_authenticated (): raise RuntimeError ( \"Bad token\" ) # Read the vault secret json_db_creds = client . read ( vault_secret_path ) db_creds = json . loads ( json_db_creds [ \"data\" ][ \"value\" ]) database = db_creds . get ( \"db\" ) host = db_creds . get ( \"host\" , \"localhost\" ) port = db_creds . get ( \"port\" , 27017 ) username = db_creds . get ( \"username\" , \"\" ) password = db_creds . get ( \"password\" , \"\" ) super ( VaultStore , self ) . __init__ ( database , collection_name , host , port , username , password ) Parameters Name Type Description Default collection_name str name of mongo collection required vault_secret_path str path on vault server with mongo creds object required Important Environment variables that must be set prior to invocation VAULT_ADDR - URL of vault server (eg. https://matgen8.lbl.gov:8200) VAULT_TOKEN or GITHUB_TOKEN - token used to authenticate to vault Special stores that combine underlying Stores together ConcatStore \u00b6 Store concatting multiple stores last_updated : datetime (property, readonly) \u00b6 Finds the most recent last_updated across all the stores. This might not be the most usefull way to do this for this type of Store since it could very easily over-estimate the last_updated based on what stores are used __eq__ ( self , other ) \u00b6 Show source code in maggma/stores/compound_stores.py 498 499 500 501 502 503 504 505 506 507 508 509 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for ConcatStore Args: other: other JointStore to compare with \"\"\" if not isinstance ( other , ConcatStore ): return False fields = [ \"stores\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for ConcatStore Parameters Name Type Description Default other object other JointStore to compare with required __init__ ( self , * stores , ** kwargs ) \u00b6 Show source code in maggma/stores/compound_stores.py 307 308 309 310 311 312 313 314 315 316 def __init__ ( self , * stores : Store , ** kwargs ): \"\"\" Initialize a ConcatStore that concatenates multiple stores together to appear as one store Args: stores: list of stores to concatenate together \"\"\" self . stores = stores super ( ConcatStore , self ) . __init__ ( ** kwargs ) Initialize a ConcatStore that concatenates multiple stores together to appear as one store Parameters Name Type Description Default stores Store list of stores to concatenate together required close ( self ) \u00b6 Show source code in maggma/stores/compound_stores.py 334 335 336 337 338 339 def close ( self ): \"\"\" Close all connections in this ConcatStore \"\"\" for store in self . stores : store . close () Close all connections in this ConcatStore connect ( self , force_reset = False ) \u00b6 Show source code in maggma/stores/compound_stores.py 324 325 326 327 328 329 330 331 332 def connect ( self , force_reset : bool = False ): \"\"\" Connect all stores in this ConcatStore Args: force_reset: Whether to forcibly reset the connection for all stores \"\"\" for store in self . stores : store . connect ( force_reset ) Connect all stores in this ConcatStore Parameters Name Type Description Default force_reset bool Whether to forcibly reset the connection for all stores False distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Show source code in maggma/stores/compound_stores.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 def distinct ( self , field : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , all_exist : bool = False , ) -> Union [ List [ Dict ], List ]: \"\"\" Get all distinct values for a field(s) For a single field, this returns a list of values For multiple fields, this return a list of of dictionaries for each unique combination Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in all_exist: ensure all fields exist for the distinct set \"\"\" distincts = [] for store in self . stores : distincts . extend ( store . distinct ( field = field , criteria = criteria , all_exist = all_exist ) ) if isinstance ( field , str ): return list ( set ( distincts )) else : return [ dict ( s ) for s in set ( frozenset ( d . items ()) for d in distincts )] Get all distinct values for a field(s) For a single field, this returns a list of values For multiple fields, this return a list of of dictionaries for each unique combination Parameters Name Type Description Default field Union [ List[str ] , str ] the field(s) to get distinct values for required criteria Optional [ Dict ] PyMongo filter for documents to search in None all_exist bool ensure all fields exist for the distinct set False ensure_index ( self , key , unique = False ) \u00b6 Show source code in maggma/stores/compound_stores.py 401 402 403 404 405 406 407 408 409 410 411 412 def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Ensure an index is properly set. Returns whether all stores support this index or not Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created on all stores \"\"\" return all ([ store . ensure_index ( key , unique ) for store in self . stores ]) Ensure an index is properly set. Returns whether all stores support this index or not Parameters Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created on all stores groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/stores/compound_stores.py 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" if isinstance ( keys , str ): keys = [ keys ] docs = [] for store in self . stores : temp_docs = list ( store . groupby ( keys = keys , criteria = criteria , properties = properties , sort = sort , skip = skip , limit = limit , ) ) for key , group in temp_docs : docs . extend ( group ) def key_set ( d : Dict ) -> Tuple : \"index function based on passed in keys\" test_d = tuple ( d . get ( k , None ) for k in keys ) return test_d sorted_docs = sorted ( docs , key = key_set ) for vals , group_iter in groupby ( sorted_docs , key = key_set ): id_dict = { key : val for key , val in zip ( keys , vals )} yield id_dict , list ( group_iter ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (dict, list of docs) name ( self ) \u00b6 Show source code in maggma/stores/compound_stores.py 318 319 320 321 322 def name ( self ) -> str : \"\"\" A string representing this data source \"\"\" return self . stores [ 0 ] . name A string representing this data source query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/stores/compound_stores.py 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries across all Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" # TODO: skip, sort and limit are broken. implement properly for store in self . stores : for d in store . query ( criteria = criteria , properties = properties ): yield d Queries across all Store for a set of documents Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 remove_docs ( self , criteria ) \u00b6 Show source code in maggma/stores/compound_stores.py 489 490 491 492 493 494 495 496 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" raise NotImplementedError ( \"No remove_docs method for JointStore\" ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required update ( self , docs , key = None ) \u00b6 Show source code in maggma/stores/compound_stores.py 360 361 362 363 364 365 366 367 368 369 370 371 372 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Not implemented in ConcatStore Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" raise NotImplementedError ( \"No update method for ConcatStore\" ) Update documents into the Store Not implemented in ConcatStore Parameters Name Type Description Default docs Union [ List[Dict ] , Dict ] the document or list of documents to update required key Optional[Union[List, str]] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None JointStore \u00b6 Store that implements a on-the-fly join across multiple collections all in the same MongoDB database. This is a Read-Only Store designed to combine data from multiple collections. last_updated : datetime (property, readonly) \u00b6 Special last_updated for this JointStore that checks all underlying collections nonmaster_names : List (property, readonly) \u00b6 alll non-master collection names __eq__ ( self , other ) \u00b6 Show source code in maggma/stores/compound_stores.py 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for JointStore Args: other: other JointStore to compare with \"\"\" if not isinstance ( other , JointStore ): return False fields = [ \"database\" , \"collection_names\" , \"host\" , \"port\" , \"master\" , \"merge_at_root\" , ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for JointStore Parameters Name Type Description Default other object other JointStore to compare with required __init__ ( self , database , collection_names , host = 'localhost' , port = 27017 , username = '' , password = '' , master = None , merge_at_root = False , ** kwargs ) \u00b6 Show source code in maggma/stores/compound_stores.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , database : str , collection_names : List [ str ], host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , master : Optional [ str ] = None , merge_at_root : bool = False , ** kwargs ): \"\"\" Args: database: The database name collection_names: list of all collections to join host: Hostname for the database port: TCP port to connect to username: Username for the collection password: Password to connect with master: name for the master collection if not specified this defaults to the first in collection_names list \"\"\" self . database = database self . collection_names = collection_names self . host = host self . port = port self . username = username self . password = password self . _collection = None # type: Any self . master = master or collection_names [ 0 ] self . merge_at_root = merge_at_root self . kwargs = kwargs super ( JointStore , self ) . __init__ ( ** kwargs ) Parameters Name Type Description Default database str The database name required collection_names List [ str ] list of all collections to join required host str Hostname for the database localhost port int TCP port to connect to 27017 username str Username for the collection required password str Password to connect with required master Optional [ str ] name for the master collection if not specified this defaults to the first in collection_names list None close ( self ) \u00b6 Show source code in maggma/stores/compound_stores.py 78 79 80 81 82 def close ( self ): \"\"\" Closes underlying database connections \"\"\" self . _collection . database . client . close () Closes underlying database connections connect ( self , force_reset = False ) \u00b6 Show source code in maggma/stores/compound_stores.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def connect ( self , force_reset : bool = False ): \"\"\" Connects the underlying Mongo database and all collection connections Args: force_reset: whether to forcibly reset the connection \"\"\" conn = MongoClient ( self . host , self . port ) db = conn [ self . database ] if self . username != \"\" : db . authenticate ( self . username , self . password ) self . _collection = db [ self . master ] self . _has_merge_objects = ( self . _collection . database . client . server_info ()[ \"version\" ] > \"3.6\" ) Connects the underlying Mongo database and all collection connections Parameters Name Type Description Default force_reset bool whether to forcibly reset the connection False ensure_index ( self , key , unique = False , ** kwargs ) \u00b6 Show source code in maggma/stores/compound_stores.py 129 130 131 132 133 def ensure_index ( self , key , unique = False , ** kwargs ): \"\"\" Can't ensure index for JointStore \"\"\" raise NotImplementedError ( \"No ensure_index method for JointStore\" ) Can't ensure index for JointStore groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/stores/compound_stores.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: pipeline = self . _get_pipeline ( criteria = criteria , properties = properties , skip = skip , limit = limit ) if not isinstance ( keys , list ): keys = [ keys ] group_id = {} # type: Dict[str,Any] for key in keys : set_ ( group_id , key , \"$ {} \" . format ( key )) pipeline . append ({ \"$group\" : { \"_id\" : group_id , \"docs\" : { \"$push\" : \"$$ROOT\" }}}) agg = self . _collection . aggregate ( pipeline ) for d in agg : yield d [ \"_id\" ], d [ \"docs\" ] Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (dict, list of docs) name ( self ) \u00b6 Show source code in maggma/stores/compound_stores.py 55 56 57 58 59 def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" return self . master Return a string representing this data source query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in maggma/stores/compound_stores.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: pipeline = self . _get_pipeline ( criteria = criteria , properties = properties , skip = skip , limit = limit ) agg = self . _collection . aggregate ( pipeline ) for d in agg : yield d Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 query_one ( self , criteria = None , properties = None , ** kwargs ) \u00b6 Show source code in maggma/stores/compound_stores.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def query_one ( self , criteria = None , properties = None , ** kwargs ): \"\"\" Get one document Args: properties: properties to return in query criteria: filter for matching kwargs: kwargs for collection.aggregate Returns: single document \"\"\" # TODO: maybe adding explicit limit in agg pipeline is better as below? # pipeline = self._get_pipeline(properties, criteria) # pipeline.append({\"$limit\": 1}) query = self . query ( criteria = criteria , properties = properties , ** kwargs ) try : doc = next ( query ) return doc except StopIteration : return None Get one document Parameters Name Type Description Default properties _empty properties to return in query None criteria _empty filter for matching None kwargs _empty kwargs for collection.aggregate required Returns Type Description _empty single document remove_docs ( self , criteria ) \u00b6 Show source code in maggma/stores/compound_stores.py 275 276 277 278 279 280 281 282 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" raise NotImplementedError ( \"No remove_docs method for JointStore\" ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required update ( self , docs , update_lu = True , key = None , ** kwargs ) \u00b6 Show source code in maggma/stores/compound_stores.py 114 115 116 117 118 119 def update ( self , docs , update_lu = True , key = None , ** kwargs ): \"\"\" Update documents into the underlying collections Not Implemented for JointStore \"\"\" raise NotImplementedError ( \"JointStore is a read-only store\" ) Update documents into the underlying collections Not Implemented for JointStore","title":"Stores"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore","text":"A Store for access to a single or multiple JSON files","title":"JSONStore"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.__eq__","text":"Show source code in maggma/stores/mongolike.py 409 410 411 412 413 414 415 416 417 418 419 420 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for JSONStore Args: other: other JSONStore to compare with \"\"\" if not isinstance ( other , JSONStore ): return False fields = [ \"paths\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for JSONStore Parameters Name Type Description Default other object other JSONStore to compare with required","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.__init__","text":"Show source code in maggma/stores/mongolike.py 383 384 385 386 387 388 389 390 391 def __init__ ( self , paths : Union [ str , List [ str ]], ** kwargs ): \"\"\" Args: paths: paths for json files to turn into a Store \"\"\" paths = paths if isinstance ( paths , ( list , tuple )) else [ paths ] self . paths = paths self . kwargs = kwargs super () . __init__ ( collection_name = \"collection\" , ** kwargs ) Parameters Name Type Description Default paths Union [ str, List[str ] ] paths for json files to turn into a Store required","title":"__init__()"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.connect","text":"Show source code in maggma/stores/mongolike.py 393 394 395 396 397 398 399 400 401 402 403 404 def connect ( self , force_reset = False ): \"\"\" Loads the files into the collection in memory \"\"\" super () . connect ( force_reset = force_reset ) for path in self . paths : with zopen ( path ) as f : data = f . read () data = data . decode () if isinstance ( data , bytes ) else data objects = json . loads ( data ) objects = [ objects ] if not isinstance ( objects , list ) else objects self . update ( objects ) Loads the files into the collection in memory","title":"connect()"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore","text":"An in-memory Store that functions similarly to a MongoStore","title":"MemoryStore"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.name","text":"Name for the store","title":"name"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.__eq__","text":"Show source code in maggma/stores/mongolike.py 366 367 368 369 370 371 372 373 374 375 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MemoryStore other: other MemoryStore to compare with \"\"\" if not isinstance ( other , MemoryStore ): return False fields = [ \"collection_name\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for MemoryStore other: other MemoryStore to compare with","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.__hash__","text":"Show source code in maggma/stores/mongolike.py 322 323 324 def __hash__ ( self ): \"\"\" Hash for the store \"\"\" return hash (( self . name , self . last_updated_field )) Hash for the store","title":"__hash__()"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.__init__","text":"Show source code in maggma/stores/mongolike.py 299 300 301 302 303 304 305 306 307 308 def __init__ ( self , collection_name : str = \"memory_db\" , ** kwargs ): \"\"\" Initializes the Memory Store Args: collection_name: name for the collection in memory \"\"\" self . collection_name = collection_name self . _collection = None self . kwargs = kwargs super ( MongoStore , self ) . __init__ ( ** kwargs ) # noqa Initializes the Memory Store Parameters Name Type Description Default collection_name str name for the collection in memory memory_db","title":"__init__()"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.connect","text":"Show source code in maggma/stores/mongolike.py 310 311 312 313 314 315 def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if not self . _collection or force_reset : self . _collection = mongomock . MongoClient () . db [ self . name ] Connect to the source data","title":"connect()"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.groupby","text":"Show source code in maggma/stores/mongolike.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (key, list of elemnts) \"\"\" keys = keys if isinstance ( keys , list ) else [ keys ] data = [ doc for doc in self . query ( properties = keys , criteria = criteria ) if all ( has ( doc , k ) for k in keys ) ] def grouper ( doc ): return tuple ( get ( doc , k ) for k in keys ) for vals , group in groupby ( sorted ( data , key = grouper ), grouper ): doc = {} # type: Dict[Any,Any] for k , v in zip ( keys , vals ): set_ ( doc , k , v ) yield doc , list ( group ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (key, list of elemnts)","title":"groupby()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore","text":"A Store that connects to a Mongo collection","title":"MongoStore"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.name","text":"Return a string representing this data source","title":"name"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.__eq__","text":"Show source code in maggma/stores/mongolike.py 281 282 283 284 285 286 287 288 289 290 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MongoStore other: other mongostore to compare with \"\"\" if not isinstance ( other , MongoStore ): return False fields = [ \"database\" , \"collection_name\" , \"host\" , \"port\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for MongoStore other: other mongostore to compare with","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.__hash__","text":"Show source code in maggma/stores/mongolike.py 82 83 84 def __hash__ ( self ) -> int : \"\"\" Hash for MongoStore \"\"\" return hash (( self . database , self . collection_name , self . last_updated_field )) Hash for MongoStore","title":"__hash__()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.__init__","text":"Show source code in maggma/stores/mongolike.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __init__ ( self , database : str , collection_name : str , host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , ** kwargs , ): \"\"\" Args: database: The database name collection_name: The collection name host: Hostname for the database port: TCP port to connect to username: Username for the collection password: Password to connect with \"\"\" self . database = database self . collection_name = collection_name self . host = host self . port = port self . username = username self . password = password self . _collection = None # type: Any self . kwargs = kwargs super () . __init__ ( ** kwargs ) Parameters Name Type Description Default database str The database name required collection_name str The collection name required host str Hostname for the database localhost port int TCP port to connect to 27017 username str Username for the collection required password str Password to connect with required","title":"__init__()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.close","text":"Show source code in maggma/stores/mongolike.py 277 278 279 def close ( self ): \"\"\" Close up all collections \"\"\" self . _collection . database . client . close () Close up all collections","title":"close()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.connect","text":"Show source code in maggma/stores/mongolike.py 71 72 73 74 75 76 77 78 79 80 def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" if not self . _collection or force_reset : conn = MongoClient ( self . host , self . port ) db = conn [ self . database ] if self . username != \"\" : db . authenticate ( self . username , self . password ) self . _collection = db [ self . collection_name ] Connect to the source data","title":"connect()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.ensure_index","text":"Show source code in maggma/stores/mongolike.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 def ensure_index ( self , key : str , unique : Optional [ bool ] = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" if confirm_field_index ( self . _collection , key ): return True else : try : self . _collection . create_index ( key , unique = unique , background = True ) return True except Exception : return False Tries to create an index and return true if it suceeded Parameters Name Type Description Default key str single key to index required unique Optional [ bool ] Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.from_collection","text":"Show source code in maggma/stores/mongolike.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 @classmethod def from_collection ( cls , collection ): \"\"\" Generates a MongoStore from a pymongo collection object This is not a fully safe operation as it gives dummy information to the MongoStore As a result, this will not serialize and can not reset its connection Args: collection: the PyMongo collection to create a MongoStore around \"\"\" # TODO: How do we make this safer? coll_name = collection . name db_name = collection . database . name store = cls ( db_name , coll_name ) store . _collection = collection return store Generates a MongoStore from a pymongo collection object This is not a fully safe operation as it gives dummy information to the MongoStore As a result, this will not serialize and can not reset its connection Parameters Name Type Description Default collection _empty the PyMongo collection to create a MongoStore around required","title":"from_collection()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.from_db_file","text":"Show source code in maggma/stores/mongolike.py 86 87 88 89 90 91 92 93 94 95 96 97 @classmethod def from_db_file ( cls , filename : str ): \"\"\" Convenience method to construct MongoStore from db_file from old QueryEngine format \"\"\" kwargs = loadfn ( filename ) if \"collection\" in kwargs : kwargs [ \"collection_name\" ] = kwargs . pop ( \"collection\" ) # Get rid of aliases from traditional query engine db docs kwargs . pop ( \"aliases\" , None ) return cls ( ** kwargs ) Convenience method to construct MongoStore from db_file from old QueryEngine format","title":"from_db_file()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.groupby","text":"Show source code in maggma/stores/mongolike.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (key, list of docs) \"\"\" pipeline = [] if isinstance ( keys , str ): keys = [ keys ] if properties is None : properties = [] if isinstance ( properties , dict ): properties = list ( properties . keys ()) if criteria is not None : pipeline . append ({ \"$match\" : criteria }) if len ( properties ) > 0 : pipeline . append ({ \"$project\" : { p : 1 for p in properties + keys }}) alpha = \"abcdefghijklmnopqrstuvwxyz\" group_id = { letter : f \"$ {key} \" for letter , key in zip ( alpha , keys )} pipeline . append ({ \"$group\" : { \"_id\" : group_id , \"docs\" : { \"$push\" : \"$$ROOT\" }}}) for d in self . _collection . aggregate ( pipeline , allowDiskUse = True ): id_doc = {} # type: Dict[str,Any] for letter , key in group_id . items (): if has ( d [ \"_id\" ], letter ): set_ ( id_doc , key [ 1 :], d [ \"_id\" ][ letter ]) yield ( id_doc , d [ \"docs\" ]) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (key, list of docs)","title":"groupby()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.query","text":"Show source code in maggma/stores/mongolike.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( properties , list ): properties = { p : 1 for p in properties } sort_list = [( k , v . value ) for k , v in sort . items ()] if sort else None for d in self . _collection . find ( filter = criteria , projection = properties , skip = skip , limit = limit , sort = sort_list , ): yield d Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0","title":"query()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.remove_docs","text":"Show source code in maggma/stores/mongolike.py 268 269 270 271 272 273 274 275 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" self . _collection . delete_many ( filter = criteria ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.update","text":"Show source code in maggma/stores/mongolike.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" requests = [] if not isinstance ( docs , list ): docs = [ docs ] for d in docs : d = jsanitize ( d , allow_bson = True ) # document-level validation is optional validates = True if self . validator : validates = self . validator . is_valid ( d ) if not validates : if self . validator . strict : raise ValueError ( self . validator . validation_errors ( d )) else : self . logger . error ( self . validator . validation_errors ( d )) if validates : key = key or self . key if isinstance ( key , list ): search_doc = { k : d [ k ] for k in key } else : search_doc = { key : d [ key ]} requests . append ( ReplaceOne ( search_doc , d , upsert = True )) if len ( requests ) > 0 : self . _collection . bulk_write ( requests , ordered = False ) Update documents into the Store Parameters Name Type Description Default docs Union [ List[Dict ] , Dict ] the document or list of documents to update required key Union[List, str, None] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Module containing various definitions of Stores. Stores are a default access pattern to data and provide various utillities","title":"update()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore","text":"A Store for GrdiFS backend. Provides a common access method consistent with other stores","title":"GridFSStore"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.last_updated","text":"Provides the most recent last_updated date time stamp from the documents in this Store","title":"last_updated"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.__eq__","text":"Show source code in maggma/stores/gridfs.py 346 347 348 349 350 351 352 353 354 355 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for GridFSStore other: other GridFSStore to compare with \"\"\" if not isinstance ( other , GridFSStore ): return False fields = [ \"database\" , \"collection_name\" , \"host\" , \"port\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for GridFSStore other: other GridFSStore to compare with","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.__init__","text":"Show source code in maggma/stores/gridfs.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , database : str , collection_name : str , host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , compression : bool = False , ** kwargs , ): \"\"\" Initializes a GrdiFS Store for binary data Args: database: database name collection_name: The name of the collection. This is the string portion before the GridFS extensions host: hostname for the database port: port to connec to username: username to connect as password: password to authenticate as \"\"\" self . database = database self . collection_name = collection_name self . host = host self . port = port self . username = username self . password = password self . _collection = None # type: Any self . compression = compression self . kwargs = kwargs self . meta_keys = set () # type: Set[str] if \"key\" not in kwargs : kwargs [ \"key\" ] = \"_id\" super () . __init__ ( ** kwargs ) Initializes a GrdiFS Store for binary data Parameters Name Type Description Default database str database name required collection_name str The name of the collection. This is the string portion before the GridFS extensions required host str hostname for the database localhost port int port to connec to 27017 username str username to connect as required password str password to authenticate as required","title":"__init__()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.close","text":"Show source code in maggma/stores/gridfs.py 343 344 def close ( self ): self . _collection . database . client . close () Closes any connections","title":"close()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.connect","text":"Show source code in maggma/stores/gridfs.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" conn = MongoClient ( self . host , self . port ) if not self . _collection or force_reset : db = conn [ self . database ] if self . username != \"\" : db . authenticate ( self . username , self . password ) self . _collection = gridfs . GridFS ( db , self . collection_name ) self . _files_collection = db [ \" {} .files\" . format ( self . collection_name )] self . _files_store = MongoStore . from_collection ( self . _files_collection ) self . _files_store . last_updated_field = f \"metadata. {self.last_updated_field} \" self . _chunks_collection = db [ \" {} .chunks\" . format ( self . collection_name )] Connect to the source data","title":"connect()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.distinct","text":"Show source code in maggma/stores/gridfs.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def distinct ( self , field : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , all_exist : bool = False , ) -> Union [ List [ Dict ], List ]: \"\"\" Function get to get all distinct values of a certain key in a GridFs store. Args: field: key or keys for which to find distinct values or sets of values. criteria: criteria for filter all_exist: whether to ensure all keys in list exist in each document, defaults to False \"\"\" criteria = ( self . transform_criteria ( criteria ) if isinstance ( criteria , dict ) else criteria ) field = [ field ] if not isinstance ( field , list ) else field field = [ f \"metadata. {k} \" if k not in self . files_collection_fields and not k . startswith ( \"metadata.\" ) else k for k in field ] return self . _files_store . distinct ( field = field , criteria = criteria , all_exist = all_exist ) Function get to get all distinct values of a certain key in a GridFs store. Parameters Name Type Description Default field Union [ List[str ] , str ] key or keys for which to find distinct values or sets of values. required criteria Optional [ Dict ] criteria for filter None all_exist bool whether to ensure all keys in list exist in each document, defaults to False False","title":"distinct()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.ensure_index","text":"Show source code in maggma/stores/gridfs.py 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 def ensure_index ( self , key : str , unique : Optional [ bool ] = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Currently operators on the GridFS files collection Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" # Transform key for gridfs first if key not in self . files_collection_fields : key = \"metadata. {} \" . format ( key ) if confirm_field_index ( self . collection , key ): return True else : try : self . _collection . create_index ( key , unique = unique , background = True ) return True except Exception : return False Tries to create an index and return true if it suceeded Currently operators on the GridFS files collection Parameters Name Type Description Default key str single key to index required unique Optional [ bool ] Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.groupby","text":"Show source code in maggma/stores/gridfs.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Will only work if the keys are included in the files collection for GridFS Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" criteria = ( self . transform_criteria ( criteria ) if isinstance ( criteria , dict ) else criteria ) keys = [ keys ] if not isinstance ( keys , list ) else keys keys = [ f \"metadata. {k} \" if k not in self . files_collection_fields and not k . startswith ( \"metadata.\" ) else k for k in keys ] for group , ids in self . _files_store . groupby ( keys , criteria = criteria , properties = [ f \"metadata. {self.key} \" ] ): ids = [ get ( doc , f \"metadata. {self.key} \" ) for doc in ids if has ( doc , f \"metadata. {self.key} \" ) ] group = { k . replace ( \"metadata.\" , \"\" ): get ( group , k ) for k in keys if has ( group , k ) } yield group , list ( self . query ( criteria = { self . key : { \"$in\" : ids }})) Simple grouping function that will group documents by keys. Will only work if the keys are included in the files collection for GridFS Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (dict, list of docs)","title":"groupby()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.name","text":"Show source code in maggma/stores/gridfs.py 84 85 86 87 88 def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" return self . collection_name Return a string representing this data source","title":"name()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.query","text":"Show source code in maggma/stores/gridfs.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the GridFS Store for a set of documents Currently ignores properties TODO: If properties wholy in metadata, just query that Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) for f in self . _collection . find ( filter = criteria , skip = skip , limit = limit , sort = sort ): data = f . read () metadata = f . metadata if metadata . get ( \"compression\" , \"\" ) == \"zlib\" : data = zlib . decompress ( data ) . decode ( \"UTF-8\" ) try : data = json . loads ( data ) except Exception : pass yield data Queries the GridFS Store for a set of documents Currently ignores properties TODO: If properties wholy in metadata, just query that Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0","title":"query()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.remove_docs","text":"Show source code in maggma/stores/gridfs.py 329 330 331 332 333 334 335 336 337 338 339 340 341 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) ids = [ cursor . _id for cursor in self . _collection . find ( criteria )] for id in ids : self . _collection . delete ( id ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.transform_criteria","text":"Show source code in maggma/stores/gridfs.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 @classmethod def transform_criteria ( cls , criteria : Dict ) -> Dict : \"\"\" Allow client to not need to prepend 'metadata.' to query fields. Args: criteria: Query criteria \"\"\" new_criteria = dict () for field in criteria : if field not in cls . files_collection_fields and not field . startswith ( \"metadata.\" ): new_criteria [ \"metadata.\" + field ] = copy . copy ( criteria [ field ]) else : new_criteria [ field ] = copy . copy ( criteria [ field ]) return new_criteria Allow client to not need to prepend 'metadata.' to query fields. Parameters Name Type Description Default criteria Dict Query criteria required","title":"transform_criteria()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.update","text":"Show source code in maggma/stores/gridfs.py 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" if not isinstance ( docs , list ): docs = [ docs ] if isinstance ( key , str ): key = [ key ] elif not key : key = [ self . key ] key = list ( set ( key ) | self . meta_keys - set ( self . files_collection_fields )) for d in docs : search_doc = { k : d [ k ] for k in key } metadata = { k : d [ k ] for k in [ self . last_updated_field ] if k in d } metadata . update ( search_doc ) data = json . dumps ( jsanitize ( d )) . encode ( \"UTF-8\" ) if self . compression : data = zlib . compress ( data ) metadata [ \"compression\" ] = \"zlib\" self . _collection . put ( data , metadata = metadata ) search_doc = self . transform_criteria ( search_doc ) # Cleans up old gridfs entries for fdoc in ( self . _files_collection . find ( search_doc , [ \"_id\" ]) . sort ( \"uploadDate\" , - 1 ) . skip ( 1 ) ): self . _collection . delete ( fdoc [ \"_id\" ]) Update documents into the Store Parameters Name Type Description Default docs Union [ List[Dict ] , Dict ] the document or list of documents to update required key Union[List, str, None] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Advanced Stores for connecting to AWS data","title":"update()"},{"location":"reference/stores/#maggma.stores.aws.AmazonS3Store","text":"GridFS like storage using Amazon S3 and a regular store for indexing Assumes Amazon AWS key and secret key are set in environment or default config file","title":"AmazonS3Store"},{"location":"reference/stores/#maggma.stores.aws.AmazonS3Store.last_updated","text":"Provides the most recent last_updated date time stamp from the documents in this Store","title":"last_updated"},{"location":"reference/stores/#maggma.stores.aws.AmazonS3Store.__eq__","text":"Show source code in maggma/stores/aws.py 295 296 297 298 299 300 301 302 303 304 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for AmazonS3Store other: other AmazonS3Store to compare with \"\"\" if not isinstance ( other , AmazonS3Store ): return False fields = [ \"index\" , \"bucket\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for AmazonS3Store other: other AmazonS3Store to compare with","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.aws.AmazonS3Store.__init__","text":"Show source code in maggma/stores/aws.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , index : Store , bucket : str , compress : bool = False , ** kwargs ): \"\"\" Initializes an S3 Store Args: index: a store to use to index the S3 Bucket bucket: name of the bucket compress: compress files inserted into the store \"\"\" if boto3 is None : raise RuntimeError ( \"boto3 and botocore are required for AmazonS3Store\" ) self . index = index self . bucket = bucket self . compress = compress self . s3 = None # type: Any self . s3_bucket = None # type: Any # Force the key to be the same as the index kwargs [ \"key\" ] = index . key super ( AmazonS3Store , self ) . __init__ ( ** kwargs ) Initializes an S3 Store Parameters Name Type Description Default index Store a store to use to index the S3 Bucket required bucket str name of the bucket required compress bool compress files inserted into the store False","title":"__init__()"},{"location":"reference/stores/#maggma.stores.aws.AmazonS3Store.close","text":"Show source code in maggma/stores/aws.py 70 71 72 73 74 75 76 def close ( self ): \"\"\" Closes any connections \"\"\" self . index . close () self . s3 = None self . s3_bucket = None Closes any connections","title":"close()"},{"location":"reference/stores/#maggma.stores.aws.AmazonS3Store.connect","text":"Show source code in maggma/stores/aws.py 57 58 59 60 61 62 63 64 65 66 67 68 def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" self . index . connect ( force_reset = force_reset ) if not self . s3 : self . s3 = boto3 . resource ( \"s3\" ) if self . bucket not in [ bucket . name for bucket in self . s3 . buckets . all ()]: raise Exception ( \"Bucket not present on AWS: {} \" . format ( self . bucket )) self . s3_bucket = self . s3 . Bucket ( self . bucket ) Connect to the source data","title":"connect()"},{"location":"reference/stores/#maggma.stores.aws.AmazonS3Store.distinct","text":"Show source code in maggma/stores/aws.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def distinct ( self , field : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , all_exist : bool = False , ) -> Union [ List [ Dict ], List ]: \"\"\" Get all distinct values for a field(s) For a single field, this returns a list of values For multiple fields, this return a list of of dictionaries for each unique combination Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in all_exist: ensure all fields exist for the distinct set \"\"\" # Index is a store so it should have its own distinct function return self . index . distinct ( field , criteria = criteria , all_exist = all_exist ) Get all distinct values for a field(s) For a single field, this returns a list of values For multiple fields, this return a list of of dictionaries for each unique combination Parameters Name Type Description Default field Union [ List[str ] , str ] the field(s) to get distinct values for required criteria Optional [ Dict ] PyMongo filter for documents to search in None all_exist bool ensure all fields exist for the distinct set False","title":"distinct()"},{"location":"reference/stores/#maggma.stores.aws.AmazonS3Store.ensure_index","text":"Show source code in maggma/stores/aws.py 183 184 185 186 187 188 189 190 191 192 193 194 def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" return self . index . ensure_index ( key , unique = unique ) Tries to create an index and return true if it suceeded Parameters Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.aws.AmazonS3Store.groupby","text":"Show source code in maggma/stores/aws.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" return self . index . groupby ( keys = keys , criteria = criteria , properties = properties , sort = sort , skip = skip , limit = limit , ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (dict, list of docs)","title":"groupby()"},{"location":"reference/stores/#maggma.stores.aws.AmazonS3Store.name","text":"Show source code in maggma/stores/aws.py 50 51 52 53 54 55 def name ( self ) -> str : \"\"\" Returns: a string representing this data source \"\"\" return self . bucket Returns Type Description str a string representing this data source","title":"name()"},{"location":"reference/stores/#maggma.stores.aws.AmazonS3Store.newer_in","text":"Show source code in maggma/stores/aws.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def newer_in ( self , target : Store , criteria : Optional [ Dict ] = None , exhaustive : bool = False ) -> List [ str ]: \"\"\" Returns the keys of documents that are newer in the target Store than this Store. Args: target: target Store criteria: PyMongo filter for documents to search in exhaustive: triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in \"\"\" return self . index . newer_in ( target = target , criteria = criteria , exhaustive = exhaustive ) Returns the keys of documents that are newer in the target Store than this Store. Parameters Name Type Description Default target Store target Store required criteria Optional [ Dict ] PyMongo filter for documents to search in None exhaustive bool triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in False","title":"newer_in()"},{"location":"reference/stores/#maggma.stores.aws.AmazonS3Store.query","text":"Show source code in maggma/stores/aws.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" for doc in self . index . query ( criteria = criteria , sort = sort , limit = limit , skip = skip ): try : # TODO: THis is ugly and unsafe, do some real checking before pulling data data = self . s3_bucket . Object ( doc [ self . key ]) . get ()[ \"Body\" ] . read () except botocore . exceptions . ClientError as e : # If a client error is thrown, then check that it was a 404 error. # If it was a 404 error, then the object does not exist. error_code = int ( e . response [ \"Error\" ][ \"Code\" ]) if error_code == 404 : self . logger . error ( \"Could not find S3 object {} \" . format ( doc [ self . key ]) ) break else : raise e if doc . get ( \"compression\" , \"\" ) == \"zlib\" : data = zlib . decompress ( data ) yield json . loads ( data ) Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0","title":"query()"},{"location":"reference/stores/#maggma.stores.aws.AmazonS3Store.rebuild_index_from_s3_data","text":"Show source code in maggma/stores/aws.py 283 284 285 286 287 288 289 290 291 292 293 def rebuild_index_from_s3_data ( self ): \"\"\" Rebuilds the index Store from the data in S3 Relies on the index document being stores as the metadata for the file \"\"\" index_docs = [] for file in self . s3_bucket . objects . all (): # TODO: Transform the data back from strings and remove AWS S3 specific keys index_docs . append ( file . metadata ) self . index . update ( index_docs ) Rebuilds the index Store from the data in S3 Relies on the index document being stores as the metadata for the file","title":"rebuild_index_from_s3_data()"},{"location":"reference/stores/#maggma.stores.aws.AmazonS3Store.remove_docs","text":"Show source code in maggma/stores/aws.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 def remove_docs ( self , criteria : Dict , remove_s3_object : bool = False ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match remove_s3_object: whether to remove the actual S3 Object or not \"\"\" if not remove_s3_object : self . index . remove_docs ( criteria = criteria ) else : to_remove = self . index . distinct ( self . key , criteria = criteria ) self . index . remove_docs ( criteria = criteria ) # Can remove up to 1000 items at a time via boto to_remove_chunks = list ( grouper ( to_remove , n = 1000 )) for chunk_to_remove in to_remove_chunks : objlist = [{ \"Key\" : obj } for obj in chunk_to_remove ] self . s3_bucket . delete_objects ( Delete = { \"Objects\" : objlist }) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required remove_s3_object bool whether to remove the actual S3 Object or not False","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.aws.AmazonS3Store.update","text":"Show source code in maggma/stores/aws.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" search_docs = [] search_keys = [] if isinstance ( key , list ): search_keys = key elif key : search_keys = [ key ] else : search_keys = [ self . key ] for d in docs : search_doc = { k : d [ k ] for k in search_keys } search_doc [ self . key ] = d [ self . key ] # Ensure key is in metadata # Remove MongoDB _id from search if \"_id\" in search_doc : del search_doc [ \"_id\" ] data = json . dumps ( jsanitize ( d )) . encode () # Compress with zlib if chosen if self . compress : search_doc [ \"compression\" ] = \"zlib\" data = zlib . compress ( data ) self . s3_bucket . put_object ( Key = d [ self . key ], Body = data , Metadata = search_doc ) search_docs . append ( search_doc ) # Use store's update to remove key clashes self . index . update ( search_docs ) Update documents into the Store Parameters Name Type Description Default docs Union [ List[Dict ] , Dict ] the document or list of documents to update required key Optional[Union[List, str]] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Advanced Stores for behavior outside normal access patterns","title":"update()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore","text":"Special Store that aliases for the primary accessors","title":"AliasingStore"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.__eq__","text":"Show source code in maggma/stores/advanced_stores.py 360 361 362 363 364 365 366 367 368 369 370 371 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for AliasingStore Args: other: other AliasingStore to compare with \"\"\" if not isinstance ( other , AliasingStore ): return False fields = [ \"store\" , \"aliases\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for AliasingStore Parameters Name Type Description Default other object other AliasingStore to compare with required","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.__init__","text":"Show source code in maggma/stores/advanced_stores.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def __init__ ( self , store : Store , aliases : Dict , ** kwargs ): \"\"\" Args: store: the store to wrap around aliases: dict of aliases of the form external key: internal key \"\"\" self . store = store # Given an external key tells what the internal key is self . aliases = aliases # Given the internal key tells us what the external key is self . reverse_aliases = { v : k for k , v in aliases . items ()} self . kwargs = kwargs kwargs . update ( { \"last_updated_field\" : store . last_updated_field , \"last_updated_type\" : store . last_updated_type , } ) super ( AliasingStore , self ) . __init__ ( ** kwargs ) Parameters Name Type Description Default store Store the store to wrap around required aliases Dict dict of aliases of the form external key: internal key required","title":"__init__()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.close","text":"Show source code in maggma/stores/advanced_stores.py 350 351 def close ( self ): self . store . close () Closes any connections","title":"close()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.connect","text":"Show source code in maggma/stores/advanced_stores.py 357 358 def connect ( self , force_reset = False ): self . store . connect ( force_reset = force_reset ) Connect to the source data Parameters Name Type Description Default force_reset _empty whether to reset the connection or not False","title":"connect()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.distinct","text":"Show source code in maggma/stores/advanced_stores.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 def distinct ( self , field : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , all_exist : bool = False , ) -> List : \"\"\" Get all distinct values for a key Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in all_exist: ensure all fields exist for the distinct set \"\"\" criteria = criteria if criteria else {} lazy_substitute ( criteria , self . reverse_aliases ) field = field if isinstance ( field , list ) else [ field ] # substitute forward field = [ self . aliases [ f ] for f in field ] return self . store . distinct ( field , criteria = criteria ) Get all distinct values for a key Parameters Name Type Description Default field Union [ List[str ] , str ] the field(s) to get distinct values for required criteria Optional [ Dict ] PyMongo filter for documents to search in None all_exist bool ensure all fields exist for the distinct set False","title":"distinct()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.ensure_index","text":"Show source code in maggma/stores/advanced_stores.py 345 346 347 348 def ensure_index ( self , key , unique = False , ** kwargs ): if key in self . aliases : key = self . aliases return self . store . ensure_index ( key , unique , ** kwargs ) Tries to create an index and return true if it suceeded Parameters Name Type Description Default key _empty single key to index required unique _empty Whether or not this index contains only unique keys False Returns Type Description _empty bool indicating if the index exists/was created","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.groupby","text":"Show source code in maggma/stores/advanced_stores.py 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" # Convert to a list keys = keys if isinstance ( keys , list ) else [ keys ] # Make the aliasing transformations on keys keys = [ self . aliases [ k ] if k in self . aliases else k for k in keys ] # Update criteria and properties based on aliases criteria = criteria if criteria else {} substitute ( properties , self . reverse_aliases ) lazy_substitute ( criteria , self . reverse_aliases ) return self . store . groupby ( keys = keys , properties = properties , criteria = criteria , skip = skip , limit = limit ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (dict, list of docs)","title":"groupby()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.name","text":"Show source code in maggma/stores/advanced_stores.py 216 217 218 219 220 def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" return self . store . name Return a string representing this data source","title":"name()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.query","text":"Show source code in maggma/stores/advanced_stores.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( properties , list ): properties = { p : 1 for p in properties } criteria = criteria if criteria else {} substitute ( properties , self . reverse_aliases ) lazy_substitute ( criteria , self . reverse_aliases ) for d in self . store . query ( properties = properties , criteria = criteria , sort = sort , limit = limit , skip = skip ): substitute ( d , self . aliases ) yield d Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0","title":"query()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.remove_docs","text":"Show source code in maggma/stores/advanced_stores.py 334 335 336 337 338 339 340 341 342 343 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" # Update criteria and properties based on aliases lazy_substitute ( criteria , self . reverse_aliases ) self . store . remove_docs ( criteria ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.update","text":"Show source code in maggma/stores/advanced_stores.py 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" key = key if key else self . key for d in docs : substitute ( d , self . reverse_aliases ) if key in self . aliases : key = self . aliases [ key ] self . store . update ( docs , key = key ) Update documents into the Store Parameters Name Type Description Default docs Union [ List[Dict ] , Dict ] the document or list of documents to update required key Optional[Union[List, str]] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None","title":"update()"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore","text":"Initialize a Store with a mongogrant \" : / .\" spec. Some class methods of MongoStore , e . g . from_db_file and from_collection , are not supported . mongogrant documentation : https : // github . com / materialsproject / mongogrant","title":"MongograntStore"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.__eq__","text":"Show source code in maggma/stores/advanced_stores.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MongograntStore Args: other: other MongograntStore to compare with \"\"\" if not isinstance ( other , MongograntStore ): return False fields = [ \"mongogrant_spec\" , \"collection_name\" , \"mgclient_config_path\" , \"last_updated_field\" , ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for MongograntStore Parameters Name Type Description Default other object other MongograntStore to compare with required","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.__init__","text":"Show source code in maggma/stores/advanced_stores.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , mongogrant_spec : str , collection_name : str , mgclient_config_path : Optional [ str ] = None , ** kwargs ): \"\"\" Args: mongogrant_spec: of the form <role>:<host>/<db>, where role is one of {\"read\", \"readWrite\"} or aliases {\"ro\", \"rw\"}; host is a db host (w/ optional port) or alias; and db is a db on that host, or alias. See mongogrant documentation. collection_name: name of mongo collection mgclient_config_path: Path to mongogrant client config file, or None if default path (`mongogrant.client.path`). \"\"\" self . mongogrant_spec = mongogrant_spec self . collection_name = collection_name self . mgclient_config_path = mgclient_config_path self . _collection = None if set (( \"username\" , \"password\" , \"database\" , \"host\" )) & set ( kwargs ): raise StoreError ( \"MongograntStore does not accept \" \"username, password, database, or host \" \"arguments. Use `mongogrant_spec`.\" ) self . kwargs = kwargs super ( MongoStore , self ) . __init__ ( ** kwargs ) # noqa Parameters Name Type Description Default mongogrant_spec str of the form : / , where role is one of required collection_name str name of mongo collection required mgclient_config_path Optional [ str ] Path to mongogrant client config file, or None if default path ( mongogrant.client.path ). None","title":"__init__()"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.connect","text":"Show source code in maggma/stores/advanced_stores.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def connect ( self , force_reset : bool = False ): \"\"\" Connect to the mongogrant source Args: force_reset: forces the connection to reset rather than just ensuring the connection is present \"\"\" if not self . _collection or force_reset : if self . mgclient_config_path : config = Config ( check = check , path = self . mgclient_config_path ) client = Client ( config ) else : client = Client () db = client . db ( self . mongogrant_spec ) self . _collection = db [ self . collection_name ] Connect to the mongogrant source Parameters Name Type Description Default force_reset bool forces the connection to reset rather than just ensuring the connection is present False","title":"connect()"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.from_collection","text":"Show source code in maggma/stores/advanced_stores.py 91 92 93 94 95 96 @classmethod def from_collection ( cls , collection ): \"\"\" Raises ValueError since MongograntStores can't be initialized from a PyMongo collection \"\"\" raise ValueError ( \"MongograntStore doesn't implement from_collection\" ) Raises ValueError since MongograntStores can't be initialized from a PyMongo collection","title":"from_collection()"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.from_db_file","text":"Show source code in maggma/stores/advanced_stores.py 84 85 86 87 88 89 @classmethod def from_db_file ( cls , file ): \"\"\" Raises ValueError since MongograntStores can't be initialized from a file \"\"\" raise ValueError ( \"MongograntStore doesn't implement from_db_file\" ) Raises ValueError since MongograntStores can't be initialized from a file","title":"from_db_file()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore","text":"Provides a sandboxed view to another store","title":"SandboxStore"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.sbx_criteria","text":"Returns Type Description Dict the sandbox criteria dict used to filter the source store","title":"sbx_criteria"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.__eq__","text":"Show source code in maggma/stores/advanced_stores.py 518 519 520 521 522 523 524 525 526 527 528 529 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for SandboxStore Args: other: other SandboxStore to compare with \"\"\" if not isinstance ( other , SandboxStore ): return False fields = [ \"store\" , \"sandbox\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for SandboxStore Parameters Name Type Description Default other object other SandboxStore to compare with required","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.__init__","text":"Show source code in maggma/stores/advanced_stores.py 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 def __init__ ( self , store : Store , sandbox : str , exclusive : bool = False ): \"\"\" Args: store: store to wrap sandboxing around sandbox: the corresponding sandbox exclusive: whether to be exclusively in this sandbox or include global items \"\"\" self . store = store self . sandbox = sandbox self . exclusive = exclusive super () . __init__ ( key = self . store . key , last_updated_field = self . store . last_updated_field , last_updated_type = self . store . last_updated_type , validator = self . store . validator , ) Parameters Name Type Description Default store Store store to wrap sandboxing around required sandbox str the corresponding sandbox required exclusive bool whether to be exclusively in this sandbox or include global items False","title":"__init__()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.close","text":"Show source code in maggma/stores/advanced_stores.py 508 509 def close ( self ): self . store . close () Closes any connections","title":"close()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.connect","text":"Show source code in maggma/stores/advanced_stores.py 515 516 def connect ( self , force_reset = False ): self . store . connect ( force_reset = force_reset ) Connect to the source data Parameters Name Type Description Default force_reset _empty whether to reset the connection or not False","title":"connect()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.ensure_index","text":"Show source code in maggma/stores/advanced_stores.py 505 506 def ensure_index ( self , key , unique = False , ** kwargs ): return self . store . ensure_index ( key , unique , ** kwargs ) Tries to create an index and return true if it suceeded Parameters Name Type Description Default key _empty single key to index required unique _empty Whether or not this index contains only unique keys False Returns Type Description _empty bool indicating if the index exists/was created","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.groupby","text":"Show source code in maggma/stores/advanced_stores.py 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . groupby ( keys = keys , properties = properties , criteria = criteria , skip = skip , limit = limit ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (dict, list of docs)","title":"groupby()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.name","text":"Show source code in maggma/stores/advanced_stores.py 396 397 398 399 400 401 def name ( self ) -> str : \"\"\" Returns: a string representing this data source \"\"\" return self . store . name Returns Type Description str a string representing this data source","title":"name()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.query","text":"Show source code in maggma/stores/advanced_stores.py 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . query ( properties = properties , criteria = criteria , sort = sort , limit = limit , skip = skip ) Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0","title":"query()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.remove_docs","text":"Show source code in maggma/stores/advanced_stores.py 492 493 494 495 496 497 498 499 500 501 502 503 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" # Update criteria and properties based on aliases criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) self . store . remove_docs ( criteria ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.update","text":"Show source code in maggma/stores/advanced_stores.py 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" for d in docs : if \"sbxn\" in d : d [ \"sbxn\" ] = list ( set ( d [ \"sbxn\" ] + [ self . sandbox ])) else : d [ \"sbxn\" ] = [ self . sandbox ] self . store . update ( docs , key = key ) Update documents into the Store Parameters Name Type Description Default docs Union [ List[Dict ] , Dict ] the document or list of documents to update required key Optional[Union[List, str]] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None","title":"update()"},{"location":"reference/stores/#maggma.stores.advanced_stores.VaultStore","text":"Extends MongoStore to read credentials out of Vault server and uses these values to initialize MongoStore instance","title":"VaultStore"},{"location":"reference/stores/#maggma.stores.advanced_stores.VaultStore.__eq__","text":"Show source code in maggma/stores/advanced_stores.py 176 177 178 179 180 181 182 183 184 185 186 187 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for VaultStore Args: other: other VaultStore to compare with \"\"\" if not isinstance ( other , VaultStore ): return False fields = [ \"vault_secret_path\" , \"collection_name\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for VaultStore Parameters Name Type Description Default other object other VaultStore to compare with required","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.advanced_stores.VaultStore.__init__","text":"Show source code in maggma/stores/advanced_stores.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 @requires ( hvac is not None , \"hvac is required to use VaultStore\" ) def __init__ ( self , collection_name : str , vault_secret_path : str ): \"\"\" Args: collection_name: name of mongo collection vault_secret_path: path on vault server with mongo creds object Important: Environment variables that must be set prior to invocation VAULT_ADDR - URL of vault server (eg. https://matgen8.lbl.gov:8200) VAULT_TOKEN or GITHUB_TOKEN - token used to authenticate to vault \"\"\" self . collection_name = collection_name self . vault_secret_path = vault_secret_path # TODO: Switch this over to Pydantic ConfigSettings vault_addr = os . getenv ( \"VAULT_ADDR\" ) if not vault_addr : raise RuntimeError ( \"VAULT_ADDR not set\" ) client = hvac . Client ( vault_addr ) # If we have a vault token use this token = os . getenv ( \"VAULT_TOKEN\" ) # Look for a github token instead if not token : github_token = os . getenv ( \"GITHUB_TOKEN\" ) if github_token : client . auth_github ( github_token ) else : raise RuntimeError ( \"VAULT_TOKEN or GITHUB_TOKEN not set\" ) else : client . token = token if not client . is_authenticated (): raise RuntimeError ( \"Bad token\" ) # Read the vault secret json_db_creds = client . read ( vault_secret_path ) db_creds = json . loads ( json_db_creds [ \"data\" ][ \"value\" ]) database = db_creds . get ( \"db\" ) host = db_creds . get ( \"host\" , \"localhost\" ) port = db_creds . get ( \"port\" , 27017 ) username = db_creds . get ( \"username\" , \"\" ) password = db_creds . get ( \"password\" , \"\" ) super ( VaultStore , self ) . __init__ ( database , collection_name , host , port , username , password ) Parameters Name Type Description Default collection_name str name of mongo collection required vault_secret_path str path on vault server with mongo creds object required Important Environment variables that must be set prior to invocation VAULT_ADDR - URL of vault server (eg. https://matgen8.lbl.gov:8200) VAULT_TOKEN or GITHUB_TOKEN - token used to authenticate to vault Special stores that combine underlying Stores together","title":"__init__()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore","text":"Store concatting multiple stores","title":"ConcatStore"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.last_updated","text":"Finds the most recent last_updated across all the stores. This might not be the most usefull way to do this for this type of Store since it could very easily over-estimate the last_updated based on what stores are used","title":"last_updated"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.__eq__","text":"Show source code in maggma/stores/compound_stores.py 498 499 500 501 502 503 504 505 506 507 508 509 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for ConcatStore Args: other: other JointStore to compare with \"\"\" if not isinstance ( other , ConcatStore ): return False fields = [ \"stores\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for ConcatStore Parameters Name Type Description Default other object other JointStore to compare with required","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.__init__","text":"Show source code in maggma/stores/compound_stores.py 307 308 309 310 311 312 313 314 315 316 def __init__ ( self , * stores : Store , ** kwargs ): \"\"\" Initialize a ConcatStore that concatenates multiple stores together to appear as one store Args: stores: list of stores to concatenate together \"\"\" self . stores = stores super ( ConcatStore , self ) . __init__ ( ** kwargs ) Initialize a ConcatStore that concatenates multiple stores together to appear as one store Parameters Name Type Description Default stores Store list of stores to concatenate together required","title":"__init__()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.close","text":"Show source code in maggma/stores/compound_stores.py 334 335 336 337 338 339 def close ( self ): \"\"\" Close all connections in this ConcatStore \"\"\" for store in self . stores : store . close () Close all connections in this ConcatStore","title":"close()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.connect","text":"Show source code in maggma/stores/compound_stores.py 324 325 326 327 328 329 330 331 332 def connect ( self , force_reset : bool = False ): \"\"\" Connect all stores in this ConcatStore Args: force_reset: Whether to forcibly reset the connection for all stores \"\"\" for store in self . stores : store . connect ( force_reset ) Connect all stores in this ConcatStore Parameters Name Type Description Default force_reset bool Whether to forcibly reset the connection for all stores False","title":"connect()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.distinct","text":"Show source code in maggma/stores/compound_stores.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 def distinct ( self , field : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , all_exist : bool = False , ) -> Union [ List [ Dict ], List ]: \"\"\" Get all distinct values for a field(s) For a single field, this returns a list of values For multiple fields, this return a list of of dictionaries for each unique combination Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in all_exist: ensure all fields exist for the distinct set \"\"\" distincts = [] for store in self . stores : distincts . extend ( store . distinct ( field = field , criteria = criteria , all_exist = all_exist ) ) if isinstance ( field , str ): return list ( set ( distincts )) else : return [ dict ( s ) for s in set ( frozenset ( d . items ()) for d in distincts )] Get all distinct values for a field(s) For a single field, this returns a list of values For multiple fields, this return a list of of dictionaries for each unique combination Parameters Name Type Description Default field Union [ List[str ] , str ] the field(s) to get distinct values for required criteria Optional [ Dict ] PyMongo filter for documents to search in None all_exist bool ensure all fields exist for the distinct set False","title":"distinct()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.ensure_index","text":"Show source code in maggma/stores/compound_stores.py 401 402 403 404 405 406 407 408 409 410 411 412 def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Ensure an index is properly set. Returns whether all stores support this index or not Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created on all stores \"\"\" return all ([ store . ensure_index ( key , unique ) for store in self . stores ]) Ensure an index is properly set. Returns whether all stores support this index or not Parameters Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created on all stores","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.groupby","text":"Show source code in maggma/stores/compound_stores.py 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" if isinstance ( keys , str ): keys = [ keys ] docs = [] for store in self . stores : temp_docs = list ( store . groupby ( keys = keys , criteria = criteria , properties = properties , sort = sort , skip = skip , limit = limit , ) ) for key , group in temp_docs : docs . extend ( group ) def key_set ( d : Dict ) -> Tuple : \"index function based on passed in keys\" test_d = tuple ( d . get ( k , None ) for k in keys ) return test_d sorted_docs = sorted ( docs , key = key_set ) for vals , group_iter in groupby ( sorted_docs , key = key_set ): id_dict = { key : val for key , val in zip ( keys , vals )} yield id_dict , list ( group_iter ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (dict, list of docs)","title":"groupby()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.name","text":"Show source code in maggma/stores/compound_stores.py 318 319 320 321 322 def name ( self ) -> str : \"\"\" A string representing this data source \"\"\" return self . stores [ 0 ] . name A string representing this data source","title":"name()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.query","text":"Show source code in maggma/stores/compound_stores.py 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries across all Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" # TODO: skip, sort and limit are broken. implement properly for store in self . stores : for d in store . query ( criteria = criteria , properties = properties ): yield d Queries across all Store for a set of documents Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0","title":"query()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.remove_docs","text":"Show source code in maggma/stores/compound_stores.py 489 490 491 492 493 494 495 496 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" raise NotImplementedError ( \"No remove_docs method for JointStore\" ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.update","text":"Show source code in maggma/stores/compound_stores.py 360 361 362 363 364 365 366 367 368 369 370 371 372 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Not implemented in ConcatStore Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" raise NotImplementedError ( \"No update method for ConcatStore\" ) Update documents into the Store Not implemented in ConcatStore Parameters Name Type Description Default docs Union [ List[Dict ] , Dict ] the document or list of documents to update required key Optional[Union[List, str]] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None","title":"update()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore","text":"Store that implements a on-the-fly join across multiple collections all in the same MongoDB database. This is a Read-Only Store designed to combine data from multiple collections.","title":"JointStore"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.last_updated","text":"Special last_updated for this JointStore that checks all underlying collections","title":"last_updated"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.nonmaster_names","text":"alll non-master collection names","title":"nonmaster_names"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.__eq__","text":"Show source code in maggma/stores/compound_stores.py 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for JointStore Args: other: other JointStore to compare with \"\"\" if not isinstance ( other , JointStore ): return False fields = [ \"database\" , \"collection_names\" , \"host\" , \"port\" , \"master\" , \"merge_at_root\" , ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for JointStore Parameters Name Type Description Default other object other JointStore to compare with required","title":"__eq__()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.__init__","text":"Show source code in maggma/stores/compound_stores.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , database : str , collection_names : List [ str ], host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , master : Optional [ str ] = None , merge_at_root : bool = False , ** kwargs ): \"\"\" Args: database: The database name collection_names: list of all collections to join host: Hostname for the database port: TCP port to connect to username: Username for the collection password: Password to connect with master: name for the master collection if not specified this defaults to the first in collection_names list \"\"\" self . database = database self . collection_names = collection_names self . host = host self . port = port self . username = username self . password = password self . _collection = None # type: Any self . master = master or collection_names [ 0 ] self . merge_at_root = merge_at_root self . kwargs = kwargs super ( JointStore , self ) . __init__ ( ** kwargs ) Parameters Name Type Description Default database str The database name required collection_names List [ str ] list of all collections to join required host str Hostname for the database localhost port int TCP port to connect to 27017 username str Username for the collection required password str Password to connect with required master Optional [ str ] name for the master collection if not specified this defaults to the first in collection_names list None","title":"__init__()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.close","text":"Show source code in maggma/stores/compound_stores.py 78 79 80 81 82 def close ( self ): \"\"\" Closes underlying database connections \"\"\" self . _collection . database . client . close () Closes underlying database connections","title":"close()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.connect","text":"Show source code in maggma/stores/compound_stores.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def connect ( self , force_reset : bool = False ): \"\"\" Connects the underlying Mongo database and all collection connections Args: force_reset: whether to forcibly reset the connection \"\"\" conn = MongoClient ( self . host , self . port ) db = conn [ self . database ] if self . username != \"\" : db . authenticate ( self . username , self . password ) self . _collection = db [ self . master ] self . _has_merge_objects = ( self . _collection . database . client . server_info ()[ \"version\" ] > \"3.6\" ) Connects the underlying Mongo database and all collection connections Parameters Name Type Description Default force_reset bool whether to forcibly reset the connection False","title":"connect()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.ensure_index","text":"Show source code in maggma/stores/compound_stores.py 129 130 131 132 133 def ensure_index ( self , key , unique = False , ** kwargs ): \"\"\" Can't ensure index for JointStore \"\"\" raise NotImplementedError ( \"No ensure_index method for JointStore\" ) Can't ensure index for JointStore","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.groupby","text":"Show source code in maggma/stores/compound_stores.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: pipeline = self . _get_pipeline ( criteria = criteria , properties = properties , skip = skip , limit = limit ) if not isinstance ( keys , list ): keys = [ keys ] group_id = {} # type: Dict[str,Any] for key in keys : set_ ( group_id , key , \"$ {} \" . format ( key )) pipeline . append ({ \"$group\" : { \"_id\" : group_id , \"docs\" : { \"$push\" : \"$$ROOT\" }}}) agg = self . _collection . aggregate ( pipeline ) for d in agg : yield d [ \"_id\" ], d [ \"docs\" ] Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union [ List[str ] , str ] fields to group documents required criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator [ Tuple[Dict, List[Dict ] ]] generator returning tuples of (dict, list of docs)","title":"groupby()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.name","text":"Show source code in maggma/stores/compound_stores.py 55 56 57 58 59 def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" return self . master Return a string representing this data source","title":"name()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.query","text":"Show source code in maggma/stores/compound_stores.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: pipeline = self . _get_pipeline ( criteria = criteria , properties = properties , skip = skip , limit = limit ) agg = self . _collection . aggregate ( pipeline ) for d in agg : yield d Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional [ Dict ] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Union[Dict[str, maggma.core.store.Sort]]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0","title":"query()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.query_one","text":"Show source code in maggma/stores/compound_stores.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def query_one ( self , criteria = None , properties = None , ** kwargs ): \"\"\" Get one document Args: properties: properties to return in query criteria: filter for matching kwargs: kwargs for collection.aggregate Returns: single document \"\"\" # TODO: maybe adding explicit limit in agg pipeline is better as below? # pipeline = self._get_pipeline(properties, criteria) # pipeline.append({\"$limit\": 1}) query = self . query ( criteria = criteria , properties = properties , ** kwargs ) try : doc = next ( query ) return doc except StopIteration : return None Get one document Parameters Name Type Description Default properties _empty properties to return in query None criteria _empty filter for matching None kwargs _empty kwargs for collection.aggregate required Returns Type Description _empty single document","title":"query_one()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.remove_docs","text":"Show source code in maggma/stores/compound_stores.py 275 276 277 278 279 280 281 282 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" raise NotImplementedError ( \"No remove_docs method for JointStore\" ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.update","text":"Show source code in maggma/stores/compound_stores.py 114 115 116 117 118 119 def update ( self , docs , update_lu = True , key = None , ** kwargs ): \"\"\" Update documents into the underlying collections Not Implemented for JointStore \"\"\" raise NotImplementedError ( \"JointStore is a read-only store\" ) Update documents into the underlying collections Not Implemented for JointStore","title":"update()"}]}
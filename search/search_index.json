{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Maggma What is Maggma Maggma is a framework to build data pipelines from files on disk all the way to a REST API in scientific environments. Maggma has been developed by the Materials Project (MP) team at Lawrence Berkeley Labs. Maggma is written in Python and supports Python 3.+. Installation from PyPI Maggma is published on the Python Package Index . The preferred tool for installing packages from PyPi is pip . This tool is provided with all modern versions of Python. Open your terminal and run the following command. pip install -U maggma Installation from source You can install Maggma directly from a clone of the Git repository . This can be done either by cloning the repo and installing from the local clone, or simply installing directly via git . Local Clone git clone https://github.com//materialsproject/maggma cd sphinx pip install . Direct Git pip install git+https://github.com/materialsproject/maggma","title":"Home"},{"location":"#maggma","text":"","title":"Maggma"},{"location":"#what-is-maggma","text":"Maggma is a framework to build data pipelines from files on disk all the way to a REST API in scientific environments. Maggma has been developed by the Materials Project (MP) team at Lawrence Berkeley Labs. Maggma is written in Python and supports Python 3.+.","title":"What is Maggma"},{"location":"#installation-from-pypi","text":"Maggma is published on the Python Package Index . The preferred tool for installing packages from PyPi is pip . This tool is provided with all modern versions of Python. Open your terminal and run the following command. pip install -U maggma","title":"Installation from PyPI"},{"location":"#installation-from-source","text":"You can install Maggma directly from a clone of the Git repository . This can be done either by cloning the repo and installing from the local clone, or simply installing directly via git . Local Clone git clone https://github.com//materialsproject/maggma cd sphinx pip install . Direct Git pip install git+https://github.com/materialsproject/maggma","title":"Installation from source"},{"location":"concepts/","text":"MSONable Maggma objects implement the MSONable pattern which enables these objects to serialize and deserialize to python dictionaries or even JSON. The MSONable encoder injects in @module and @class info so that the object can be deserialized without the manual. This enables much of Maggma to operate like a plugin system. Store Stores are document-based data sources and data sinks. They are modeled around the MongoDB collection although they can represent more complex data sources as well. Stores implement methods to connect , query , find distinct values, groupby fields, update documents, and remove documents. Stores also implement a number of critical fields for Maggma: the key and the last_updated_field . key is the field that is used to index the underlying data source. last_updated_field is the timestamp of when that document. Builder Builders represent a data transformation step. Builders break down each transformation into 3 key steps: get_items , process_item , and update_targets . Both get_items and update_targets can perform IO to the data stores. process_item is expected to not perform any IO so that it can be parallelized by Maggma. Builders can be chained together into a array and then saved as a JSON file to be run on a production system.","title":"Core Concepts"},{"location":"concepts/#msonable","text":"Maggma objects implement the MSONable pattern which enables these objects to serialize and deserialize to python dictionaries or even JSON. The MSONable encoder injects in @module and @class info so that the object can be deserialized without the manual. This enables much of Maggma to operate like a plugin system.","title":"MSONable"},{"location":"concepts/#store","text":"Stores are document-based data sources and data sinks. They are modeled around the MongoDB collection although they can represent more complex data sources as well. Stores implement methods to connect , query , find distinct values, groupby fields, update documents, and remove documents. Stores also implement a number of critical fields for Maggma: the key and the last_updated_field . key is the field that is used to index the underlying data source. last_updated_field is the timestamp of when that document.","title":"Store"},{"location":"concepts/#builder","text":"Builders represent a data transformation step. Builders break down each transformation into 3 key steps: get_items , process_item , and update_targets . Both get_items and update_targets can perform IO to the data stores. process_item is expected to not perform any IO so that it can be parallelized by Maggma. Builders can be chained together into a array and then saved as a JSON file to be run on a production system.","title":"Builder"},{"location":"getting_started/advanced_builder/","text":"Advanced Builder Concepts There are a number of features in maggma designed to assist with advanced features: Logging maggma builders have a python logger object that is already setup to output to the correct level. You can directly use it to output info , debug , and error messages. def get_items ( self ) -> Iterable : ... self . logger . info ( f \"Got {len(to_process_ids)} to process\" ) ... Querying for Updated Documents One of the most important features in a builder is incremental building which allows the builder to just process new documents. One of the parameters for a maggma store is the last_updated_field and the last_updated_type which tell maggma how to deal with dates in the source and target documents. This allows us to get the id of any documents that are newer in the target than the newest document in the source: new_ids = self . target . newer_in ( self . source ) Speeding up IO Since maggma is designed around Mongo style data sources and sinks, building indexes or in-memory copies of fields you want to search on is critical to get the fastest possible IO. Since this is very builder and document style dependent, maggma provides a direct interface to ensure_indexes on a Store. A common paradigm is to do this in the beginning of get_items : def ensure_indexes ( self ): self . source . ensure_index ( \"some_search_fields\" ) self . target . ensure_index ( self . target . key ) def get_items ( self ) -> Iterable : self . ensure_indexes () ...","title":"Advanced Builders"},{"location":"getting_started/advanced_builder/#advanced-builder-concepts","text":"There are a number of features in maggma designed to assist with advanced features:","title":"Advanced Builder Concepts"},{"location":"getting_started/advanced_builder/#logging","text":"maggma builders have a python logger object that is already setup to output to the correct level. You can directly use it to output info , debug , and error messages. def get_items ( self ) -> Iterable : ... self . logger . info ( f \"Got {len(to_process_ids)} to process\" ) ...","title":"Logging"},{"location":"getting_started/advanced_builder/#querying-for-updated-documents","text":"One of the most important features in a builder is incremental building which allows the builder to just process new documents. One of the parameters for a maggma store is the last_updated_field and the last_updated_type which tell maggma how to deal with dates in the source and target documents. This allows us to get the id of any documents that are newer in the target than the newest document in the source: new_ids = self . target . newer_in ( self . source )","title":"Querying for Updated Documents"},{"location":"getting_started/advanced_builder/#speeding-up-io","text":"Since maggma is designed around Mongo style data sources and sinks, building indexes or in-memory copies of fields you want to search on is critical to get the fastest possible IO. Since this is very builder and document style dependent, maggma provides a direct interface to ensure_indexes on a Store. A common paradigm is to do this in the beginning of get_items : def ensure_indexes ( self ): self . source . ensure_index ( \"some_search_fields\" ) self . target . ensure_index ( self . target . key ) def get_items ( self ) -> Iterable : self . ensure_indexes () ...","title":"Speeding up IO"},{"location":"getting_started/simple_builder/","text":"Writing a Builder Builder Architecture A Builder is a class that inherits from maggma.core.Builder and implement 3 methods: get_items : This method should return some iterable of items to run through process_items process_item : This method should take a single item, process it, and return the processed item update_targets : This method should take a list of processed items and update the target stores. To make this less abstract, we will write a builder that multiplies the \"a\" sub-document by a pre-configured multiplier . Let's assume we have some source collection in MongoDB with documents that look like this: { \"id\" : 1 , \"a\" : 3 , \"last_updated\" : \"2019-11-3\" } Class definition and __init__ A simple class definition for a Maggma-based builder looks like this: from maggma.core import Builder from maggma.core import Store class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" The __init__ for a builder can have any set of parameters. Generally, you want a source Store and a target Store along with any parameters that configure the builder. Due to the MSONable pattern, any parameters to __init__ have to be stored as attributes. A simple __init__ would look like this: def __init__ ( self , source : Store , target : Store , multiplier : int = 2. ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) Python type annotations provide a really nice way of documenting the types we expect and being able to later type check using mypy . We defined the type for source and target as Store since we only care that implements that pattern. How exactly these Store s operate doesn't concern us here. Note that the __init__ arguments: source , target , multiplier , and kwargs get saved as attributess: self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs Finally, we want to call the base Builder 's __init__ to tell it our sources and targets for this builder. In addition, we pass along any extra parameters that might configured the base builder class. super () . __init__ ( sources = source , targets = target , ** kwargs ) Calling the parent class __init__ is a good practice as sub-classing builders is a good way to encapsulate complex logic. get_items get_items is one of the easiest and most difficult methods to implement. All of the IO logic getting data from the sources has to happen here, which requires some planning. get_items shoudl also sort all of the data into induvidual items to process. This simple builder has a very easy get_items : def get_items ( self ) -> Iterator : \"\"\" Gets induvidual documents to multiply \"\"\" return self . source . query () Here, get items just returns the results of query() from the store. It could also have been written as a generator: def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" for doc in self . source . query (): yield doc We could have also returned a list of items: def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" docs = list ( self . source . query ()) process_item process_item just has to do the parallelizable work on each item. Since the item is whatever comes out of get_items , you know exactly what it should be. It may be a single document, a list of documents, a mapping, a set, etc. Our simple process item just has to multiply one field by self.mulitplier : def process_items ( self , item : Dict ) -> Dict : \"\"\" Multiplies the \"a\" sub-document by self.multiplier \"\"\" new_item = dict ( ** item ) new_item [ \"a\" ] *= self . multiplier return new_item update_targets Finally, we have to put the processed item in to the target store: def update_targets ( self , items : List [ Dict ]): \"\"\" Adds the processed items into the target store \"\"\" self . target . update ( items )","title":"Writing a Builder"},{"location":"getting_started/simple_builder/#writing-a-builder","text":"","title":"Writing a Builder"},{"location":"getting_started/simple_builder/#builder-architecture","text":"A Builder is a class that inherits from maggma.core.Builder and implement 3 methods: get_items : This method should return some iterable of items to run through process_items process_item : This method should take a single item, process it, and return the processed item update_targets : This method should take a list of processed items and update the target stores. To make this less abstract, we will write a builder that multiplies the \"a\" sub-document by a pre-configured multiplier . Let's assume we have some source collection in MongoDB with documents that look like this: { \"id\" : 1 , \"a\" : 3 , \"last_updated\" : \"2019-11-3\" }","title":"Builder Architecture"},{"location":"getting_started/simple_builder/#class-definition-and-__init__","text":"A simple class definition for a Maggma-based builder looks like this: from maggma.core import Builder from maggma.core import Store class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" The __init__ for a builder can have any set of parameters. Generally, you want a source Store and a target Store along with any parameters that configure the builder. Due to the MSONable pattern, any parameters to __init__ have to be stored as attributes. A simple __init__ would look like this: def __init__ ( self , source : Store , target : Store , multiplier : int = 2. ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) Python type annotations provide a really nice way of documenting the types we expect and being able to later type check using mypy . We defined the type for source and target as Store since we only care that implements that pattern. How exactly these Store s operate doesn't concern us here. Note that the __init__ arguments: source , target , multiplier , and kwargs get saved as attributess: self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs Finally, we want to call the base Builder 's __init__ to tell it our sources and targets for this builder. In addition, we pass along any extra parameters that might configured the base builder class. super () . __init__ ( sources = source , targets = target , ** kwargs ) Calling the parent class __init__ is a good practice as sub-classing builders is a good way to encapsulate complex logic.","title":"Class definition and __init__"},{"location":"getting_started/simple_builder/#get_items","text":"get_items is one of the easiest and most difficult methods to implement. All of the IO logic getting data from the sources has to happen here, which requires some planning. get_items shoudl also sort all of the data into induvidual items to process. This simple builder has a very easy get_items : def get_items ( self ) -> Iterator : \"\"\" Gets induvidual documents to multiply \"\"\" return self . source . query () Here, get items just returns the results of query() from the store. It could also have been written as a generator: def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" for doc in self . source . query (): yield doc We could have also returned a list of items: def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" docs = list ( self . source . query ())","title":"get_items"},{"location":"getting_started/simple_builder/#process_item","text":"process_item just has to do the parallelizable work on each item. Since the item is whatever comes out of get_items , you know exactly what it should be. It may be a single document, a list of documents, a mapping, a set, etc. Our simple process item just has to multiply one field by self.mulitplier : def process_items ( self , item : Dict ) -> Dict : \"\"\" Multiplies the \"a\" sub-document by self.multiplier \"\"\" new_item = dict ( ** item ) new_item [ \"a\" ] *= self . multiplier return new_item","title":"process_item"},{"location":"getting_started/simple_builder/#update_targets","text":"Finally, we have to put the processed item in to the target store: def update_targets ( self , items : List [ Dict ]): \"\"\" Adds the processed items into the target store \"\"\" self . target . update ( items )","title":"update_targets"}]}
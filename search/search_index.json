{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Maggma \u00b6 What is Maggma \u00b6 Maggma is a framework to build data pipelines from files on disk all the way to a REST API in scientific environments. Maggma has been developed by the Materials Project (MP) team at Lawrence Berkeley Labs. Maggma is written in Python and supports Python 3.+. Installation from PyPI \u00b6 Maggma is published on the Python Package Index . The preferred tool for installing packages from PyPi is pip . This tool is provided with all modern versions of Python. Open your terminal and run the following command. pip install --upgrade maggma Installation from source \u00b6 You can install Maggma directly from a clone of the Git repository . This can be done either by cloning the repo and installing from the local clone, or simply installing directly via git . Local Clone git clone https://github.com//materialsproject/maggma cd maggma python setup.py install Direct Git pip install git+https://github.com/materialsproject/maggma","title":"Home"},{"location":"#maggma","text":"","title":"Maggma"},{"location":"#what-is-maggma","text":"Maggma is a framework to build data pipelines from files on disk all the way to a REST API in scientific environments. Maggma has been developed by the Materials Project (MP) team at Lawrence Berkeley Labs. Maggma is written in Python and supports Python 3.+.","title":"What is Maggma"},{"location":"#installation-from-pypi","text":"Maggma is published on the Python Package Index . The preferred tool for installing packages from PyPi is pip . This tool is provided with all modern versions of Python. Open your terminal and run the following command. pip install --upgrade maggma","title":"Installation from PyPI"},{"location":"#installation-from-source","text":"You can install Maggma directly from a clone of the Git repository . This can be done either by cloning the repo and installing from the local clone, or simply installing directly via git . Local Clone git clone https://github.com//materialsproject/maggma cd maggma python setup.py install Direct Git pip install git+https://github.com/materialsproject/maggma","title":"Installation from source"},{"location":"concepts/","text":"Concepts \u00b6 MSONable \u00b6 One challenge in building complex data-transformation codes is keeping track of all the settings necessary to make some output database. One bad solution is to hard-code these settings, but then any modification is difficult to keep track of. Maggma solves this by putting the configuration with the pipeline definition in JSON or YAML files. This is done using the MSONable pattern, which requires that any Maggma object (the databases and transformation steps) can convert itself to a python dictionary with it's configuration parameters in a process called serialization. These dictionaries can then be converted back to the origianl Maggma object without having to know what class it belonged. MSONable does this by injecting in @class and @module keys that tell it where to find the original python code for that Maggma object. Store \u00b6 Another challenge is dealing with all the different types of databases out there. Maggma was originally built off MongoDB, so it's interface looks a lot like PyMongo . Still, there are a number of usefull new object databases that can be used to store large quantities of data you don't need to search in such as Amazon S3 and Google Cloud. It would be nice to have a single interface to all of these so you could write your datapipeline only once. Stores are databases containing organized document-based data. They represent either a data source or a data sink. They are modeled around the MongoDB collection although they can represent more complex data sources that auto-alias keys without the user knowing, or even providing concatenation or joining of Stores. Stores implement methods to connect , query , find distinct values, groupby fields, update documents, and remove documents. Stores also implement a number of critical fields for Maggma that help in efficient document processing: the key and the last_updated_field . key is the field that is used to uniquely index the underlying data source. last_updated_field is the timestamp of when that document was last modified. Builder \u00b6 Builders represent a data processing step. Builders break down each transformation into 3 phases: get_items , process_item , and update_targets : get_items : Retrieve items from the source Store(s) for processing by the next phase process_item : Manipulate the input item and create an output document that is sent to the next phase for storage. update_target : Add the processed item to the target Store(s). Both get_items and update_targets can perform IO (input/output) to the data stores. process_item is expected to not perform any IO so that it can be parallelized by Maggma. Builders can be chained together into an array and then saved as a JSON file to be run on a production system.","title":"Core Concepts"},{"location":"concepts/#concepts","text":"","title":"Concepts"},{"location":"concepts/#msonable","text":"One challenge in building complex data-transformation codes is keeping track of all the settings necessary to make some output database. One bad solution is to hard-code these settings, but then any modification is difficult to keep track of. Maggma solves this by putting the configuration with the pipeline definition in JSON or YAML files. This is done using the MSONable pattern, which requires that any Maggma object (the databases and transformation steps) can convert itself to a python dictionary with it's configuration parameters in a process called serialization. These dictionaries can then be converted back to the origianl Maggma object without having to know what class it belonged. MSONable does this by injecting in @class and @module keys that tell it where to find the original python code for that Maggma object.","title":"MSONable"},{"location":"concepts/#store","text":"Another challenge is dealing with all the different types of databases out there. Maggma was originally built off MongoDB, so it's interface looks a lot like PyMongo . Still, there are a number of usefull new object databases that can be used to store large quantities of data you don't need to search in such as Amazon S3 and Google Cloud. It would be nice to have a single interface to all of these so you could write your datapipeline only once. Stores are databases containing organized document-based data. They represent either a data source or a data sink. They are modeled around the MongoDB collection although they can represent more complex data sources that auto-alias keys without the user knowing, or even providing concatenation or joining of Stores. Stores implement methods to connect , query , find distinct values, groupby fields, update documents, and remove documents. Stores also implement a number of critical fields for Maggma that help in efficient document processing: the key and the last_updated_field . key is the field that is used to uniquely index the underlying data source. last_updated_field is the timestamp of when that document was last modified.","title":"Store"},{"location":"concepts/#builder","text":"Builders represent a data processing step. Builders break down each transformation into 3 phases: get_items , process_item , and update_targets : get_items : Retrieve items from the source Store(s) for processing by the next phase process_item : Manipulate the input item and create an output document that is sent to the next phase for storage. update_target : Add the processed item to the target Store(s). Both get_items and update_targets can perform IO (input/output) to the data stores. process_item is expected to not perform any IO so that it can be parallelized by Maggma. Builders can be chained together into an array and then saved as a JSON file to be run on a production system.","title":"Builder"},{"location":"getting_started/advanced_builder/","text":"Advanced Builder Concepts \u00b6 There are a number of features in maggma designed to assist with advanced features: Logging \u00b6 maggma builders have a python logger object that is already setup to output to the correct level. You can directly use it to output info , debug , and error messages. def get_items ( self ) -> Iterable : ... self . logger . info ( f \"Got { len ( to_process_ids ) } to process\" ) ... Querying for Updated Documents \u00b6 One of the most important features in a builder is incremental building which allows the builder to just process new documents. One of the parameters for a maggma store is the last_updated_field and the last_updated_type which tell maggma how to deal with dates in the source and target documents. This allows us to get the id of any documents that are newer in the target than the newest document in the source: new_ids = self . target . newer_in ( self . source ) Speeding up Data Transfers \u00b6 Since maggma is designed around Mongo style data sources and sinks, building indexes or in-memory copies of fields you want to search on is critical to get the fastest possible data input/output (IO). Since this is very builder and document style dependent, maggma provides a direct interface to ensure_indexes on a Store. A common paradigm is to do this in the beginning of get_items : def ensure_indexes ( self ): self . source . ensure_index ( \"some_search_fields\" ) self . target . ensure_index ( self . target . key ) def get_items ( self ) -> Iterable : self . ensure_indexes () ... Built in Templates for Advanced Builders \u00b6 maggma implements templates for builders that have many of these advanced features listed above: MapBuilder Creates one-to-one document mapping of items in the source Store to the transformed documents in the target Store. GroupBuilder Creates many-to-one document mapping of items in the source Store to transformed documents in the traget Store","title":"Advanced Builders"},{"location":"getting_started/advanced_builder/#advanced-builder-concepts","text":"There are a number of features in maggma designed to assist with advanced features:","title":"Advanced Builder Concepts"},{"location":"getting_started/advanced_builder/#logging","text":"maggma builders have a python logger object that is already setup to output to the correct level. You can directly use it to output info , debug , and error messages. def get_items ( self ) -> Iterable : ... self . logger . info ( f \"Got { len ( to_process_ids ) } to process\" ) ...","title":"Logging"},{"location":"getting_started/advanced_builder/#querying-for-updated-documents","text":"One of the most important features in a builder is incremental building which allows the builder to just process new documents. One of the parameters for a maggma store is the last_updated_field and the last_updated_type which tell maggma how to deal with dates in the source and target documents. This allows us to get the id of any documents that are newer in the target than the newest document in the source: new_ids = self . target . newer_in ( self . source )","title":"Querying for Updated Documents"},{"location":"getting_started/advanced_builder/#speeding-up-data-transfers","text":"Since maggma is designed around Mongo style data sources and sinks, building indexes or in-memory copies of fields you want to search on is critical to get the fastest possible data input/output (IO). Since this is very builder and document style dependent, maggma provides a direct interface to ensure_indexes on a Store. A common paradigm is to do this in the beginning of get_items : def ensure_indexes ( self ): self . source . ensure_index ( \"some_search_fields\" ) self . target . ensure_index ( self . target . key ) def get_items ( self ) -> Iterable : self . ensure_indexes () ...","title":"Speeding up Data Transfers"},{"location":"getting_started/advanced_builder/#built-in-templates-for-advanced-builders","text":"maggma implements templates for builders that have many of these advanced features listed above: MapBuilder Creates one-to-one document mapping of items in the source Store to the transformed documents in the target Store. GroupBuilder Creates many-to-one document mapping of items in the source Store to transformed documents in the traget Store","title":"Built in Templates for Advanced Builders"},{"location":"getting_started/group_builder/","text":"Group Builder \u00b6 Another advanced template in maggma is the GroupBuilder , which groups documents together before applying your function on the group of items. Just like MapBuilder , GroupBuilder also handles incremental building, keeping track of errors, getting only the data you need, and managing timeouts. GroupBuilder won't delete orphaned documents since that reverse relationshop isn't valid. Let's create a simple ResupplyBuilder , which will look at the inventory of items and determine what items need resupply. The source document will look something like this: { \"name\" : \"Banana\" , \"type\" : \"fruit\" , \"quantity\" : 20 , \"minimum\" : 10 , \"last_updated\" : \"2019-11-3T19:09:45\" } Our builder should give us documents that look like this: { \"names\" : [ \"Grapes\" , \"Apples\" , \"Bananas\" ], \"type\" : \"fruit\" , \"resupply\" : { \"Apples\" : 10 , \"Bananes\" : 0 , \"Grapes\" : 5 }, \"last_updated\" : \"2019-11-3T19:09:45\" } To begin, we define our GroupBuilder : from maggma.builders import GroupBuilder from maggma.core import Store class ResupplyBuilder ( GroupBuilder ): \"\"\" Simple builder that determines which items to resupply \"\"\" def __init__ ( inventory : Store , resupply : Store , resupply_percent : int = 100 , ** kwargs ): \"\"\" Arguments: inventory: current inventory information resupply: target resupply information resupply_percent: the percent of the minimum to include in the resupply \"\"\" self . inventory = inventory self . resupply = resupply self . resupply_percent = resupply_percent self . kwargs = kwargs super () . __init__ ( source = inventory , target = resupply , grouping_properties = [ \"type\" ], ** kwargs ) Note that unlike the previous MapBuilder example, we didn't call the source and target stores as such. Providing more usefull names is a good idea in writing builders to make it clearer what the underlying data should look like. GroupBuilder inherits from MapBuilder so it has the same configurational parameters. projection: list of the fields you want to project. This can reduce the data transfer load if you only need certain fields or sub-documents from the source documents timeout: optional timeout on the process function store_process_timeout: adds the process time into the target document for profiling retry_failed: retries running the process function on previously failed documents One parameter that doens't work in GroupBuilder is delete_orphans , since the Many-to-One relationshop makes determining orphaned documents very difficult. Finally let's get to the hard part which is running our function. We do this by defining unary_function def unary_function ( self , items : List [ Dict ]) -> Dict : resupply = {} for item in items : if item [ \"quantity\" ] > item [ \"minimum\" ]: resupply [ item [ \"name\" ]] = int ( item [ \"minimum\" ] * self . resupply_percent ) else : resupply [ item [ \"name\" ]] = 0 return { \"resupply\" : resupply } Just as in MapBuilder , we're not returning all the extra information typically kept in the originally item. Normally, we would have to write code that copies over the source key and convert it to the target key . Same goes for the last_updated_field . GroupBuilder takes care of this, while also recording errors, processing time, and the Builder version. GroupBuilder also keeps a plural version of the source.key field, so in this example, all the name values wil be put together and kept in names","title":"Working with GroupBuilder"},{"location":"getting_started/group_builder/#group-builder","text":"Another advanced template in maggma is the GroupBuilder , which groups documents together before applying your function on the group of items. Just like MapBuilder , GroupBuilder also handles incremental building, keeping track of errors, getting only the data you need, and managing timeouts. GroupBuilder won't delete orphaned documents since that reverse relationshop isn't valid. Let's create a simple ResupplyBuilder , which will look at the inventory of items and determine what items need resupply. The source document will look something like this: { \"name\" : \"Banana\" , \"type\" : \"fruit\" , \"quantity\" : 20 , \"minimum\" : 10 , \"last_updated\" : \"2019-11-3T19:09:45\" } Our builder should give us documents that look like this: { \"names\" : [ \"Grapes\" , \"Apples\" , \"Bananas\" ], \"type\" : \"fruit\" , \"resupply\" : { \"Apples\" : 10 , \"Bananes\" : 0 , \"Grapes\" : 5 }, \"last_updated\" : \"2019-11-3T19:09:45\" } To begin, we define our GroupBuilder : from maggma.builders import GroupBuilder from maggma.core import Store class ResupplyBuilder ( GroupBuilder ): \"\"\" Simple builder that determines which items to resupply \"\"\" def __init__ ( inventory : Store , resupply : Store , resupply_percent : int = 100 , ** kwargs ): \"\"\" Arguments: inventory: current inventory information resupply: target resupply information resupply_percent: the percent of the minimum to include in the resupply \"\"\" self . inventory = inventory self . resupply = resupply self . resupply_percent = resupply_percent self . kwargs = kwargs super () . __init__ ( source = inventory , target = resupply , grouping_properties = [ \"type\" ], ** kwargs ) Note that unlike the previous MapBuilder example, we didn't call the source and target stores as such. Providing more usefull names is a good idea in writing builders to make it clearer what the underlying data should look like. GroupBuilder inherits from MapBuilder so it has the same configurational parameters. projection: list of the fields you want to project. This can reduce the data transfer load if you only need certain fields or sub-documents from the source documents timeout: optional timeout on the process function store_process_timeout: adds the process time into the target document for profiling retry_failed: retries running the process function on previously failed documents One parameter that doens't work in GroupBuilder is delete_orphans , since the Many-to-One relationshop makes determining orphaned documents very difficult. Finally let's get to the hard part which is running our function. We do this by defining unary_function def unary_function ( self , items : List [ Dict ]) -> Dict : resupply = {} for item in items : if item [ \"quantity\" ] > item [ \"minimum\" ]: resupply [ item [ \"name\" ]] = int ( item [ \"minimum\" ] * self . resupply_percent ) else : resupply [ item [ \"name\" ]] = 0 return { \"resupply\" : resupply } Just as in MapBuilder , we're not returning all the extra information typically kept in the originally item. Normally, we would have to write code that copies over the source key and convert it to the target key . Same goes for the last_updated_field . GroupBuilder takes care of this, while also recording errors, processing time, and the Builder version. GroupBuilder also keeps a plural version of the source.key field, so in this example, all the name values wil be put together and kept in names","title":"Group Builder"},{"location":"getting_started/map_builder/","text":"Map Builder \u00b6 maggma has a built in builder called the MapBuilder which handles a number of tedious tasks in writing a builder. This class is designed to be used similar to a map operator in any other framework in even the map function in python. MapBuilder will take each document in the source store, apply the function you give it, and then store that in the target store. It handles incremental building, keeping track of errors, getting only the data you need, managing timeouts, and deleting orphaned documents through configurational options. Let's create the same MultiplierBuilder we wrote earlier using MapBuilder : from maggma.builders import MapBuilder from maggma.core import Store class MultiplyBuilder ( MapBuilder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" Just like before we define a new class, but this time it should inherit from MapBuilder . def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs kwargs = { k , v in kwargs . items () if k not in [ \"projection\" , \"delete_orphans\" , \"timeout\" , \"store_process_time\" , \"retry_failed\" ]} super () . __init__ ( source = source , target = target , projection = [ \"a\" ], delete_orphans = False , timeout = 10 , store_process_time = True , retry_failed = True , ** kwargs ) MapBuilder has a number of configurational options that you can hardcode as above or expose as properties for the user through **kwargs: projection: list of the fields you want to project. This can reduce the data transfer load if you only need certain fields or sub-documents from the source documents delete_orphans: this will delete documents in the target which don't have a corresponding document in the source timeout: optional timeout on the process function store_process_timeout: adds the process time into the target document for profiling retry_failed: retries running the process function on previously failed documents Finally let's get to the hard part which is running our function. We do this by defining unary_function def unary_function ( self , item ): return { \"a\" : item [ \"a\" ] * self . multiplier } Note that we're not returning all the extra information typically kept in the originally item. Normally, we would have to write code that copies over the source key and convert it to the target key . Same goes for the last_updated_field . MapBuilder takes care of this, while also recording errors, processing time, and the Builder version.","title":"Working with MapBuilder"},{"location":"getting_started/map_builder/#map-builder","text":"maggma has a built in builder called the MapBuilder which handles a number of tedious tasks in writing a builder. This class is designed to be used similar to a map operator in any other framework in even the map function in python. MapBuilder will take each document in the source store, apply the function you give it, and then store that in the target store. It handles incremental building, keeping track of errors, getting only the data you need, managing timeouts, and deleting orphaned documents through configurational options. Let's create the same MultiplierBuilder we wrote earlier using MapBuilder : from maggma.builders import MapBuilder from maggma.core import Store class MultiplyBuilder ( MapBuilder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" Just like before we define a new class, but this time it should inherit from MapBuilder . def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs kwargs = { k , v in kwargs . items () if k not in [ \"projection\" , \"delete_orphans\" , \"timeout\" , \"store_process_time\" , \"retry_failed\" ]} super () . __init__ ( source = source , target = target , projection = [ \"a\" ], delete_orphans = False , timeout = 10 , store_process_time = True , retry_failed = True , ** kwargs ) MapBuilder has a number of configurational options that you can hardcode as above or expose as properties for the user through **kwargs: projection: list of the fields you want to project. This can reduce the data transfer load if you only need certain fields or sub-documents from the source documents delete_orphans: this will delete documents in the target which don't have a corresponding document in the source timeout: optional timeout on the process function store_process_timeout: adds the process time into the target document for profiling retry_failed: retries running the process function on previously failed documents Finally let's get to the hard part which is running our function. We do this by defining unary_function def unary_function ( self , item ): return { \"a\" : item [ \"a\" ] * self . multiplier } Note that we're not returning all the extra information typically kept in the originally item. Normally, we would have to write code that copies over the source key and convert it to the target key . Same goes for the last_updated_field . MapBuilder takes care of this, while also recording errors, processing time, and the Builder version.","title":"Map Builder"},{"location":"getting_started/running_builders/","text":"Running Builders \u00b6 maggma is designed to run build-pipelines in a production environment. Builders can be run directly in a python environment, but this gives you none of the performance features such as multiprocessing. The base Builder class implements a simple run method that can be used to run that builder: class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" ... my_builder = MultiplyBuilder ( source_store , target_store , multiplier = 3 ) my_builder . run () A better way to run this builder would be to use the mrun command line tool. Since evrything in maggma is MSONable, we can use monty to dump the builders into a JSON file: from monty.serialization import dumpfn dumpfn ( my_builder , \"my_builder.json\" ) Then we can run the builder using mrun : mrun my_builder.json mrun has a number of usefull options: mrun --help Usage: mrun [ OPTIONS ] [ BUILDERS ] ... Options: -v, --verbose Controls logging level per number of v ' s -n, --num-workers INTEGER RANGE Number of worker processes. Defaults to single processing --help Show this message and exit. We can use the -n option to control how many workers run process_items in parallel. Similarly, -v controls the logging verbosity from just WARNINGs to INFO to DEBUG output. The result will be something that looks like this: 2020 -01-08 14 :33:17,187 - Builder - INFO - Starting Builder Builder 2020 -01-08 14 :33:17,217 - Builder - INFO - Processing 100 items Get: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 15366 .00it/s ] 2020 -01-08 14 :33:17,235 - MultiProcessor - INFO - Processing batch of 1000 items Update Targets: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 584 .51it/s ] Process Items: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 567 .39it/s ] There are progress bars for each of the three steps, which lets you understand what the slowest step is and the overall progress of the system. Running Distributed \u00b6 maggma can distribute work across multiple computers. There are two steps to this: Run a mrun master by providing it with a --url to listen for workers on and --num-chunks ( -N ) which tells mrun how many sub-pieces to break up the work into. You can can run fewer workers then chunks. This will cause mrun to call the builder's prechunk to get the distribution of work and run distributd work on all workers Run mrun workers b y providing it with a --url to listen for a master and --num-workers ( -n ) to tell it how many processes to run in this worker. The url argument takes a fully qualified url including protocol. tcp is recommended: Example: tcp://127.0.0.1:8080 Reporting Build State \u00b6 mrun has the ability to report the status of the build pipeline to a user-provided Store . To do this, you first have to save the Store as a JSON or YAML file. Then you can use the -r option to give this to mrun . It will then periodicially add documents to the Store for one of 3 different events: BUILD_STARTED - This event tells us that a new builder started, the names of the sources and targets as well as the total number of items the builder expects to process UPDATE - This event tells us that a batch of items was processed and is going to update_targets . The number of items is stored in items . BUILD_ENDED - This event tells us the build process finished this specific builder. It also indicates the total number of errors and warnings that were caught during the process. These event docs also contain the builder , a build_id which is unique for each time a builder is run and anonymous but unique ID for the machine the builder was run on.","title":"Running a Builder Pipeline"},{"location":"getting_started/running_builders/#running-builders","text":"maggma is designed to run build-pipelines in a production environment. Builders can be run directly in a python environment, but this gives you none of the performance features such as multiprocessing. The base Builder class implements a simple run method that can be used to run that builder: class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" ... my_builder = MultiplyBuilder ( source_store , target_store , multiplier = 3 ) my_builder . run () A better way to run this builder would be to use the mrun command line tool. Since evrything in maggma is MSONable, we can use monty to dump the builders into a JSON file: from monty.serialization import dumpfn dumpfn ( my_builder , \"my_builder.json\" ) Then we can run the builder using mrun : mrun my_builder.json mrun has a number of usefull options: mrun --help Usage: mrun [ OPTIONS ] [ BUILDERS ] ... Options: -v, --verbose Controls logging level per number of v ' s -n, --num-workers INTEGER RANGE Number of worker processes. Defaults to single processing --help Show this message and exit. We can use the -n option to control how many workers run process_items in parallel. Similarly, -v controls the logging verbosity from just WARNINGs to INFO to DEBUG output. The result will be something that looks like this: 2020 -01-08 14 :33:17,187 - Builder - INFO - Starting Builder Builder 2020 -01-08 14 :33:17,217 - Builder - INFO - Processing 100 items Get: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 15366 .00it/s ] 2020 -01-08 14 :33:17,235 - MultiProcessor - INFO - Processing batch of 1000 items Update Targets: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 584 .51it/s ] Process Items: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100 /100 [ 00 :00< 00 :00, 567 .39it/s ] There are progress bars for each of the three steps, which lets you understand what the slowest step is and the overall progress of the system.","title":"Running Builders"},{"location":"getting_started/running_builders/#running-distributed","text":"maggma can distribute work across multiple computers. There are two steps to this: Run a mrun master by providing it with a --url to listen for workers on and --num-chunks ( -N ) which tells mrun how many sub-pieces to break up the work into. You can can run fewer workers then chunks. This will cause mrun to call the builder's prechunk to get the distribution of work and run distributd work on all workers Run mrun workers b y providing it with a --url to listen for a master and --num-workers ( -n ) to tell it how many processes to run in this worker. The url argument takes a fully qualified url including protocol. tcp is recommended: Example: tcp://127.0.0.1:8080","title":"Running Distributed"},{"location":"getting_started/running_builders/#reporting-build-state","text":"mrun has the ability to report the status of the build pipeline to a user-provided Store . To do this, you first have to save the Store as a JSON or YAML file. Then you can use the -r option to give this to mrun . It will then periodicially add documents to the Store for one of 3 different events: BUILD_STARTED - This event tells us that a new builder started, the names of the sources and targets as well as the total number of items the builder expects to process UPDATE - This event tells us that a batch of items was processed and is going to update_targets . The number of items is stored in items . BUILD_ENDED - This event tells us the build process finished this specific builder. It also indicates the total number of errors and warnings that were caught during the process. These event docs also contain the builder , a build_id which is unique for each time a builder is run and anonymous but unique ID for the machine the builder was run on.","title":"Reporting Build State"},{"location":"getting_started/simple_builder/","text":"Writing a Builder \u00b6 Builder Architecture \u00b6 A Builder is a class that inherits from maggma.core.Builder and implement 3 methods: get_items : This method should return some iterable of items to run through process_items process_item : This method should take a single item, process it, and return the processed item update_targets : This method should take a list of processed items and update the target stores. To make this less abstract, we will write a builder that multiplies the \"a\" sub-document by a pre-configured multiplier . Let's assume we have some source collection in MongoDB with documents that look like this: { \"id\" : 1 , \"a\" : 3 , \"last_updated\" : \"2019-11-3\" } Class definition and __init__ \u00b6 A simple class definition for a Maggma-based builder looks like this: from maggma.core import Builder from maggma.core import Store class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" The __init__ for a builder can have any set of parameters. Generally, you want a source Store and a target Store along with any parameters that configure the builder. Due to the MSONable pattern, any parameters to __init__ have to be stored as attributes. A simple __init__ would look like this: def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) Python type annotations provide a really nice way of documenting the types we expect and being able to later type check using mypy . We defined the type for source and target as Store since we only care that implements that pattern. How exactly these Store s operate doesn't concern us here. Note that the __init__ arguments: source , target , multiplier , and kwargs get saved as attributess: self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs Finally, we want to call the base Builder 's __init__ to tell it our sources and targets for this builder. In addition, we pass along any extra parameters that might configured the base builder class. super () . __init__ ( sources = source , targets = target , ** kwargs ) Calling the parent class __init__ is a good practice as sub-classing builders is a good way to encapsulate complex logic. get_items \u00b6 get_items is conceptually a simple method to implement, but in practice can easily be more code than the rest of the builder. All of the logic for getting data from the sources has to happen here, which requires some planning. get_items should also sort all of the data into induvidual items to process. This simple builder has a very easy get_items : def get_items ( self ) -> Iterator : \"\"\" Gets induvidual documents to multiply \"\"\" return self . source . query () Here, get items just returns the results of query() from the store. It could also have been written as a generator: def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" for doc in self . source . query (): yield doc We could have also returned a list of items: def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" docs = list ( self . source . query ()) One advantage of using the generator approach is it is less memory intensive than the approach where a list of items returned. For large datasets, returning a list of all items for processing may be prohibitive due to memory constraints. process_item \u00b6 process_item just has to do the parallelizable work on each item. Since the item is whatever comes out of get_items , you know exactly what it should be. It may be a single document, a list of documents, a mapping, a set, etc. Our simple process item just has to multiply one field by self.mulitplier : def process_items ( self , item : Dict ) -> Dict : \"\"\" Multiplies the \"a\" sub-document by self.multiplier \"\"\" new_item = dict ( ** item ) new_item [ \"a\" ] *= self . multiplier return new_item update_targets \u00b6 Finally, we have to put the processed item in to the target store: def update_targets ( self , items : List [ Dict ]): \"\"\" Adds the processed items into the target store \"\"\" self . target . update ( items ) Note Note that whatever process_items returns, update_targets takes a List of these: For instance, if process_items returns str , then update_targets would look like: def update_target ( self , items : List [ str ]): Putting it all together we get: from typing import Dict , Iterable , List from maggma.core import Builder from maggma.core import Store class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" docs = list ( self . source . query ()) def process_items ( self , item : Dict ) -> Dict : \"\"\" Multiplies the \"a\" sub-document by self.multiplier \"\"\" new_item = dict ( ** item ) new_item [ \"a\" ] *= self . multiplier return new_item def update_targets ( self , items : List [ Dict ]): \"\"\" Adds the processed items into the target store \"\"\" self . target . update ( items ) Distributed Processing \u00b6 maggma can distribute a builder across multiple computers. The Builder must have a prechunk method defined. prechunk should do a subset of get_items to figure out what needs to be processed and then return dictionaries that modify the Builder in-place to only work on each subset. For example, if in the above example we'd first have to update the builder to be able to work on a subset of keys. One pattern is to define a generic query argument for the builder and use that in get items: def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , query : Optional [ Dict ] = None , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . query = query self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" query = self . query or {} docs = list ( self . source . query ( criteria = query )) Then we can define a prechunk method that modifies the Builder dict in place to operate on just a subset of the keys: from maggma.utils import grouper def prechunk ( self , number_splits : int ) -> Iterable [ Dict ]: keys = self . source . distinct ( self . source . key ) for split in grouper ( keys , N ): yield { \"query\" : { self . source . key : { \"$in\" : list ( split )}} } When distributed processing runs, it will modify the Builder dictionary in place by the prechunk dictionary. In this case, each builder distribute to a worker will get a modified query parameter that only runs on a subset of all posible keys.","title":"Writing a Builder"},{"location":"getting_started/simple_builder/#writing-a-builder","text":"","title":"Writing a Builder"},{"location":"getting_started/simple_builder/#builder-architecture","text":"A Builder is a class that inherits from maggma.core.Builder and implement 3 methods: get_items : This method should return some iterable of items to run through process_items process_item : This method should take a single item, process it, and return the processed item update_targets : This method should take a list of processed items and update the target stores. To make this less abstract, we will write a builder that multiplies the \"a\" sub-document by a pre-configured multiplier . Let's assume we have some source collection in MongoDB with documents that look like this: { \"id\" : 1 , \"a\" : 3 , \"last_updated\" : \"2019-11-3\" }","title":"Builder Architecture"},{"location":"getting_started/simple_builder/#class-definition-and-__init__","text":"A simple class definition for a Maggma-based builder looks like this: from maggma.core import Builder from maggma.core import Store class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" The __init__ for a builder can have any set of parameters. Generally, you want a source Store and a target Store along with any parameters that configure the builder. Due to the MSONable pattern, any parameters to __init__ have to be stored as attributes. A simple __init__ would look like this: def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) Python type annotations provide a really nice way of documenting the types we expect and being able to later type check using mypy . We defined the type for source and target as Store since we only care that implements that pattern. How exactly these Store s operate doesn't concern us here. Note that the __init__ arguments: source , target , multiplier , and kwargs get saved as attributess: self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs Finally, we want to call the base Builder 's __init__ to tell it our sources and targets for this builder. In addition, we pass along any extra parameters that might configured the base builder class. super () . __init__ ( sources = source , targets = target , ** kwargs ) Calling the parent class __init__ is a good practice as sub-classing builders is a good way to encapsulate complex logic.","title":"Class definition and __init__"},{"location":"getting_started/simple_builder/#get_items","text":"get_items is conceptually a simple method to implement, but in practice can easily be more code than the rest of the builder. All of the logic for getting data from the sources has to happen here, which requires some planning. get_items should also sort all of the data into induvidual items to process. This simple builder has a very easy get_items : def get_items ( self ) -> Iterator : \"\"\" Gets induvidual documents to multiply \"\"\" return self . source . query () Here, get items just returns the results of query() from the store. It could also have been written as a generator: def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" for doc in self . source . query (): yield doc We could have also returned a list of items: def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" docs = list ( self . source . query ()) One advantage of using the generator approach is it is less memory intensive than the approach where a list of items returned. For large datasets, returning a list of all items for processing may be prohibitive due to memory constraints.","title":"get_items"},{"location":"getting_started/simple_builder/#process_item","text":"process_item just has to do the parallelizable work on each item. Since the item is whatever comes out of get_items , you know exactly what it should be. It may be a single document, a list of documents, a mapping, a set, etc. Our simple process item just has to multiply one field by self.mulitplier : def process_items ( self , item : Dict ) -> Dict : \"\"\" Multiplies the \"a\" sub-document by self.multiplier \"\"\" new_item = dict ( ** item ) new_item [ \"a\" ] *= self . multiplier return new_item","title":"process_item"},{"location":"getting_started/simple_builder/#update_targets","text":"Finally, we have to put the processed item in to the target store: def update_targets ( self , items : List [ Dict ]): \"\"\" Adds the processed items into the target store \"\"\" self . target . update ( items ) Note Note that whatever process_items returns, update_targets takes a List of these: For instance, if process_items returns str , then update_targets would look like: def update_target ( self , items : List [ str ]): Putting it all together we get: from typing import Dict , Iterable , List from maggma.core import Builder from maggma.core import Store class MultiplyBuilder ( Builder ): \"\"\" Simple builder that multiplies the \"a\" sub-document by pre-set value \"\"\" def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" docs = list ( self . source . query ()) def process_items ( self , item : Dict ) -> Dict : \"\"\" Multiplies the \"a\" sub-document by self.multiplier \"\"\" new_item = dict ( ** item ) new_item [ \"a\" ] *= self . multiplier return new_item def update_targets ( self , items : List [ Dict ]): \"\"\" Adds the processed items into the target store \"\"\" self . target . update ( items )","title":"update_targets"},{"location":"getting_started/simple_builder/#distributed-processing","text":"maggma can distribute a builder across multiple computers. The Builder must have a prechunk method defined. prechunk should do a subset of get_items to figure out what needs to be processed and then return dictionaries that modify the Builder in-place to only work on each subset. For example, if in the above example we'd first have to update the builder to be able to work on a subset of keys. One pattern is to define a generic query argument for the builder and use that in get items: def __init__ ( self , source : Store , target : Store , multiplier : int = 2 , query : Optional [ Dict ] = None , ** kwargs ): \"\"\" Arguments: source: the source store target: the target store multiplier: the multiplier to apply to \"a\" sub-document \"\"\" self . source = source self . target = target self . multiplier = multiplier self . query = query self . kwargs = kwargs super () . __init__ ( sources = source , targets = target , ** kwargs ) def get_items ( self ) -> Iterable : \"\"\" Gets induvidual documents to multiply \"\"\" query = self . query or {} docs = list ( self . source . query ( criteria = query )) Then we can define a prechunk method that modifies the Builder dict in place to operate on just a subset of the keys: from maggma.utils import grouper def prechunk ( self , number_splits : int ) -> Iterable [ Dict ]: keys = self . source . distinct ( self . source . key ) for split in grouper ( keys , N ): yield { \"query\" : { self . source . key : { \"$in\" : list ( split )}} } When distributed processing runs, it will modify the Builder dictionary in place by the prechunk dictionary. In this case, each builder distribute to a worker will get a modified query parameter that only runs on a subset of all posible keys.","title":"Distributed Processing"},{"location":"getting_started/stores/","text":"Using Store \u00b6 A Store is just a wrapper to access data from somewhere. That somewhere is typically a MongoDB collection, but it could also be GridFS which lets you keep large binary objects. maggma makes GridFS and MongoDB collections feel the same. Beyond that it adds in something that looks like GridFS but is actually using AWS S3 as the storage space. Finally, Store can actually perform logic, concatenating two or more Stores together to make them look like one data source for instance. This means you only have to write a Builder for one scenario of how to transform data and the choice of Store lets you control where the data comes from and goes. List of Stores \u00b6 Current working and tested Stores include: MongoStore: interfaces to a MongoDB Collection MongoURIStore: MongoDB Introduced advanced URIs including their special \"mongodb+srv://\" which uses a combination of SRV and TXT DNS records to fully setup the client. This store is to safely handle these kinds of URIs. MemoryStore: just a Store that exists temporarily in memory JSONStore: buids a MemoryStore and then populates it with the contents of the given JSON files GridFSStore: interfaces to GridFS collection in MongoDB MongograntStore: uses Mongogrant to get credentials for MongoDB database VaulStore: uses Vault to get credentials for a MongoDB database AliasingStore: aliases keys from the underlying store to new names SandboxStore: provides permission control to documents via a _sbxn sandbox key S3Store: provides an interface to an S3 Bucket either on AWS or self-hosted solutions JointStore: joins several MongoDB collections together, merging documents with the same key , so they look like one collection ConcatStore: concatenates several MongoDB collections in series so they look like one collection The Store interface \u00b6 Initializing a Store \u00b6 All Store s have a few basic arguments that are critical to understand. Every Store has two attributes that the user should customize based on the data contained in that store: key and last_updated_field . The key defines how the Store tells documents part. Typically this is _id in MongoDB, but you could use your own field (be sure all values under the key field can be used to uniquely identify documents). last_updated_field tells Store how to order the documents by a date, which is typically in the datetime format, but can also be a ISO 8601-format (ex: 2009-05-28T16:15:00 ) Store s can also take a Validator object to make sure the data going into obeys some schema. Using a Store \u00b6 Stores provide a number of basic methods that make easy to use: query: Standard mongo style find method that lets you search the store. query_one: Same as above but limits returned results to just the first document that matches your query. update: Update the documents into the collection. This will override documents if the key field matches. ensure_index: This creates an index the underlying data-source for fast querying. distinct: Gets distinct values of a field. groupby: Similar to query but performs a grouping operation and returns sets of documents. remove_docs: Removes documents from the underlying data source. last_updated: Finds the most recently updated last_updated_field value and returns that. Usefull for knowing how old a data-source is. newer_in: Finds all documents that are newer in the target collection and returns their key s. This is a very useful way of performing incremental processing.","title":"Using Stores"},{"location":"getting_started/stores/#using-store","text":"A Store is just a wrapper to access data from somewhere. That somewhere is typically a MongoDB collection, but it could also be GridFS which lets you keep large binary objects. maggma makes GridFS and MongoDB collections feel the same. Beyond that it adds in something that looks like GridFS but is actually using AWS S3 as the storage space. Finally, Store can actually perform logic, concatenating two or more Stores together to make them look like one data source for instance. This means you only have to write a Builder for one scenario of how to transform data and the choice of Store lets you control where the data comes from and goes.","title":"Using Store"},{"location":"getting_started/stores/#list-of-stores","text":"Current working and tested Stores include: MongoStore: interfaces to a MongoDB Collection MongoURIStore: MongoDB Introduced advanced URIs including their special \"mongodb+srv://\" which uses a combination of SRV and TXT DNS records to fully setup the client. This store is to safely handle these kinds of URIs. MemoryStore: just a Store that exists temporarily in memory JSONStore: buids a MemoryStore and then populates it with the contents of the given JSON files GridFSStore: interfaces to GridFS collection in MongoDB MongograntStore: uses Mongogrant to get credentials for MongoDB database VaulStore: uses Vault to get credentials for a MongoDB database AliasingStore: aliases keys from the underlying store to new names SandboxStore: provides permission control to documents via a _sbxn sandbox key S3Store: provides an interface to an S3 Bucket either on AWS or self-hosted solutions JointStore: joins several MongoDB collections together, merging documents with the same key , so they look like one collection ConcatStore: concatenates several MongoDB collections in series so they look like one collection","title":"List of Stores"},{"location":"getting_started/stores/#the-store-interface","text":"","title":"The Store interface"},{"location":"getting_started/stores/#initializing-a-store","text":"All Store s have a few basic arguments that are critical to understand. Every Store has two attributes that the user should customize based on the data contained in that store: key and last_updated_field . The key defines how the Store tells documents part. Typically this is _id in MongoDB, but you could use your own field (be sure all values under the key field can be used to uniquely identify documents). last_updated_field tells Store how to order the documents by a date, which is typically in the datetime format, but can also be a ISO 8601-format (ex: 2009-05-28T16:15:00 ) Store s can also take a Validator object to make sure the data going into obeys some schema.","title":"Initializing a Store"},{"location":"getting_started/stores/#using-a-store","text":"Stores provide a number of basic methods that make easy to use: query: Standard mongo style find method that lets you search the store. query_one: Same as above but limits returned results to just the first document that matches your query. update: Update the documents into the collection. This will override documents if the key field matches. ensure_index: This creates an index the underlying data-source for fast querying. distinct: Gets distinct values of a field. groupby: Similar to query but performs a grouping operation and returns sets of documents. remove_docs: Removes documents from the underlying data source. last_updated: Finds the most recently updated last_updated_field value and returns that. Usefull for knowing how old a data-source is. newer_in: Finds all documents that are newer in the target collection and returns their key s. This is a very useful way of performing incremental processing.","title":"Using a Store"},{"location":"reference/builders/","text":"One-to-One Map Builder and a simple CopyBuilder implementation CopyBuilder \u00b6 Sync a source store with a target store. unary_function ( self , item ) \u00b6 Show source code in builders/map_builder.py 220 221 222 223 224 225 226 def unary_function ( self , item ): \"\"\" Identity function for copy builder map operation \"\"\" if \"_id\" in item : del item [ \"_id\" ] return item Identity function for copy builder map operation MapBuilder \u00b6 Apply a unary function to yield a target document for each source document. Supports incremental building, where a source document gets built only if it has newer (by last_updated_field) data than the corresponding (by key) target document. __init__ ( self , source , target , query = None , projection = None , delete_orphans = False , timeout = 0 , store_process_time = True , retry_failed = False , ** kwargs ) \u00b6 Show source code in builders/map_builder.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , source : Store , target : Store , query : Optional [ Dict ] = None , projection : Optional [ List ] = None , delete_orphans : bool = False , timeout : int = 0 , store_process_time : bool = True , retry_failed : bool = False , ** kwargs , ): \"\"\" Apply a unary function to each source document. Args: source: source store target: target store query: optional query to filter source store projection: list of keys to project from the source for processing. Limits data transfer to improve efficiency. delete_orphans: Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. timeout: maximum running time per item in seconds store_process_time: If True, add \"_process_time\" key to document for profiling purposes retry_failed: If True, will retry building documents that previously failed \"\"\" self . source = source self . target = target self . query = query self . projection = projection self . delete_orphans = delete_orphans self . kwargs = kwargs self . timeout = timeout self . store_process_time = store_process_time self . retry_failed = retry_failed super () . __init__ ( sources = [ source ], targets = [ target ], ** kwargs ) Apply a unary function to each source document. Parameters Name Type Description Default source Store source store required target Store target store required query Optional[Dict] optional query to filter source store None projection Optional[List] list of keys to project from the source for processing. Limits data transfer to improve efficiency. None delete_orphans bool Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize. False timeout int maximum running time per item in seconds 0 store_process_time bool If True, add \"_process_time\" key to document for profiling purposes True retry_failed bool If True, will retry building documents that previously failed False ensure_indexes ( self ) \u00b6 Show source code in builders/map_builder.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def ensure_indexes ( self ): \"\"\" Ensures indicies on critical fields for MapBuilder \"\"\" index_checks = [ self . source . ensure_index ( self . source . key ), self . source . ensure_index ( self . source . last_updated_field ), self . target . ensure_index ( self . target . key ), self . target . ensure_index ( self . target . last_updated_field ), self . target . ensure_index ( \"state\" ), ] if not all ( index_checks ): self . logger . warning ( \"Missing one or more important indices on stores. \" \"Performance for large stores may be severely degraded. \" \"Ensure indices on target.key and \" \"[(store.last_updated_field, -1), (store.key, 1)] \" \"for each of source and target.\" ) Ensures indicies on critical fields for MapBuilder finalize ( self ) \u00b6 Show source code in builders/map_builder.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def finalize ( self ): \"\"\" Finalize MapBuilder operations including removing orphaned documents \"\"\" if self . delete_orphans : source_keyvals = set ( self . source . distinct ( self . source . key )) target_keyvals = set ( self . target . distinct ( self . target . key )) to_delete = list ( target_keyvals - source_keyvals ) if len ( to_delete ): self . logger . info ( \"Finalize: Deleting {} orphans.\" . format ( len ( to_delete )) ) self . target . remove_docs ({ self . target . key : { \"$in\" : to_delete }}) super () . finalize () Finalize MapBuilder operations including removing orphaned documents get_items ( self ) \u00b6 Show source code in builders/map_builder.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def get_items ( self ): \"\"\" Generic get items for Map Builder designed to perform incremental building \"\"\" self . logger . info ( \"Starting {} Builder\" . format ( self . __class__ . __name__ )) self . ensure_indexes () keys = self . target . newer_in ( self . source , criteria = self . query , exhaustive = True ) if self . retry_failed : failed_keys = self . target . distinct ( self . target . key , criteria = { \"state\" : { \"$ne\" : \"failed\" }} ) keys = list ( set ( keys + failed_keys )) self . logger . info ( \"Processing {} items\" . format ( len ( keys ))) if self . projection : projection = list ( set ( self . projection + [ self . source . key , self . source . last_updated_field ]) ) else : projection = None self . total = len ( keys ) for chunked_keys in grouper ( keys , self . chunk_size ): chunked_keys = list ( chunked_keys ) for doc in list ( self . source . query ( criteria = { self . source . key : { \"$in\" : chunked_keys }}, properties = projection , ) ): yield doc Generic get items for Map Builder designed to perform incremental building prechunk ( self , number_splits ) \u00b6 Show source code in builders/map_builder.py 87 88 89 90 91 92 93 94 95 96 97 def prechunk ( self , number_splits : int ) -> Iterator [ Dict ]: \"\"\" Generic prechunk for map builder to perform domain-decompostion by the key field \"\"\" self . ensure_indexes () keys = self . target . newer_in ( self . source , criteria = self . query , exhaustive = True ) N = ceil ( len ( keys ) / number_splits ) for split in grouper ( keys , N ): yield { \"query\" : { self . source . key : { \"$in\" : list ( split )}}} Generic prechunk for map builder to perform domain-decompostion by the key field process_item ( self , item ) \u00b6 Show source code in builders/map_builder.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def process_item ( self , item : Dict ): \"\"\" Generic process items to process a dictionary using a map function \"\"\" self . logger . debug ( \"Processing: {} \" . format ( item [ self . source . key ])) time_start = time () try : with Timeout ( seconds = self . timeout ): processed = dict ( self . unary_function ( item )) processed . update ({ \"state\" : \"successful\" }) for k in [ self . source . key , self . source . last_updated_field ]: if k in processed : del processed [ k ] except Exception as e : self . logger . error ( traceback . format_exc ()) processed = { \"error\" : str ( e ), \"state\" : \"failed\" } time_end = time () key , last_updated_field = self . source . key , self . source . last_updated_field out = { self . target . key : item [ key ], self . target . last_updated_field : self . source . _lu_func [ 0 ]( item . get ( last_updated_field , datetime . utcnow ()) ), } if self . store_process_time : out [ \"_process_time\" ] = time_end - time_start out . update ( processed ) return out Generic process items to process a dictionary using a map function unary_function ( self , item ) \u00b6 Show source code in builders/map_builder.py 204 205 206 207 208 209 210 211 212 213 214 @abstractmethod def unary_function ( self , item ): \"\"\" ufn: Unary function to process item You do not need to provide values for source.key and source.last_updated_field in the output. Any uncaught exceptions will be caught by process_item and logged to the \"error\" field in the target document. \"\"\" pass ufn: Unary function to process item You do not need to provide values for source.key and source.last_updated_field in the output. Any uncaught exceptions will be caught by process_item and logged to the \"error\" field in the target document. update_targets ( self , items ) \u00b6 Show source code in builders/map_builder.py 176 177 178 179 180 181 182 183 184 185 186 187 def update_targets ( self , items : List [ Dict ]): \"\"\" Generic update targets for Map Builder \"\"\" target = self . target for item in items : item [ \"_bt\" ] = datetime . utcnow () if \"_id\" in item : del item [ \"_id\" ] if len ( items ) > 0 : target . update ( items ) Generic update targets for Map Builder Many-to-Many GroupBuilder GroupBuilder \u00b6 Group source docs and produces merged documents for each group Supports incremental building, where a source group gets (re)built only if it has a newer (by last_updated_field) doc than the corresponding (by key) target doc. This is a Many-to-One Builder. As a result, this builder can't determined when a source document is orphaned. ensure_indexes ( self ) \u00b6 Show source code in builders/group_builder.py 34 35 36 37 38 39 40 41 42 def ensure_indexes ( self ): \"\"\" Ensures indicies on critical fields for GroupBuilder which include the plural version of the target's key field \"\"\" if not self . target . ensure_index ( f \" { self . target . key } s\" ): self . logger . warning () super () . ensure_indexes () Ensures indicies on critical fields for GroupBuilder which include the plural version of the target's key field get_groups_from_keys ( self , keys ) \u00b6 Show source code in builders/group_builder.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def get_groups_from_keys ( self , keys ) -> Set [ Tuple ]: \"\"\" Get the groups by grouping_keys for these documents \"\"\" grouping_keys = self . grouping_keys groups : Set [ Tuple ] = set () for chunked_keys in grouper ( keys , self . chunk_size ): docs = list ( self . source . query ( criteria = { self . source . key : { \"$in\" : chunked_keys }}, properties = grouping_keys , ) ) sub_groups = set ( tuple ( get ( d , prop , None ) for prop in grouping_keys ) for d in docs ) self . logger . debug ( f \"Found { len ( sub_groups ) } subgroups to process\" ) groups |= sub_groups self . logger . info ( f \"Found { len ( groups ) } groups to process\" ) return groups Get the groups by grouping_keys for these documents get_ids_to_process ( self ) \u00b6 Show source code in builders/group_builder.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def get_ids_to_process ( self ) -> Iterable : \"\"\" Gets the IDs that need to be processed \"\"\" query = self . query or {} distinct_from_target = list ( self . target . distinct ( f \" { self . source . key } s\" , criteria = query ) ) processed_ids = [] # Not always gauranteed that MongoDB will unpack the list so we # have to make sure we do that for d in distinct_from_target : if isinstance ( d , list ): processed_ids . extend ( d ) else : processed_ids . append ( d ) all_ids = set ( self . source . distinct ( self . source . key , criteria = query )) unprocessed_ids = all_ids - set ( processed_ids ) self . logger . debug ( f \"Found { len ( all_ids ) } total docs in source\" ) self . logger . info ( f \"Found { len ( unprocessed_ids ) } IDs to process\" ) new_ids = set ( self . source . newer_in ( self . target , criteria = query , exhaustive = False ) ) return list ( new_ids | unprocessed_ids ) Gets the IDs that need to be processed get_items ( self ) \u00b6 Show source code in builders/group_builder.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def get_items ( self ): self . logger . info ( \"Starting {} Builder\" . format ( self . __class__ . __name__ )) self . ensure_indexes () keys = self . get_ids_to_process () groups = self . get_groups_from_keys ( keys ) if self . projection : projection = list ( set ( self . projection + [ self . source . key , self . source . last_updated_field ]) ) else : projection = None self . total = len ( groups ) for group in groups : docs = list ( self . source . query ( criteria = dict ( zip ( self . grouping_keys , group )), properties = projection ) ) yield docs Generic get items for Map Builder designed to perform incremental building prechunk ( self , number_splits ) \u00b6 Show source code in builders/group_builder.py 44 45 46 47 48 49 50 51 52 53 54 55 def prechunk ( self , number_splits : int ) -> Iterator [ Dict ]: \"\"\" Generic prechunk for group builder to perform domain-decompostion by the grouping keys \"\"\" self . ensure_indexes () keys = self . get_ids_to_process () groups = self . get_groups_from_keys ( keys ) for split in grouper ( groups , number_splits ): yield { \"query\" : dict ( zip ( self . grouping_keys , split ))} Generic prechunk for group builder to perform domain-decompostion by the grouping keys process_item ( self , item ) \u00b6 Show source code in builders/group_builder.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def process_item ( self , item : List [ Dict ]) -> Dict [ Tuple , Dict ]: # type: ignore keys = list ( d [ self . source . key ] for d in item ) self . logger . debug ( \"Processing: {} \" . format ( keys )) time_start = time () try : with Timeout ( seconds = self . timeout ): processed = self . unary_function ( item ) processed . update ({ \"state\" : \"successful\" }) except Exception as e : self . logger . error ( traceback . format_exc ()) processed = { \"error\" : str ( e ), \"state\" : \"failed\" } time_end = time () last_updated = [ self . source . _lu_func [ 0 ]( d [ self . source . last_updated_field ]) for d in item ] processed . update ( { self . target . key : keys [ 0 ], f \" { self . source . key } s\" : keys , self . target . last_updated_field : max ( last_updated ), \"_bt\" : datetime . utcnow (), } ) if self . store_process_time : processed [ \"_process_time\" ] = time_end - time_start return processed Generic process items to process a dictionary using a map function unary_function ( self , items ) \u00b6 Show source code in builders/group_builder.py 174 175 176 177 178 179 180 181 182 183 184 185 @abstractmethod def unary_function ( self , items : List [ Dict ]) -> Dict : \"\"\" Processing function for GroupBuilder Returns: Dictionary mapping: tuple of source document keys that are in the grouped document to the grouped and processed document \"\"\" pass Processing function for GroupBuilder Returns Type Description Dict Dictionary mapping: tuple of source document keys that are in the grouped document to the grouped and processed document","title":"Builders"},{"location":"reference/builders/#maggma.builders.map_builder.CopyBuilder","text":"Sync a source store with a target store.","title":"CopyBuilder"},{"location":"reference/builders/#maggma.builders.map_builder.CopyBuilder.unary_function","text":"Show source code in builders/map_builder.py 220 221 222 223 224 225 226 def unary_function ( self , item ): \"\"\" Identity function for copy builder map operation \"\"\" if \"_id\" in item : del item [ \"_id\" ] return item Identity function for copy builder map operation","title":"unary_function()"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder","text":"Apply a unary function to yield a target document for each source document. Supports incremental building, where a source document gets built only if it has newer (by last_updated_field) data than the corresponding (by key) target document.","title":"MapBuilder"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.ensure_indexes","text":"Show source code in builders/map_builder.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def ensure_indexes ( self ): \"\"\" Ensures indicies on critical fields for MapBuilder \"\"\" index_checks = [ self . source . ensure_index ( self . source . key ), self . source . ensure_index ( self . source . last_updated_field ), self . target . ensure_index ( self . target . key ), self . target . ensure_index ( self . target . last_updated_field ), self . target . ensure_index ( \"state\" ), ] if not all ( index_checks ): self . logger . warning ( \"Missing one or more important indices on stores. \" \"Performance for large stores may be severely degraded. \" \"Ensure indices on target.key and \" \"[(store.last_updated_field, -1), (store.key, 1)] \" \"for each of source and target.\" ) Ensures indicies on critical fields for MapBuilder","title":"ensure_indexes()"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.finalize","text":"Show source code in builders/map_builder.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def finalize ( self ): \"\"\" Finalize MapBuilder operations including removing orphaned documents \"\"\" if self . delete_orphans : source_keyvals = set ( self . source . distinct ( self . source . key )) target_keyvals = set ( self . target . distinct ( self . target . key )) to_delete = list ( target_keyvals - source_keyvals ) if len ( to_delete ): self . logger . info ( \"Finalize: Deleting {} orphans.\" . format ( len ( to_delete )) ) self . target . remove_docs ({ self . target . key : { \"$in\" : to_delete }}) super () . finalize () Finalize MapBuilder operations including removing orphaned documents","title":"finalize()"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.get_items","text":"Show source code in builders/map_builder.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def get_items ( self ): \"\"\" Generic get items for Map Builder designed to perform incremental building \"\"\" self . logger . info ( \"Starting {} Builder\" . format ( self . __class__ . __name__ )) self . ensure_indexes () keys = self . target . newer_in ( self . source , criteria = self . query , exhaustive = True ) if self . retry_failed : failed_keys = self . target . distinct ( self . target . key , criteria = { \"state\" : { \"$ne\" : \"failed\" }} ) keys = list ( set ( keys + failed_keys )) self . logger . info ( \"Processing {} items\" . format ( len ( keys ))) if self . projection : projection = list ( set ( self . projection + [ self . source . key , self . source . last_updated_field ]) ) else : projection = None self . total = len ( keys ) for chunked_keys in grouper ( keys , self . chunk_size ): chunked_keys = list ( chunked_keys ) for doc in list ( self . source . query ( criteria = { self . source . key : { \"$in\" : chunked_keys }}, properties = projection , ) ): yield doc Generic get items for Map Builder designed to perform incremental building","title":"get_items()"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.prechunk","text":"Show source code in builders/map_builder.py 87 88 89 90 91 92 93 94 95 96 97 def prechunk ( self , number_splits : int ) -> Iterator [ Dict ]: \"\"\" Generic prechunk for map builder to perform domain-decompostion by the key field \"\"\" self . ensure_indexes () keys = self . target . newer_in ( self . source , criteria = self . query , exhaustive = True ) N = ceil ( len ( keys ) / number_splits ) for split in grouper ( keys , N ): yield { \"query\" : { self . source . key : { \"$in\" : list ( split )}}} Generic prechunk for map builder to perform domain-decompostion by the key field","title":"prechunk()"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.process_item","text":"Show source code in builders/map_builder.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def process_item ( self , item : Dict ): \"\"\" Generic process items to process a dictionary using a map function \"\"\" self . logger . debug ( \"Processing: {} \" . format ( item [ self . source . key ])) time_start = time () try : with Timeout ( seconds = self . timeout ): processed = dict ( self . unary_function ( item )) processed . update ({ \"state\" : \"successful\" }) for k in [ self . source . key , self . source . last_updated_field ]: if k in processed : del processed [ k ] except Exception as e : self . logger . error ( traceback . format_exc ()) processed = { \"error\" : str ( e ), \"state\" : \"failed\" } time_end = time () key , last_updated_field = self . source . key , self . source . last_updated_field out = { self . target . key : item [ key ], self . target . last_updated_field : self . source . _lu_func [ 0 ]( item . get ( last_updated_field , datetime . utcnow ()) ), } if self . store_process_time : out [ \"_process_time\" ] = time_end - time_start out . update ( processed ) return out Generic process items to process a dictionary using a map function","title":"process_item()"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.unary_function","text":"Show source code in builders/map_builder.py 204 205 206 207 208 209 210 211 212 213 214 @abstractmethod def unary_function ( self , item ): \"\"\" ufn: Unary function to process item You do not need to provide values for source.key and source.last_updated_field in the output. Any uncaught exceptions will be caught by process_item and logged to the \"error\" field in the target document. \"\"\" pass ufn: Unary function to process item You do not need to provide values for source.key and source.last_updated_field in the output. Any uncaught exceptions will be caught by process_item and logged to the \"error\" field in the target document.","title":"unary_function()"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.update_targets","text":"Show source code in builders/map_builder.py 176 177 178 179 180 181 182 183 184 185 186 187 def update_targets ( self , items : List [ Dict ]): \"\"\" Generic update targets for Map Builder \"\"\" target = self . target for item in items : item [ \"_bt\" ] = datetime . utcnow () if \"_id\" in item : del item [ \"_id\" ] if len ( items ) > 0 : target . update ( items ) Generic update targets for Map Builder Many-to-Many GroupBuilder","title":"update_targets()"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder","text":"Group source docs and produces merged documents for each group Supports incremental building, where a source group gets (re)built only if it has a newer (by last_updated_field) doc than the corresponding (by key) target doc. This is a Many-to-One Builder. As a result, this builder can't determined when a source document is orphaned.","title":"GroupBuilder"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.ensure_indexes","text":"Show source code in builders/group_builder.py 34 35 36 37 38 39 40 41 42 def ensure_indexes ( self ): \"\"\" Ensures indicies on critical fields for GroupBuilder which include the plural version of the target's key field \"\"\" if not self . target . ensure_index ( f \" { self . target . key } s\" ): self . logger . warning () super () . ensure_indexes () Ensures indicies on critical fields for GroupBuilder which include the plural version of the target's key field","title":"ensure_indexes()"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.get_groups_from_keys","text":"Show source code in builders/group_builder.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def get_groups_from_keys ( self , keys ) -> Set [ Tuple ]: \"\"\" Get the groups by grouping_keys for these documents \"\"\" grouping_keys = self . grouping_keys groups : Set [ Tuple ] = set () for chunked_keys in grouper ( keys , self . chunk_size ): docs = list ( self . source . query ( criteria = { self . source . key : { \"$in\" : chunked_keys }}, properties = grouping_keys , ) ) sub_groups = set ( tuple ( get ( d , prop , None ) for prop in grouping_keys ) for d in docs ) self . logger . debug ( f \"Found { len ( sub_groups ) } subgroups to process\" ) groups |= sub_groups self . logger . info ( f \"Found { len ( groups ) } groups to process\" ) return groups Get the groups by grouping_keys for these documents","title":"get_groups_from_keys()"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.get_ids_to_process","text":"Show source code in builders/group_builder.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def get_ids_to_process ( self ) -> Iterable : \"\"\" Gets the IDs that need to be processed \"\"\" query = self . query or {} distinct_from_target = list ( self . target . distinct ( f \" { self . source . key } s\" , criteria = query ) ) processed_ids = [] # Not always gauranteed that MongoDB will unpack the list so we # have to make sure we do that for d in distinct_from_target : if isinstance ( d , list ): processed_ids . extend ( d ) else : processed_ids . append ( d ) all_ids = set ( self . source . distinct ( self . source . key , criteria = query )) unprocessed_ids = all_ids - set ( processed_ids ) self . logger . debug ( f \"Found { len ( all_ids ) } total docs in source\" ) self . logger . info ( f \"Found { len ( unprocessed_ids ) } IDs to process\" ) new_ids = set ( self . source . newer_in ( self . target , criteria = query , exhaustive = False ) ) return list ( new_ids | unprocessed_ids ) Gets the IDs that need to be processed","title":"get_ids_to_process()"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.get_items","text":"Show source code in builders/group_builder.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def get_items ( self ): self . logger . info ( \"Starting {} Builder\" . format ( self . __class__ . __name__ )) self . ensure_indexes () keys = self . get_ids_to_process () groups = self . get_groups_from_keys ( keys ) if self . projection : projection = list ( set ( self . projection + [ self . source . key , self . source . last_updated_field ]) ) else : projection = None self . total = len ( groups ) for group in groups : docs = list ( self . source . query ( criteria = dict ( zip ( self . grouping_keys , group )), properties = projection ) ) yield docs Generic get items for Map Builder designed to perform incremental building","title":"get_items()"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.prechunk","text":"Show source code in builders/group_builder.py 44 45 46 47 48 49 50 51 52 53 54 55 def prechunk ( self , number_splits : int ) -> Iterator [ Dict ]: \"\"\" Generic prechunk for group builder to perform domain-decompostion by the grouping keys \"\"\" self . ensure_indexes () keys = self . get_ids_to_process () groups = self . get_groups_from_keys ( keys ) for split in grouper ( groups , number_splits ): yield { \"query\" : dict ( zip ( self . grouping_keys , split ))} Generic prechunk for group builder to perform domain-decompostion by the grouping keys","title":"prechunk()"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.process_item","text":"Show source code in builders/group_builder.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def process_item ( self , item : List [ Dict ]) -> Dict [ Tuple , Dict ]: # type: ignore keys = list ( d [ self . source . key ] for d in item ) self . logger . debug ( \"Processing: {} \" . format ( keys )) time_start = time () try : with Timeout ( seconds = self . timeout ): processed = self . unary_function ( item ) processed . update ({ \"state\" : \"successful\" }) except Exception as e : self . logger . error ( traceback . format_exc ()) processed = { \"error\" : str ( e ), \"state\" : \"failed\" } time_end = time () last_updated = [ self . source . _lu_func [ 0 ]( d [ self . source . last_updated_field ]) for d in item ] processed . update ( { self . target . key : keys [ 0 ], f \" { self . source . key } s\" : keys , self . target . last_updated_field : max ( last_updated ), \"_bt\" : datetime . utcnow (), } ) if self . store_process_time : processed [ \"_process_time\" ] = time_end - time_start return processed Generic process items to process a dictionary using a map function","title":"process_item()"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.unary_function","text":"Show source code in builders/group_builder.py 174 175 176 177 178 179 180 181 182 183 184 185 @abstractmethod def unary_function ( self , items : List [ Dict ]) -> Dict : \"\"\" Processing function for GroupBuilder Returns: Dictionary mapping: tuple of source document keys that are in the grouped document to the grouped and processed document \"\"\" pass Processing function for GroupBuilder Returns Type Description Dict Dictionary mapping: tuple of source document keys that are in the grouped document to the grouped and processed document","title":"unary_function()"},{"location":"reference/core_builder/","text":"Module containing the core builder definition Builder \u00b6 Base Builder class At minimum this class should implement: get_items - Get items from the sources update_targets - Updates the sources with results Multiprocessing and MPI processing can be used if all the data processing is limited to process_items __init__ ( self , sources , targets , chunk_size = 1000 ) \u00b6 Show source code in core/builder.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , sources : Union [ List [ Store ], Store ], targets : Union [ List [ Store ], Store ], chunk_size : int = 1000 , ): \"\"\" Initialize the builder the framework. Arguments: sources: source Store(s) targets: target Store(s) chunk_size: chunk size for processing \"\"\" self . sources = sources if isinstance ( sources , list ) else [ sources ] self . targets = targets if isinstance ( targets , list ) else [ targets ] self . chunk_size = chunk_size self . total = None # type: Optional[int] self . logger = logging . getLogger ( type ( self ) . __name__ ) self . logger . addHandler ( logging . NullHandler ()) Initialize the builder the framework. Parameters Name Type Description Default sources Union[List[Store], Store] source Store(s) required targets Union[List[Store], Store] target Store(s) required chunk_size int chunk size for processing 1000 connect ( self ) \u00b6 Show source code in core/builder.py 48 49 50 51 52 53 def connect ( self ): \"\"\" Connect to the builder sources and targets. \"\"\" for s in self . sources + self . targets : s . connect () Connect to the builder sources and targets. finalize ( self ) \u00b6 Show source code in core/builder.py 110 111 112 113 114 115 116 117 118 119 def finalize ( self ): \"\"\" Perform any final clean up. \"\"\" # Close any Mongo connections. for store in self . sources + self . targets : try : store . close () except AttributeError : continue Perform any final clean up. get_items ( self ) \u00b6 Show source code in core/builder.py 74 75 76 77 78 79 80 81 82 @abstractmethod def get_items ( self ) -> Iterable : \"\"\" Returns all the items to process. Returns: generator or list of items to process \"\"\" pass Returns all the items to process. Returns Type Description Iterable generator or list of items to process prechunk ( self , number_splits ) \u00b6 Show source code in core/builder.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def prechunk ( self , number_splits : int ) -> Iterable [ Dict ]: \"\"\" Part of a domain-decomposition paradigm to allow the builder to operate on multiple nodes by divinding up the IO as well as the compute This function should return an iterator of dictionaries that can be distributed to multiple instances of the builder to get/process/udpate on Arguments: number_splits: The number of groups to split the documents to work on \"\"\" self . logger . info ( f \" { self . __class__ . __name__ } doesn't have distributed processing capabillities.\" \" Instead this builder will run on just one worker for all processing\" ) raise NotImplementedError ( f \" { self . __class__ . __name__ } doesn't have distributed processing capabillities.\" \" Instead this builder will run on just one worker for all processing\" ) Part of a domain-decomposition paradigm to allow the builder to operate on multiple nodes by divinding up the IO as well as the compute This function should return an iterator of dictionaries that can be distributed to multiple instances of the builder to get/process/udpate on Parameters Name Type Description Default number_splits int The number of groups to split the documents to work on required process_item ( self , item ) \u00b6 Show source code in core/builder.py 84 85 86 87 88 89 90 91 92 93 94 def process_item ( self , item : Any ) -> Any : \"\"\" Process an item. There should be no database operations in this method. Default behavior is to return the item. Arguments: item: Returns: item: an item to update \"\"\" return item Process an item. There should be no database operations in this method. Default behavior is to return the item. Parameters Name Type Description Default item Any required Returns Type Description Any item: an item to update run ( self , log_level = 10 ) \u00b6 Show source code in core/builder.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def run ( self , log_level = logging . DEBUG ): \"\"\" Run the builder serially This is only intended for diagnostic purposes \"\"\" # Set up logging root = logging . getLogger () root . setLevel ( log_level ) ch = TqdmLoggingHandler () formatter = logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s - %(message)s \" ) ch . setFormatter ( formatter ) root . addHandler ( ch ) self . connect () cursor = self . get_items () for chunk in grouper ( tqdm ( cursor ), self . chunk_size ): self . logger . info ( \"Processing batch of {} items\" . format ( self . chunk_size )) processed_chunk = [ self . process_item ( item ) for item in chunk ] processed_items = [ item for item in processed_chunk if item is not None ] self . update_targets ( processed_items ) self . finalize () Run the builder serially This is only intended for diagnostic purposes update_targets ( self , items ) \u00b6 Show source code in core/builder.py 96 97 98 99 100 101 102 103 104 105 106 107 108 @abstractmethod def update_targets ( self , items : List ): \"\"\" Takes a list of items from process item and updates the targets with them. Can also perform other book keeping in the process such as storing gridfs oids, etc. Arguments: items: Returns: \"\"\" pass Takes a list of items from process item and updates the targets with them. Can also perform other book keeping in the process such as storing gridfs oids, etc. Parameters Name Type Description Default items List required Returns Type Description ``","title":"Builder"},{"location":"reference/core_builder/#maggma.core.builder.Builder","text":"Base Builder class At minimum this class should implement: get_items - Get items from the sources update_targets - Updates the sources with results Multiprocessing and MPI processing can be used if all the data processing is limited to process_items","title":"Builder"},{"location":"reference/core_builder/#maggma.core.builder.Builder.connect","text":"Show source code in core/builder.py 48 49 50 51 52 53 def connect ( self ): \"\"\" Connect to the builder sources and targets. \"\"\" for s in self . sources + self . targets : s . connect () Connect to the builder sources and targets.","title":"connect()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.finalize","text":"Show source code in core/builder.py 110 111 112 113 114 115 116 117 118 119 def finalize ( self ): \"\"\" Perform any final clean up. \"\"\" # Close any Mongo connections. for store in self . sources + self . targets : try : store . close () except AttributeError : continue Perform any final clean up.","title":"finalize()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.get_items","text":"Show source code in core/builder.py 74 75 76 77 78 79 80 81 82 @abstractmethod def get_items ( self ) -> Iterable : \"\"\" Returns all the items to process. Returns: generator or list of items to process \"\"\" pass Returns all the items to process. Returns Type Description Iterable generator or list of items to process","title":"get_items()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.prechunk","text":"Show source code in core/builder.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def prechunk ( self , number_splits : int ) -> Iterable [ Dict ]: \"\"\" Part of a domain-decomposition paradigm to allow the builder to operate on multiple nodes by divinding up the IO as well as the compute This function should return an iterator of dictionaries that can be distributed to multiple instances of the builder to get/process/udpate on Arguments: number_splits: The number of groups to split the documents to work on \"\"\" self . logger . info ( f \" { self . __class__ . __name__ } doesn't have distributed processing capabillities.\" \" Instead this builder will run on just one worker for all processing\" ) raise NotImplementedError ( f \" { self . __class__ . __name__ } doesn't have distributed processing capabillities.\" \" Instead this builder will run on just one worker for all processing\" ) Part of a domain-decomposition paradigm to allow the builder to operate on multiple nodes by divinding up the IO as well as the compute This function should return an iterator of dictionaries that can be distributed to multiple instances of the builder to get/process/udpate on Parameters Name Type Description Default number_splits int The number of groups to split the documents to work on required","title":"prechunk()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.process_item","text":"Show source code in core/builder.py 84 85 86 87 88 89 90 91 92 93 94 def process_item ( self , item : Any ) -> Any : \"\"\" Process an item. There should be no database operations in this method. Default behavior is to return the item. Arguments: item: Returns: item: an item to update \"\"\" return item Process an item. There should be no database operations in this method. Default behavior is to return the item. Parameters Name Type Description Default item Any required Returns Type Description Any item: an item to update","title":"process_item()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.run","text":"Show source code in core/builder.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def run ( self , log_level = logging . DEBUG ): \"\"\" Run the builder serially This is only intended for diagnostic purposes \"\"\" # Set up logging root = logging . getLogger () root . setLevel ( log_level ) ch = TqdmLoggingHandler () formatter = logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s - %(message)s \" ) ch . setFormatter ( formatter ) root . addHandler ( ch ) self . connect () cursor = self . get_items () for chunk in grouper ( tqdm ( cursor ), self . chunk_size ): self . logger . info ( \"Processing batch of {} items\" . format ( self . chunk_size )) processed_chunk = [ self . process_item ( item ) for item in chunk ] processed_items = [ item for item in processed_chunk if item is not None ] self . update_targets ( processed_items ) self . finalize () Run the builder serially This is only intended for diagnostic purposes","title":"run()"},{"location":"reference/core_builder/#maggma.core.builder.Builder.update_targets","text":"Show source code in core/builder.py 96 97 98 99 100 101 102 103 104 105 106 107 108 @abstractmethod def update_targets ( self , items : List ): \"\"\" Takes a list of items from process item and updates the targets with them. Can also perform other book keeping in the process such as storing gridfs oids, etc. Arguments: items: Returns: \"\"\" pass Takes a list of items from process item and updates the targets with them. Can also perform other book keeping in the process such as storing gridfs oids, etc. Parameters Name Type Description Default items List required Returns Type Description ``","title":"update_targets()"},{"location":"reference/core_store/","text":"Module containing the core Store definition DateTimeFormat \u00b6 Datetime format in store document Sort \u00b6 Enumeration for sorting order Store \u00b6 Abstract class for a data Store Defines the interface for all data going in and out of a Builder last_updated : datetime (property, readonly) \u00b6 Provides the most recent last_updated date time stamp from the documents in this Store name : str (property, readonly) \u00b6 Return a string representing this data source __init__ ( self , key = 'task_id' , last_updated_field = 'last_updated' , last_updated_type =< DateTimeFormat . DateTime : 'datetime' > , validator = None ) \u00b6 Show source code in core/store.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , key : str = \"task_id\" , last_updated_field : str = \"last_updated\" , last_updated_type : DateTimeFormat = DateTimeFormat ( \"datetime\" ), validator : Optional [ Validator ] = None , ): \"\"\" Args: key: master key to index on last_updated_field: field for date/time stamping the data last_updated_type: the date/time format for the last_updated_field. Can be \"datetime\" or \"isoformat\" validator: Validator to validate documents going into the store \"\"\" self . key = key self . last_updated_field = last_updated_field self . last_updated_type = last_updated_type self . _lu_func = ( LU_KEY_ISOFORMAT if DateTimeFormat ( last_updated_type ) == DateTimeFormat . IsoFormat else ( identity , identity ) ) # type: Tuple[Callable, Callable] self . validator = validator self . logger = logging . getLogger ( type ( self ) . __name__ ) self . logger . addHandler ( logging . NullHandler ()) Parameters Name Type Description Default key str master key to index on 'task_id' last_updated_field str field for date/time stamping the data 'last_updated' last_updated_type DateTimeFormat the date/time format for the last_updated_field. Can be \"datetime\" or \"isoformat\" <DateTimeFormat.DateTime: 'datetime'> validator Optional[Validator] Validator to validate documents going into the store None close ( self ) \u00b6 Show source code in core/store.py 93 94 95 96 97 @abstractmethod def close ( self ): \"\"\" Closes any connections \"\"\" Closes any connections connect ( self , force_reset = False ) \u00b6 Show source code in core/store.py 84 85 86 87 88 89 90 91 @abstractmethod def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data Args: force_reset: whether to reset the connection or not \"\"\" Connect to the source data Parameters Name Type Description Default force_reset bool whether to reset the connection or not False count ( self , criteria = None ) \u00b6 Show source code in core/store.py 99 100 101 102 103 104 105 106 @abstractmethod def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" Counts the number of documents matching the query criteria Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Show source code in core/store.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria or {} results = [ key for key , _ in self . groupby ( field , properties = [ field ], criteria = criteria ) ] results = [ get ( r , field ) for r in results ] return results Get all distinct values for a field Parameters Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None ensure_index ( self , key , unique = False ) \u00b6 Show source code in core/store.py 141 142 143 144 145 146 147 148 149 150 151 152 @abstractmethod def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" Tries to create an index and return true if it suceeded Parameters Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in core/store.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @abstractmethod def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) newer_in ( self , target , criteria = None , exhaustive = False ) \u00b6 Show source code in core/store.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 def newer_in ( self , target : Store , criteria : Optional [ Dict ] = None , exhaustive : bool = False ) -> List [ str ]: \"\"\" Returns the keys of documents that are newer in the target Store than this Store. Args: target: target Store to criteria: PyMongo filter for documents to search in exhaustive: triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in \"\"\" self . ensure_index ( self . key ) self . ensure_index ( self . last_updated_field ) if exhaustive : # Get our current last_updated dates for each key value props = { self . key : 1 , self . last_updated_field : 1 , \"_id\" : 0 } dates = { d [ self . key ]: self . _lu_func [ 0 ]( d . get ( self . last_updated_field , datetime . max ) ) for d in self . query ( properties = props ) } # Get the last_updated for the store we're comparing with props = { target . key : 1 , target . last_updated_field : 1 , \"_id\" : 0 } target_dates = { d [ target . key ]: target . _lu_func [ 0 ]( d . get ( target . last_updated_field , datetime . min ) ) for d in target . query ( criteria = criteria , properties = props ) } new_keys = set ( target_dates . keys ()) - set ( dates . keys ()) updated_keys = { key for key , date in dates . items () if target_dates . get ( key , datetime . min ) > date } return list ( new_keys | updated_keys ) else : criteria = { self . last_updated_field : { \"$gt\" : self . _lu_func [ 1 ]( self . last_updated )} } return target . distinct ( field = self . key , criteria = criteria ) Returns the keys of documents that are newer in the target Store than this Store. Parameters Name Type Description Default target Store target Store to required criteria Optional[Dict] PyMongo filter for documents to search in None exhaustive bool triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in False query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in core/store.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @abstractmethod def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 query_one ( self , criteria = None , properties = None , sort = None ) \u00b6 Show source code in core/store.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def query_one ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , ): \"\"\" Queries the Store for a single document Args: criteria: PyMongo filter for documents to search properties: properties to return in the document sort: Dictionary of sort order for fields \"\"\" return next ( self . query ( criteria = criteria , properties = properties , sort = sort ), None ) Queries the Store for a single document Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search None properties Union[Dict, List, None] properties to return in the document None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None remove_docs ( self , criteria ) \u00b6 Show source code in core/store.py 180 181 182 183 184 185 186 187 @abstractmethod def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required update ( self , docs , key = None ) \u00b6 Show source code in core/store.py 128 129 130 131 132 133 134 135 136 137 138 139 @abstractmethod def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" Update documents into the Store Parameters Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str, None] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None StoreError \u00b6 General Store-related error","title":"Store"},{"location":"reference/core_store/#maggma.core.store.DateTimeFormat","text":"Datetime format in store document","title":"DateTimeFormat"},{"location":"reference/core_store/#maggma.core.store.Sort","text":"Enumeration for sorting order","title":"Sort"},{"location":"reference/core_store/#maggma.core.store.Store","text":"Abstract class for a data Store Defines the interface for all data going in and out of a Builder","title":"Store"},{"location":"reference/core_store/#maggma.core.store.Store.last_updated","text":"Provides the most recent last_updated date time stamp from the documents in this Store","title":"last_updated"},{"location":"reference/core_store/#maggma.core.store.Store.name","text":"Return a string representing this data source","title":"name"},{"location":"reference/core_store/#maggma.core.store.Store.close","text":"Show source code in core/store.py 93 94 95 96 97 @abstractmethod def close ( self ): \"\"\" Closes any connections \"\"\" Closes any connections","title":"close()"},{"location":"reference/core_store/#maggma.core.store.Store.connect","text":"Show source code in core/store.py 84 85 86 87 88 89 90 91 @abstractmethod def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data Args: force_reset: whether to reset the connection or not \"\"\" Connect to the source data Parameters Name Type Description Default force_reset bool whether to reset the connection or not False","title":"connect()"},{"location":"reference/core_store/#maggma.core.store.Store.count","text":"Show source code in core/store.py 99 100 101 102 103 104 105 106 @abstractmethod def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" Counts the number of documents matching the query criteria Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None","title":"count()"},{"location":"reference/core_store/#maggma.core.store.Store.distinct","text":"Show source code in core/store.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria or {} results = [ key for key , _ in self . groupby ( field , properties = [ field ], criteria = criteria ) ] results = [ get ( r , field ) for r in results ] return results Get all distinct values for a field Parameters Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None","title":"distinct()"},{"location":"reference/core_store/#maggma.core.store.Store.ensure_index","text":"Show source code in core/store.py 141 142 143 144 145 146 147 148 149 150 151 152 @abstractmethod def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" Tries to create an index and return true if it suceeded Parameters Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created","title":"ensure_index()"},{"location":"reference/core_store/#maggma.core.store.Store.groupby","text":"Show source code in core/store.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 @abstractmethod def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs)","title":"groupby()"},{"location":"reference/core_store/#maggma.core.store.Store.newer_in","text":"Show source code in core/store.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 def newer_in ( self , target : Store , criteria : Optional [ Dict ] = None , exhaustive : bool = False ) -> List [ str ]: \"\"\" Returns the keys of documents that are newer in the target Store than this Store. Args: target: target Store to criteria: PyMongo filter for documents to search in exhaustive: triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in \"\"\" self . ensure_index ( self . key ) self . ensure_index ( self . last_updated_field ) if exhaustive : # Get our current last_updated dates for each key value props = { self . key : 1 , self . last_updated_field : 1 , \"_id\" : 0 } dates = { d [ self . key ]: self . _lu_func [ 0 ]( d . get ( self . last_updated_field , datetime . max ) ) for d in self . query ( properties = props ) } # Get the last_updated for the store we're comparing with props = { target . key : 1 , target . last_updated_field : 1 , \"_id\" : 0 } target_dates = { d [ target . key ]: target . _lu_func [ 0 ]( d . get ( target . last_updated_field , datetime . min ) ) for d in target . query ( criteria = criteria , properties = props ) } new_keys = set ( target_dates . keys ()) - set ( dates . keys ()) updated_keys = { key for key , date in dates . items () if target_dates . get ( key , datetime . min ) > date } return list ( new_keys | updated_keys ) else : criteria = { self . last_updated_field : { \"$gt\" : self . _lu_func [ 1 ]( self . last_updated )} } return target . distinct ( field = self . key , criteria = criteria ) Returns the keys of documents that are newer in the target Store than this Store. Parameters Name Type Description Default target Store target Store to required criteria Optional[Dict] PyMongo filter for documents to search in None exhaustive bool triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in False","title":"newer_in()"},{"location":"reference/core_store/#maggma.core.store.Store.query","text":"Show source code in core/store.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @abstractmethod def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0","title":"query()"},{"location":"reference/core_store/#maggma.core.store.Store.query_one","text":"Show source code in core/store.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def query_one ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , ): \"\"\" Queries the Store for a single document Args: criteria: PyMongo filter for documents to search properties: properties to return in the document sort: Dictionary of sort order for fields \"\"\" return next ( self . query ( criteria = criteria , properties = properties , sort = sort ), None ) Queries the Store for a single document Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search None properties Union[Dict, List, None] properties to return in the document None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None","title":"query_one()"},{"location":"reference/core_store/#maggma.core.store.Store.remove_docs","text":"Show source code in core/store.py 180 181 182 183 184 185 186 187 @abstractmethod def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required","title":"remove_docs()"},{"location":"reference/core_store/#maggma.core.store.Store.update","text":"Show source code in core/store.py 128 129 130 131 132 133 134 135 136 137 138 139 @abstractmethod def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" Update documents into the Store Parameters Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str, None] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None","title":"update()"},{"location":"reference/core_store/#maggma.core.store.StoreError","text":"General Store-related error","title":"StoreError"},{"location":"reference/core_validator/","text":"Validator class for document-level validation on Stores. Attach an instance of a Validator subclass to a Store .schema variable to enable validation on that Store. Validator \u00b6 A generic class to perform document-level validation on Stores. Attach a Validator to a Store during initialization, any all documents added to the Store will call .validate_doc() before being added. is_valid ( self , doc ) \u00b6 Show source code in core/validator.py 20 21 22 23 24 25 26 27 @abstractmethod def is_valid ( self , doc : Dict ) -> bool : \"\"\" Determines if the document is valid Args: doc: document to check \"\"\" Determines if the document is valid Parameters Name Type Description Default doc Dict document to check required validation_errors ( self , doc ) \u00b6 Show source code in core/validator.py 29 30 31 32 33 34 35 36 37 38 39 @abstractmethod def validation_errors ( self , doc : Dict ) -> List [ str ]: \"\"\" If document is not valid, provides a list of strings to display for why validation has failed Returns empty list if the document is valid Args: doc: document to check \"\"\" If document is not valid, provides a list of strings to display for why validation has failed Returns empty list if the document is valid Parameters Name Type Description Default doc Dict document to check required","title":"Validator"},{"location":"reference/core_validator/#maggma.core.validator.Validator","text":"A generic class to perform document-level validation on Stores. Attach a Validator to a Store during initialization, any all documents added to the Store will call .validate_doc() before being added.","title":"Validator"},{"location":"reference/core_validator/#maggma.core.validator.Validator.is_valid","text":"Show source code in core/validator.py 20 21 22 23 24 25 26 27 @abstractmethod def is_valid ( self , doc : Dict ) -> bool : \"\"\" Determines if the document is valid Args: doc: document to check \"\"\" Determines if the document is valid Parameters Name Type Description Default doc Dict document to check required","title":"is_valid()"},{"location":"reference/core_validator/#maggma.core.validator.Validator.validation_errors","text":"Show source code in core/validator.py 29 30 31 32 33 34 35 36 37 38 39 @abstractmethod def validation_errors ( self , doc : Dict ) -> List [ str ]: \"\"\" If document is not valid, provides a list of strings to display for why validation has failed Returns empty list if the document is valid Args: doc: document to check \"\"\" If document is not valid, provides a list of strings to display for why validation has failed Returns empty list if the document is valid Parameters Name Type Description Default doc Dict document to check required","title":"validation_errors()"},{"location":"reference/stores/","text":"Module containing various definitions of Stores. Stores are a default access pattern to data and provide various utilities JSONStore \u00b6 A Store for access to a single or multiple JSON files __eq__ ( self , other ) \u00b6 Show source code in stores/mongolike.py 494 495 496 497 498 499 500 501 502 503 504 505 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for JSONStore Args: other: other JSONStore to compare with \"\"\" if not isinstance ( other , JSONStore ): return False fields = [ \"paths\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for JSONStore Parameters Name Type Description Default other object other JSONStore to compare with required __init__ ( self , paths , ** kwargs ) \u00b6 Show source code in stores/mongolike.py 464 465 466 467 468 469 470 471 472 def __init__ ( self , paths : Union [ str , List [ str ]], ** kwargs ): \"\"\" Args: paths: paths for json files to turn into a Store \"\"\" paths = paths if isinstance ( paths , ( list , tuple )) else [ paths ] self . paths = paths self . kwargs = kwargs super () . __init__ ( collection_name = \"collection\" , ** kwargs ) Parameters Name Type Description Default paths Union[str, List[str]] paths for json files to turn into a Store required connect ( self , force_reset = False , ssh_tunnel = None ) \u00b6 Show source code in stores/mongolike.py 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 def connect ( self , force_reset = False , ssh_tunnel = None ): # lgtm[py/conflicting-attributes] \"\"\" Loads the files into the collection in memory \"\"\" if ssh_tunnel is not None : warnings . warn ( f \"SSH Tunnel not needed for { self . __name__ } \" ) super () . connect ( force_reset = force_reset ) for path in self . paths : with zopen ( path ) as f : data = f . read () data = data . decode () if isinstance ( data , bytes ) else data objects = json . loads ( data ) objects = [ objects ] if not isinstance ( objects , list ) else objects self . update ( objects ) Loads the files into the collection in memory MemoryStore \u00b6 An in-memory Store that functions similarly to a MongoStore name : (property, readonly) \u00b6 Name for the store __eq__ ( self , other ) \u00b6 Show source code in stores/mongolike.py 447 448 449 450 451 452 453 454 455 456 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MemoryStore other: other MemoryStore to compare with \"\"\" if not isinstance ( other , MemoryStore ): return False fields = [ \"collection_name\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for MemoryStore other: other MemoryStore to compare with __hash__ ( self ) \u00b6 Show source code in stores/mongolike.py 403 404 405 def __hash__ ( self ): \"\"\" Hash for the store \"\"\" return hash (( self . name , self . last_updated_field )) Hash for the store __init__ ( self , collection_name = 'memory_db' , ** kwargs ) \u00b6 Show source code in stores/mongolike.py 376 377 378 379 380 381 382 383 384 385 def __init__ ( self , collection_name : str = \"memory_db\" , ** kwargs ): \"\"\" Initializes the Memory Store Args: collection_name: name for the collection in memory \"\"\" self . collection_name = collection_name self . _collection = None self . kwargs = kwargs super ( MongoStore , self ) . __init__ ( ** kwargs ) # noqa Initializes the Memory Store Parameters Name Type Description Default collection_name str name for the collection in memory 'memory_db' connect ( self , force_reset = False , ssh_tunnel = None ) \u00b6 Show source code in stores/mongolike.py 387 388 389 390 391 392 393 394 395 396 def connect ( self , force_reset : bool = False , ssh_tunnel : SSHTunnelForwarder = None ): # lgtm[py/conflicting-attributes] \"\"\" Connect to the source data \"\"\" if ssh_tunnel is not None : warnings . warn ( f \"SSH Tunnel not needed for { self . __class__ . __name__ } \" ) if not self . _collection or force_reset : self . _collection = mongomock . MongoClient () . db [ self . name ] Connect to the source data groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in stores/mongolike.py 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (key, list of elemnts) \"\"\" keys = keys if isinstance ( keys , list ) else [ keys ] data = [ doc for doc in self . query ( properties = keys , criteria = criteria ) if all ( has ( doc , k ) for k in keys ) ] def grouping_keys ( doc ): return tuple ( get ( doc , k ) for k in keys ) for vals , group in groupby ( sorted ( data , key = grouping_keys ), key = grouping_keys ): doc = {} # type: Dict[Any,Any] for k , v in zip ( keys , vals ): set_ ( doc , k , v ) yield doc , list ( group ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (key, list of elemnts) MongoStore \u00b6 A Store that connects to a Mongo collection name : str (property, readonly) \u00b6 Return a string representing this data source __eq__ ( self , other ) \u00b6 Show source code in stores/mongolike.py 315 316 317 318 319 320 321 322 323 324 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MongoStore other: other mongostore to compare with \"\"\" if not isinstance ( other , MongoStore ): return False fields = [ \"database\" , \"collection_name\" , \"host\" , \"port\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for MongoStore other: other mongostore to compare with __hash__ ( self ) \u00b6 Show source code in stores/mongolike.py 89 90 91 def __hash__ ( self ) -> int : \"\"\" Hash for MongoStore \"\"\" return hash (( self . database , self . collection_name , self . last_updated_field )) Hash for MongoStore __init__ ( self , database , collection_name , host = 'localhost' , port = 27017 , username = '' , password = '' , ** kwargs ) \u00b6 Show source code in stores/mongolike.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , database : str , collection_name : str , host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , ** kwargs , ): \"\"\" Args: database: The database name collection_name: The collection name host: Hostname for the database port: TCP port to connect to username: Username for the collection password: Password to connect with \"\"\" self . database = database self . collection_name = collection_name self . host = host self . port = port self . username = username self . password = password self . _collection = None # type: Any self . kwargs = kwargs super () . __init__ ( ** kwargs ) Parameters Name Type Description Default database str The database name required collection_name str The collection name required host str Hostname for the database 'localhost' port int TCP port to connect to 27017 username str Username for the collection '' password str Password to connect with '' close ( self ) \u00b6 Show source code in stores/mongolike.py 311 312 313 def close ( self ): \"\"\" Close up all collections \"\"\" self . _collection . database . client . close () Close up all collections connect ( self , force_reset = False , ssh_tunnel = None ) \u00b6 Show source code in stores/mongolike.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def connect ( self , force_reset : bool = False , ssh_tunnel : SSHTunnelForwarder = None ): # lgtm[py/conflicting-attributes] \"\"\" Connect to the source data \"\"\" if not self . _collection or force_reset : if ssh_tunnel is None : conn = MongoClient ( self . host , self . port ) else : conn = MongoClient ( * ssh_tunnel . local_bind_address ) db = conn [ self . database ] if self . username != \"\" : db . authenticate ( self . username , self . password ) self . _collection = db [ self . collection_name ] Connect to the source data count ( self , criteria = None ) \u00b6 Show source code in stores/mongolike.py 197 198 199 200 201 202 203 204 205 206 def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" criteria = criteria if criteria else {} return self . _collection . find ( filter = criteria ) . count () Counts the number of documents matching the query criteria Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Show source code in stores/mongolike.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria or {} distinct_vals = self . _collection . distinct ( field , criteria ) return distinct_vals if distinct_vals is not None else [] Get all distinct values for a field Parameters Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None ensure_index ( self , key , unique = False ) \u00b6 Show source code in stores/mongolike.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 def ensure_index ( self , key : str , unique : Optional [ bool ] = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" if confirm_field_index ( self . _collection , key ): return True else : try : self . _collection . create_index ( key , unique = unique , background = True ) return True except Exception : return False Tries to create an index and return true if it suceeded Parameters Name Type Description Default key str single key to index required unique Optional[bool] Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created from_collection ( collection ) (classmethod) \u00b6 Show source code in stores/mongolike.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 @classmethod def from_collection ( cls , collection ): \"\"\" Generates a MongoStore from a pymongo collection object This is not a fully safe operation as it gives dummy information to the MongoStore As a result, this will not serialize and can not reset its connection Args: collection: the PyMongo collection to create a MongoStore around \"\"\" # TODO: How do we make this safer? coll_name = collection . name db_name = collection . database . name store = cls ( db_name , coll_name ) store . _collection = collection return store Generates a MongoStore from a pymongo collection object This is not a fully safe operation as it gives dummy information to the MongoStore As a result, this will not serialize and can not reset its connection Parameters Name Type Description Default collection the PyMongo collection to create a MongoStore around required from_db_file ( filename ) (classmethod) \u00b6 Show source code in stores/mongolike.py 93 94 95 96 97 98 99 100 101 102 103 104 @classmethod def from_db_file ( cls , filename : str ): \"\"\" Convenience method to construct MongoStore from db_file from old QueryEngine format \"\"\" kwargs = loadfn ( filename ) if \"collection\" in kwargs : kwargs [ \"collection_name\" ] = kwargs . pop ( \"collection\" ) # Get rid of aliases from traditional query engine db docs kwargs . pop ( \"aliases\" , None ) return cls ( ** kwargs ) Convenience method to construct MongoStore from db_file from old QueryEngine format groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in stores/mongolike.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (key, list of docs) \"\"\" pipeline = [] if isinstance ( keys , str ): keys = [ keys ] if properties is None : properties = [] if isinstance ( properties , dict ): properties = list ( properties . keys ()) if criteria is not None : pipeline . append ({ \"$match\" : criteria }) if len ( properties ) > 0 : pipeline . append ({ \"$project\" : { p : 1 for p in properties + keys }}) alpha = \"abcdefghijklmnopqrstuvwxyz\" group_id = { letter : f \"$ { key } \" for letter , key in zip ( alpha , keys )} pipeline . append ({ \"$group\" : { \"_id\" : group_id , \"docs\" : { \"$push\" : \"$$ROOT\" }}}) for d in self . _collection . aggregate ( pipeline , allowDiskUse = True ): id_doc = {} # type: Dict[str,Any] for letter , key in group_id . items (): if has ( d [ \"_id\" ], letter ): set_ ( id_doc , key [ 1 :], d [ \"_id\" ][ letter ]) yield ( id_doc , d [ \"docs\" ]) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (key, list of docs) query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in stores/mongolike.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( properties , list ): properties = { p : 1 for p in properties } sort_list = [( k , v . value ) for k , v in sort . items ()] if sort else None for d in self . _collection . find ( filter = criteria , projection = properties , skip = skip , limit = limit , sort = sort_list , ): yield d Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 remove_docs ( self , criteria ) \u00b6 Show source code in stores/mongolike.py 302 303 304 305 306 307 308 309 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" self . _collection . delete_many ( filter = criteria ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required update ( self , docs , key = None ) \u00b6 Show source code in stores/mongolike.py 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" requests = [] if not isinstance ( docs , list ): docs = [ docs ] for d in docs : d = jsanitize ( d , allow_bson = True ) # document-level validation is optional validates = True if self . validator : validates = self . validator . is_valid ( d ) if not validates : if self . validator . strict : raise ValueError ( self . validator . validation_errors ( d )) else : self . logger . error ( self . validator . validation_errors ( d )) if validates : key = key or self . key if isinstance ( key , list ): search_doc = { k : d [ k ] for k in key } else : search_doc = { key : d [ key ]} requests . append ( ReplaceOne ( search_doc , d , upsert = True )) if len ( requests ) > 0 : self . _collection . bulk_write ( requests , ordered = False ) Update documents into the Store Parameters Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str, None] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None MongoURIStore \u00b6 A Store that connects to a Mongo collection via a URI This is expected to be a special mongodb+srv:// URIs that include client parameters via TXT records name : str (property, readonly) \u00b6 Return a string representing this data source __init__ ( self , uri , database , collection_name , ** kwargs ) \u00b6 Show source code in stores/mongolike.py 334 335 336 337 338 339 340 341 342 343 344 345 346 def __init__ ( self , uri : str , database : str , collection_name : str , ** kwargs ): \"\"\" Args: uri: MongoDB+SRV URI database: database to connect to collection_name: The collection name \"\"\" self . uri = uri self . database = database self . collection_name = collection_name self . kwargs = kwargs self . _collection = None super ( MongoStore , self ) . __init__ ( ** kwargs ) # lgtm Parameters Name Type Description Default uri str MongoDB+SRV URI required database str database to connect to required collection_name str The collection name required connect ( self , force_reset = False , ssh_tunnel = None ) \u00b6 Show source code in stores/mongolike.py 356 357 358 359 360 361 362 363 364 365 366 367 def connect ( self , force_reset : bool = False , ssh_tunnel : SSHTunnelForwarder = None ): # lgtm[py/conflicting-attributes] \"\"\" Connect to the source data \"\"\" if ssh_tunnel is not None : warnings . warn ( f \"SSH Tunnel not needed for { self . __class__ . __name__ } \" ) if not self . _collection or force_reset : conn = MongoClient ( self . uri ) db = conn [ self . database ] self . _collection = db [ self . collection_name ] Connect to the source data Module containing various definitions of Stores. Stores are a default access pattern to data and provide various utillities GridFSStore \u00b6 A Store for GrdiFS backend. Provides a common access method consistent with other stores last_updated : datetime (property, readonly) \u00b6 Provides the most recent last_updated date time stamp from the documents in this Store __eq__ ( self , other ) \u00b6 Show source code in stores/gridfs.py 342 343 344 345 346 347 348 349 350 351 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for GridFSStore other: other GridFSStore to compare with \"\"\" if not isinstance ( other , GridFSStore ): return False fields = [ \"database\" , \"collection_name\" , \"host\" , \"port\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for GridFSStore other: other GridFSStore to compare with __init__ ( self , database , collection_name , host = 'localhost' , port = 27017 , username = '' , password = '' , compression = False , ** kwargs ) \u00b6 Show source code in stores/gridfs.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , database : str , collection_name : str , host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , compression : bool = False , ** kwargs , ): \"\"\" Initializes a GrdiFS Store for binary data Args: database: database name collection_name: The name of the collection. This is the string portion before the GridFS extensions host: hostname for the database port: port to connec to username: username to connect as password: password to authenticate as \"\"\" self . database = database self . collection_name = collection_name self . host = host self . port = port self . username = username self . password = password self . _collection = None # type: Any self . compression = compression self . kwargs = kwargs self . meta_keys = set () # type: Set[str] if \"key\" not in kwargs : kwargs [ \"key\" ] = \"_id\" super () . __init__ ( ** kwargs ) Initializes a GrdiFS Store for binary data Parameters Name Type Description Default database str database name required collection_name str The name of the collection. This is the string portion before the GridFS extensions required host str hostname for the database 'localhost' port int port to connec to 27017 username str username to connect as '' password str password to authenticate as '' close ( self ) \u00b6 Show source code in stores/gridfs.py 339 340 def close ( self ): self . _collection . database . client . close () Closes any connections connect ( self , force_reset = False ) \u00b6 Show source code in stores/gridfs.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" conn = MongoClient ( self . host , self . port ) if not self . _collection or force_reset : db = conn [ self . database ] if self . username != \"\" : db . authenticate ( self . username , self . password ) self . _collection = gridfs . GridFS ( db , self . collection_name ) self . _files_collection = db [ \" {} .files\" . format ( self . collection_name )] self . _files_store = MongoStore . from_collection ( self . _files_collection ) self . _files_store . last_updated_field = f \"metadata. { self . last_updated_field } \" self . _chunks_collection = db [ \" {} .chunks\" . format ( self . collection_name )] Connect to the source data count ( self , criteria = None ) \u00b6 Show source code in stores/gridfs.py 136 137 138 139 140 141 142 143 144 145 146 def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) return self . _files_store . count ( criteria ) Counts the number of documents matching the query criteria Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Show source code in stores/gridfs.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = ( self . transform_criteria ( criteria ) if isinstance ( criteria , dict ) else criteria ) field = ( f \"metadata. { field } \" if field not in self . files_collection_fields and not field . startswith ( \"metadata.\" ) else field ) return self . _files_store . distinct ( field = field , criteria = criteria ) Get all distinct values for a field Parameters Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None ensure_index ( self , key , unique = False ) \u00b6 Show source code in stores/gridfs.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 def ensure_index ( self , key : str , unique : Optional [ bool ] = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Currently operators on the GridFS files collection Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" # Transform key for gridfs first if key not in self . files_collection_fields : files_col_key = \"metadata. {} \" . format ( key ) return self . _files_store . ensure_index ( files_col_key , unique = unique ) else : return self . _files_store . ensure_index ( key , unique = unique ) Tries to create an index and return true if it suceeded Currently operators on the GridFS files collection Parameters Name Type Description Default key str single key to index required unique Optional[bool] Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in stores/gridfs.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Will only work if the keys are included in the files collection for GridFS Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" criteria = ( self . transform_criteria ( criteria ) if isinstance ( criteria , dict ) else criteria ) keys = [ keys ] if not isinstance ( keys , list ) else keys keys = [ f \"metadata. { k } \" if k not in self . files_collection_fields and not k . startswith ( \"metadata.\" ) else k for k in keys ] for group , ids in self . _files_store . groupby ( keys , criteria = criteria , properties = [ f \"metadata. { self . key } \" ] ): ids = [ get ( doc , f \"metadata. { self . key } \" ) for doc in ids if has ( doc , f \"metadata. { self . key } \" ) ] group = { k . replace ( \"metadata.\" , \"\" ): get ( group , k ) for k in keys if has ( group , k ) } yield group , list ( self . query ( criteria = { self . key : { \"$in\" : ids }})) Simple grouping function that will group documents by keys. Will only work if the keys are included in the files collection for GridFS Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) name ( self ) \u00b6 Show source code in stores/gridfs.py 83 84 85 86 87 def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" return f \"gridfs:// { self . host } / { self . database } / { self . collection_name } \" Return a string representing this data source query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in stores/gridfs.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the GridFS Store for a set of documents Currently ignores properties TODO: If properties wholy in metadata, just query that Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) for f in self . _collection . find ( filter = criteria , skip = skip , limit = limit , sort = sort ): data = f . read () metadata = f . metadata if metadata . get ( \"compression\" , \"\" ) == \"zlib\" : data = zlib . decompress ( data ) . decode ( \"UTF-8\" ) try : data = json . loads ( data ) except Exception : pass yield data Queries the GridFS Store for a set of documents Currently ignores properties TODO: If properties wholy in metadata, just query that Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 remove_docs ( self , criteria ) \u00b6 Show source code in stores/gridfs.py 325 326 327 328 329 330 331 332 333 334 335 336 337 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) ids = [ cursor . _id for cursor in self . _collection . find ( criteria )] for id in ids : self . _collection . delete ( id ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required transform_criteria ( criteria ) (classmethod) \u00b6 Show source code in stores/gridfs.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 @classmethod def transform_criteria ( cls , criteria : Dict ) -> Dict : \"\"\" Allow client to not need to prepend 'metadata.' to query fields. Args: criteria: Query criteria \"\"\" new_criteria = dict () for field in criteria : if field not in cls . files_collection_fields and not field . startswith ( \"metadata.\" ): new_criteria [ \"metadata.\" + field ] = copy . copy ( criteria [ field ]) else : new_criteria [ field ] = copy . copy ( criteria [ field ]) return new_criteria Allow client to not need to prepend 'metadata.' to query fields. Parameters Name Type Description Default criteria Dict Query criteria required update ( self , docs , key = None ) \u00b6 Show source code in stores/gridfs.py 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" if not isinstance ( docs , list ): docs = [ docs ] if isinstance ( key , str ): key = [ key ] elif not key : key = [ self . key ] key = list ( set ( key ) | self . meta_keys - set ( self . files_collection_fields )) for d in docs : search_doc = { k : d [ k ] for k in key } metadata = { k : d [ k ] for k in [ self . last_updated_field ] if k in d } metadata . update ( search_doc ) data = json . dumps ( jsanitize ( d )) . encode ( \"UTF-8\" ) if self . compression : data = zlib . compress ( data ) metadata [ \"compression\" ] = \"zlib\" self . _collection . put ( data , metadata = metadata ) search_doc = self . transform_criteria ( search_doc ) # Cleans up old gridfs entries for fdoc in ( self . _files_collection . find ( search_doc , [ \"_id\" ]) . sort ( \"uploadDate\" , - 1 ) . skip ( 1 ) ): self . _collection . delete ( fdoc [ \"_id\" ]) Update documents into the Store Parameters Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str, None] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Advanced Stores for connecting to AWS data S3Store \u00b6 GridFS like storage using Amazon S3 and a regular store for indexing Assumes Amazon AWS key and secret key are set in environment or default config file last_updated : (property, readonly) \u00b6 Provides the most recent last_updated date time stamp from the documents in this Store __eq__ ( self , other ) \u00b6 Show source code in stores/aws.py 334 335 336 337 338 339 340 341 342 343 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for S3Store other: other S3Store to compare with \"\"\" if not isinstance ( other , S3Store ): return False fields = [ \"index\" , \"bucket\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for S3Store other: other S3Store to compare with __init__ ( self , index , bucket , s3_profile = None , compress = False , endpoint_url = None , sub_dir = None , ** kwargs ) \u00b6 Show source code in stores/aws.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , index : Store , bucket : str , s3_profile : str = None , compress : bool = False , endpoint_url : str = None , sub_dir : str = None , ** kwargs , ): \"\"\" Initializes an S3 Store Args: index: a store to use to index the S3 Bucket bucket: name of the bucket s3_profile: name of aws profile containing credentials for role compress: compress files inserted into the store endpoint_url: endpoint_url to allow interface to minio service sub_dir: (optional) subdirectory of the s3 bucket to store the data \"\"\" if boto3 is None : raise RuntimeError ( \"boto3 and botocore are required for S3Store\" ) self . index = index self . bucket = bucket self . s3_profile = s3_profile self . compress = compress self . endpoint_url = endpoint_url self . sub_dir = sub_dir . strip ( \"/\" ) + \"/\" if sub_dir else \"\" self . s3 = None # type: Any self . s3_bucket = None # type: Any # Force the key to be the same as the index kwargs [ \"key\" ] = str ( index . key ) super ( S3Store , self ) . __init__ ( ** kwargs ) Initializes an S3 Store Parameters Name Type Description Default index Store a store to use to index the S3 Bucket required bucket str name of the bucket required s3_profile str name of aws profile containing credentials for role None compress bool compress files inserted into the store False endpoint_url str endpoint_url to allow interface to minio service None sub_dir str (optional) subdirectory of the s3 bucket to store the data None close ( self ) \u00b6 Show source code in stores/aws.py 89 90 91 92 93 94 95 def close ( self ): \"\"\" Closes any connections \"\"\" self . index . close () self . s3 = None self . s3_bucket = None Closes any connections connect ( self , force_reset = False ) \u00b6 Show source code in stores/aws.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" self . index . connect ( force_reset = force_reset ) session = Session ( profile_name = self . s3_profile ) resource = session . resource ( \"s3\" , endpoint_url = self . endpoint_url ) if not self . s3 : self . s3 = resource if self . bucket not in [ bucket . name for bucket in self . s3 . buckets . all ()]: raise Exception ( \"Bucket not present on AWS: {} \" . format ( self . bucket )) self . s3_bucket = resource . Bucket ( self . bucket ) Connect to the source data count ( self , criteria = None ) \u00b6 Show source code in stores/aws.py 110 111 112 113 114 115 116 117 118 def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" return self . index . count ( criteria ) Counts the number of documents matching the query criteria Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Show source code in stores/aws.py 173 174 175 176 177 178 179 180 181 182 183 184 def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" # Index is a store so it should have its own distinct function return self . index . distinct ( field , criteria = criteria ) Get all distinct values for a field Parameters Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None ensure_index ( self , key , unique = False ) \u00b6 Show source code in stores/aws.py 219 220 221 222 223 224 225 226 227 228 229 230 def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" return self . index . ensure_index ( key , unique = unique ) Tries to create an index and return true if it suceeded Parameters Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in stores/aws.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" return self . index . groupby ( keys = keys , criteria = criteria , properties = properties , sort = sort , skip = skip , limit = limit , ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) name ( self ) \u00b6 Show source code in stores/aws.py 66 67 68 69 70 71 def name ( self ) -> str : \"\"\" Returns: a string representing this data source \"\"\" return f \"s3:// { self . bucket } \" Returns Type Description str a string representing this data source newer_in ( self , target , criteria = None , exhaustive = False ) \u00b6 Show source code in stores/aws.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 def newer_in ( self , target : Store , criteria : Optional [ Dict ] = None , exhaustive : bool = False ) -> List [ str ]: \"\"\" Returns the keys of documents that are newer in the target Store than this Store. Args: target: target Store criteria: PyMongo filter for documents to search in exhaustive: triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in \"\"\" return self . index . newer_in ( target = target , criteria = criteria , exhaustive = exhaustive ) Returns the keys of documents that are newer in the target Store than this Store. Parameters Name Type Description Default target Store target Store required criteria Optional[Dict] PyMongo filter for documents to search in None exhaustive bool triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in False query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in stores/aws.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" prop_keys = set () if isinstance ( properties , dict ): prop_keys = set ( properties . keys ()) elif isinstance ( properties , list ): prop_keys = set ( properties ) for doc in self . index . query ( criteria = criteria , sort = sort , limit = limit , skip = skip ): if properties is not None and prop_keys . issubset ( set ( doc . keys ())): yield { p : doc [ p ] for p in properties if p in doc } else : try : # TODO: THis is ugly and unsafe, do some real checking before pulling data data = ( self . s3_bucket . Object ( self . sub_dir + doc [ self . key ]) . get ()[ \"Body\" ] . read () ) except botocore . exceptions . ClientError as e : # If a client error is thrown, then check that it was a 404 error. # If it was a 404 error, then the object does not exist. error_code = int ( e . response [ \"Error\" ][ \"Code\" ]) if error_code == 404 : self . logger . error ( \"Could not find S3 object {} \" . format ( doc [ self . key ]) ) break else : raise e if doc . get ( \"compression\" , \"\" ) == \"zlib\" : data = zlib . decompress ( data ) yield json . loads ( data ) Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 rebuild_index_from_s3_data ( self ) \u00b6 Show source code in stores/aws.py 322 323 324 325 326 327 328 329 330 331 332 def rebuild_index_from_s3_data ( self ): \"\"\" Rebuilds the index Store from the data in S3 Relies on the index document being stores as the metadata for the file \"\"\" index_docs = [] for file in self . s3_bucket . objects . all (): # TODO: Transform the data back from strings and remove AWS S3 specific keys index_docs . append ( file . metadata ) self . index . update ( index_docs ) Rebuilds the index Store from the data in S3 Relies on the index document being stores as the metadata for the file remove_docs ( self , criteria , remove_s3_object = False ) \u00b6 Show source code in stores/aws.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def remove_docs ( self , criteria : Dict , remove_s3_object : bool = False ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match remove_s3_object: whether to remove the actual S3 Object or not \"\"\" if not remove_s3_object : self . index . remove_docs ( criteria = criteria ) else : to_remove = self . index . distinct ( self . key , criteria = criteria ) self . index . remove_docs ( criteria = criteria ) # Can remove up to 1000 items at a time via boto to_remove_chunks = list ( grouper ( to_remove , n = 1000 )) for chunk_to_remove in to_remove_chunks : objlist = [{ \"Key\" : self . sub_dir + obj } for obj in chunk_to_remove ] self . s3_bucket . delete_objects ( Delete = { \"Objects\" : objlist }) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required remove_s3_object bool whether to remove the actual S3 Object or not False update ( self , docs , key = None ) \u00b6 Show source code in stores/aws.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" search_docs = [] search_keys = [] if isinstance ( key , list ): search_keys = key elif key : search_keys = [ key ] else : search_keys = [ self . key ] for d in docs : search_doc = { k : d [ k ] for k in search_keys } search_doc [ self . key ] = d [ self . key ] # Ensure key is in metadata if self . sub_dir != \"\" : search_doc [ \"sub_dir\" ] = self . sub_dir # Remove MongoDB _id from search if \"_id\" in search_doc : del search_doc [ \"_id\" ] data = json . dumps ( jsanitize ( d )) . encode () # Compress with zlib if chosen if self . compress : search_doc [ \"compression\" ] = \"zlib\" data = zlib . compress ( data ) self . s3_bucket . put_object ( Key = self . sub_dir + str ( d [ self . key ]), Body = data , Metadata = search_doc ) search_docs . append ( search_doc ) # Use store's update to remove key clashes self . index . update ( search_docs ) Update documents into the Store Parameters Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Optional[Union[List, str]] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Advanced Stores for behavior outside normal access patterns AliasingStore \u00b6 Special Store that aliases for the primary accessors __eq__ ( self , other ) \u00b6 Show source code in stores/advanced_stores.py 376 377 378 379 380 381 382 383 384 385 386 387 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for AliasingStore Args: other: other AliasingStore to compare with \"\"\" if not isinstance ( other , AliasingStore ): return False fields = [ \"store\" , \"aliases\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for AliasingStore Parameters Name Type Description Default other object other AliasingStore to compare with required __init__ ( self , store , aliases , ** kwargs ) \u00b6 Show source code in stores/advanced_stores.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def __init__ ( self , store : Store , aliases : Dict , ** kwargs ): \"\"\" Args: store: the store to wrap around aliases: dict of aliases of the form external key: internal key \"\"\" self . store = store # Given an external key tells what the internal key is self . aliases = aliases # Given the internal key tells us what the external key is self . reverse_aliases = { v : k for k , v in aliases . items ()} self . kwargs = kwargs kwargs . update ( { \"last_updated_field\" : store . last_updated_field , \"last_updated_type\" : store . last_updated_type , } ) super ( AliasingStore , self ) . __init__ ( ** kwargs ) Parameters Name Type Description Default store Store the store to wrap around required aliases Dict dict of aliases of the form external key: internal key required close ( self ) \u00b6 Show source code in stores/advanced_stores.py 366 367 def close ( self ): self . store . close () Closes any connections connect ( self , force_reset = False ) \u00b6 Show source code in stores/advanced_stores.py 373 374 def connect ( self , force_reset = False ): self . store . connect ( force_reset = force_reset ) Connect to the source data Parameters Name Type Description Default force_reset whether to reset the connection or not False count ( self , criteria = None ) \u00b6 Show source code in stores/advanced_stores.py 225 226 227 228 229 230 231 232 233 234 def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" criteria = criteria if criteria else {} lazy_substitute ( criteria , self . reverse_aliases ) return self . store . count ( criteria ) Counts the number of documents matching the query criteria Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Show source code in stores/advanced_stores.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria if criteria else {} lazy_substitute ( criteria , self . reverse_aliases ) # substitute forward return self . store . distinct ( self . aliases [ field ], criteria = criteria ) Get all distinct values for a field Parameters Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None ensure_index ( self , key , unique = False , ** kwargs ) \u00b6 Show source code in stores/advanced_stores.py 361 362 363 364 def ensure_index ( self , key , unique = False , ** kwargs ): if key in self . aliases : key = self . aliases return self . store . ensure_index ( key , unique , ** kwargs ) Tries to create an index and return true if it suceeded Parameters Name Type Description Default key single key to index required unique Whether or not this index contains only unique keys False Returns Type Description `` bool indicating if the index exists/was created groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in stores/advanced_stores.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" # Convert to a list keys = keys if isinstance ( keys , list ) else [ keys ] # Make the aliasing transformations on keys keys = [ self . aliases [ k ] if k in self . aliases else k for k in keys ] # Update criteria and properties based on aliases criteria = criteria if criteria else {} if properties is not None : if isinstance ( properties , list ): properties = { p : 1 for p in properties } substitute ( properties , self . reverse_aliases ) lazy_substitute ( criteria , self . reverse_aliases ) return self . store . groupby ( keys = keys , properties = properties , criteria = criteria , skip = skip , limit = limit ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) name ( self ) \u00b6 Show source code in stores/advanced_stores.py 219 220 221 222 223 def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" return self . store . name Return a string representing this data source query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in stores/advanced_stores.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" criteria = criteria if criteria else {} if properties is not None : if isinstance ( properties , list ): properties = { p : 1 for p in properties } substitute ( properties , self . reverse_aliases ) lazy_substitute ( criteria , self . reverse_aliases ) for d in self . store . query ( properties = properties , criteria = criteria , sort = sort , limit = limit , skip = skip ): substitute ( d , self . aliases ) yield d Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 remove_docs ( self , criteria ) \u00b6 Show source code in stores/advanced_stores.py 350 351 352 353 354 355 356 357 358 359 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" # Update criteria and properties based on aliases lazy_substitute ( criteria , self . reverse_aliases ) self . store . remove_docs ( criteria ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required update ( self , docs , key = None ) \u00b6 Show source code in stores/advanced_stores.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" key = key if key else self . key for d in docs : substitute ( d , self . reverse_aliases ) if key in self . aliases : key = self . aliases [ key ] self . store . update ( docs , key = key ) Update documents into the Store Parameters Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Optional[Union[List, str]] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None MongograntStore \u00b6 Initialize a Store with a mongogrant \" <role> : <host> / <db> .\" spec. Some class methods of MongoStore, e.g. from_db_file and from_collection, are not supported. mongogrant documentation: https://github.com/materialsproject/mongogrant name : (property, readonly) \u00b6 Return a string representing this data source __eq__ ( self , other ) \u00b6 Show source code in stores/advanced_stores.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for MongograntStore Args: other: other MongograntStore to compare with \"\"\" if not isinstance ( other , MongograntStore ): return False fields = [ \"mongogrant_spec\" , \"collection_name\" , \"mgclient_config_path\" , \"last_updated_field\" , ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for MongograntStore Parameters Name Type Description Default other object other MongograntStore to compare with required __init__ ( self , mongogrant_spec , collection_name , mgclient_config_path = None , ** kwargs ) \u00b6 Show source code in stores/advanced_stores.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , mongogrant_spec : str , collection_name : str , mgclient_config_path : Optional [ str ] = None , ** kwargs , ): \"\"\" Args: mongogrant_spec: of the form `<role>`:`<host>`/`<db>`, where role is one of {\"read\", \"readWrite\"} or aliases {\"ro\", \"rw\"}; host is a db host (w/ optional port) or alias; and db is a db on that host, or alias. See mongogrant documentation. collection_name: name of mongo collection mgclient_config_path: Path to mongogrant client config file, or None if default path (`mongogrant.client.path`). \"\"\" self . mongogrant_spec = mongogrant_spec self . collection_name = collection_name self . mgclient_config_path = mgclient_config_path self . _collection = None if self . mgclient_config_path : config = Config ( check = check , path = self . mgclient_config_path ) client = Client ( config ) else : client = Client () if set (( \"username\" , \"password\" , \"database\" , \"host\" )) & set ( kwargs ): raise StoreError ( \"MongograntStore does not accept \" \"username, password, database, or host \" \"arguments. Use `mongogrant_spec`.\" ) self . kwargs = kwargs _auth_info = client . get_db_auth_from_spec ( self . mongogrant_spec ) super ( MongograntStore , self ) . __init__ ( host = _auth_info [ \"host\" ], database = _auth_info [ \"authSource\" ], username = _auth_info [ \"username\" ], password = _auth_info [ \"password\" ], collection_name = self . collection_name , ** kwargs , ) Parameters Name Type Description Default mongogrant_spec str of the form <role> : <host> / <db> , where role is one of {\"read\", \"readWrite\"} or aliases {\"ro\", \"rw\"}; host is a db host (w/ optional port) or alias; and db is a db on that host, or alias. See mongogrant documentation. required collection_name str name of mongo collection required mgclient_config_path Optional[str] Path to mongogrant client config file, or None if default path ( mongogrant.client.path ). None from_collection ( collection ) (classmethod) \u00b6 Show source code in stores/advanced_stores.py 94 95 96 97 98 99 @classmethod def from_collection ( cls , collection ): \"\"\" Raises ValueError since MongograntStores can't be initialized from a PyMongo collection \"\"\" raise ValueError ( \"MongograntStore doesn't implement from_collection\" ) Raises ValueError since MongograntStores can't be initialized from a PyMongo collection from_db_file ( file ) (classmethod) \u00b6 Show source code in stores/advanced_stores.py 87 88 89 90 91 92 @classmethod def from_db_file ( cls , file ): \"\"\" Raises ValueError since MongograntStores can't be initialized from a file \"\"\" raise ValueError ( \"MongograntStore doesn't implement from_db_file\" ) Raises ValueError since MongograntStores can't be initialized from a file SandboxStore \u00b6 Provides a sandboxed view to another store sbx_criteria : Dict (property, readonly) \u00b6 Returns Type Description Dict the sandbox criteria dict used to filter the source store __eq__ ( self , other ) \u00b6 Show source code in stores/advanced_stores.py 546 547 548 549 550 551 552 553 554 555 556 557 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for SandboxStore Args: other: other SandboxStore to compare with \"\"\" if not isinstance ( other , SandboxStore ): return False fields = [ \"store\" , \"sandbox\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for SandboxStore Parameters Name Type Description Default other object other SandboxStore to compare with required __init__ ( self , store , sandbox , exclusive = False ) \u00b6 Show source code in stores/advanced_stores.py 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def __init__ ( self , store : Store , sandbox : str , exclusive : bool = False ): \"\"\" Args: store: store to wrap sandboxing around sandbox: the corresponding sandbox exclusive: whether to be exclusively in this sandbox or include global items \"\"\" self . store = store self . sandbox = sandbox self . exclusive = exclusive super () . __init__ ( key = self . store . key , last_updated_field = self . store . last_updated_field , last_updated_type = self . store . last_updated_type , validator = self . store . validator , ) Parameters Name Type Description Default store Store store to wrap sandboxing around required sandbox str the corresponding sandbox required exclusive bool whether to be exclusively in this sandbox or include global items False close ( self ) \u00b6 Show source code in stores/advanced_stores.py 536 537 def close ( self ): self . store . close () Closes any connections connect ( self , force_reset = False ) \u00b6 Show source code in stores/advanced_stores.py 543 544 def connect ( self , force_reset = False ): self . store . connect ( force_reset = force_reset ) Connect to the source data Parameters Name Type Description Default force_reset whether to reset the connection or not False count ( self , criteria = None ) \u00b6 Show source code in stores/advanced_stores.py 432 433 434 435 436 437 438 439 440 441 442 def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . count ( criteria = criteria ) Counts the number of documents matching the query criteria Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None ensure_index ( self , key , unique = False , ** kwargs ) \u00b6 Show source code in stores/advanced_stores.py 533 534 def ensure_index ( self , key , unique = False , ** kwargs ): return self . store . ensure_index ( key , unique , ** kwargs ) Tries to create an index and return true if it suceeded Parameters Name Type Description Default key single key to index required unique Whether or not this index contains only unique keys False Returns Type Description `` bool indicating if the index exists/was created groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in stores/advanced_stores.py 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . groupby ( keys = keys , properties = properties , criteria = criteria , skip = skip , limit = limit ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) name ( self ) \u00b6 Show source code in stores/advanced_stores.py 412 413 414 415 416 417 def name ( self ) -> str : \"\"\" Returns: a string representing this data source \"\"\" return f \"Sandbox[ { self . store . name } ][ { self . sandbox } ]\" Returns Type Description str a string representing this data source query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in stores/advanced_stores.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . query ( properties = properties , criteria = criteria , sort = sort , limit = limit , skip = skip ) Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 remove_docs ( self , criteria ) \u00b6 Show source code in stores/advanced_stores.py 520 521 522 523 524 525 526 527 528 529 530 531 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" # Update criteria and properties based on aliases criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) self . store . remove_docs ( criteria ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required update ( self , docs , key = None ) \u00b6 Show source code in stores/advanced_stores.py 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" for d in docs : if \"sbxn\" in d : d [ \"sbxn\" ] = list ( set ( d [ \"sbxn\" ] + [ self . sandbox ])) else : d [ \"sbxn\" ] = [ self . sandbox ] self . store . update ( docs , key = key ) Update documents into the Store Parameters Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Optional[Union[List, str]] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None VaultStore \u00b6 Extends MongoStore to read credentials out of Vault server and uses these values to initialize MongoStore instance __eq__ ( self , other ) \u00b6 Show source code in stores/advanced_stores.py 179 180 181 182 183 184 185 186 187 188 189 190 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for VaultStore Args: other: other VaultStore to compare with \"\"\" if not isinstance ( other , VaultStore ): return False fields = [ \"vault_secret_path\" , \"collection_name\" , \"last_updated_field\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for VaultStore Parameters Name Type Description Default other object other VaultStore to compare with required __init__ ( self , collection_name , vault_secret_path ) \u00b6 Show source code in stores/advanced_stores.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 @requires ( hvac is not None , \"hvac is required to use VaultStore\" ) def __init__ ( self , collection_name : str , vault_secret_path : str ): \"\"\" Args: collection_name: name of mongo collection vault_secret_path: path on vault server with mongo creds object Important: Environment variables that must be set prior to invocation VAULT_ADDR - URL of vault server (eg. https://matgen8.lbl.gov:8200) VAULT_TOKEN or GITHUB_TOKEN - token used to authenticate to vault \"\"\" self . collection_name = collection_name self . vault_secret_path = vault_secret_path # TODO: Switch this over to Pydantic ConfigSettings vault_addr = os . getenv ( \"VAULT_ADDR\" ) if not vault_addr : raise RuntimeError ( \"VAULT_ADDR not set\" ) client = hvac . Client ( vault_addr ) # If we have a vault token use this token = os . getenv ( \"VAULT_TOKEN\" ) # Look for a github token instead if not token : github_token = os . getenv ( \"GITHUB_TOKEN\" ) if github_token : client . auth_github ( github_token ) else : raise RuntimeError ( \"VAULT_TOKEN or GITHUB_TOKEN not set\" ) else : client . token = token if not client . is_authenticated (): raise RuntimeError ( \"Bad token\" ) # Read the vault secret json_db_creds = client . read ( vault_secret_path ) db_creds = json . loads ( json_db_creds [ \"data\" ][ \"value\" ]) database = db_creds . get ( \"db\" ) host = db_creds . get ( \"host\" , \"localhost\" ) port = db_creds . get ( \"port\" , 27017 ) username = db_creds . get ( \"username\" , \"\" ) password = db_creds . get ( \"password\" , \"\" ) super ( VaultStore , self ) . __init__ ( database , collection_name , host , port , username , password ) Parameters Name Type Description Default collection_name str name of mongo collection required vault_secret_path str path on vault server with mongo creds object required Important Environment variables that must be set prior to invocation VAULT_ADDR - URL of vault server (eg. https://matgen8.lbl.gov:8200) VAULT_TOKEN or GITHUB_TOKEN - token used to authenticate to vault Special stores that combine underlying Stores together ConcatStore \u00b6 Store concatting multiple stores last_updated : datetime (property, readonly) \u00b6 Finds the most recent last_updated across all the stores. This might not be the most usefull way to do this for this type of Store since it could very easily over-estimate the last_updated based on what stores are used __eq__ ( self , other ) \u00b6 Show source code in stores/compound_stores.py 513 514 515 516 517 518 519 520 521 522 523 524 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for ConcatStore Args: other: other JointStore to compare with \"\"\" if not isinstance ( other , ConcatStore ): return False fields = [ \"stores\" ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for ConcatStore Parameters Name Type Description Default other object other JointStore to compare with required __init__ ( self , stores , ** kwargs ) \u00b6 Show source code in stores/compound_stores.py 320 321 322 323 324 325 326 327 328 329 330 def __init__ ( self , stores : List [ Store ], ** kwargs ): \"\"\" Initialize a ConcatStore that concatenates multiple stores together to appear as one store Args: stores: list of stores to concatenate together \"\"\" self . stores = stores self . kwargs = kwargs super ( ConcatStore , self ) . __init__ ( ** kwargs ) Initialize a ConcatStore that concatenates multiple stores together to appear as one store Parameters Name Type Description Default stores List[maggma.core.store.Store] list of stores to concatenate together required close ( self ) \u00b6 Show source code in stores/compound_stores.py 349 350 351 352 353 354 def close ( self ): \"\"\" Close all connections in this ConcatStore \"\"\" for store in self . stores : store . close () Close all connections in this ConcatStore connect ( self , force_reset = False ) \u00b6 Show source code in stores/compound_stores.py 339 340 341 342 343 344 345 346 347 def connect ( self , force_reset : bool = False ): \"\"\" Connect all stores in this ConcatStore Args: force_reset: Whether to forcibly reset the connection for all stores \"\"\" for store in self . stores : store . connect ( force_reset ) Connect all stores in this ConcatStore Parameters Name Type Description Default force_reset bool Whether to forcibly reset the connection for all stores False count ( self , criteria = None ) \u00b6 Show source code in stores/compound_stores.py 418 419 420 421 422 423 424 425 426 427 def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" counts = [ store . count ( criteria ) for store in self . stores ] return sum ( counts ) Counts the number of documents matching the query criteria Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None distinct ( self , field , criteria = None , all_exist = False ) \u00b6 Show source code in stores/compound_stores.py 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" distincts = [] for store in self . stores : distincts . extend ( store . distinct ( field = field , criteria = criteria )) return list ( set ( distincts )) Get all distinct values for a field Parameters Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None ensure_index ( self , key , unique = False ) \u00b6 Show source code in stores/compound_stores.py 405 406 407 408 409 410 411 412 413 414 415 416 def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Ensure an index is properly set. Returns whether all stores support this index or not Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created on all stores \"\"\" return all ([ store . ensure_index ( key , unique ) for store in self . stores ]) Ensure an index is properly set. Returns whether all stores support this index or not Parameters Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created on all stores groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in stores/compound_stores.py 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" if isinstance ( keys , str ): keys = [ keys ] docs = [] for store in self . stores : temp_docs = list ( store . groupby ( keys = keys , criteria = criteria , properties = properties , sort = sort , skip = skip , limit = limit , ) ) for key , group in temp_docs : docs . extend ( group ) def key_set ( d : Dict ) -> Tuple : \"index function based on passed in keys\" test_d = tuple ( d . get ( k , None ) for k in keys ) return test_d sorted_docs = sorted ( docs , key = key_set ) for vals , group_iter in groupby ( sorted_docs , key = key_set ): id_dict = { key : val for key , val in zip ( keys , vals )} yield id_dict , list ( group_iter ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) name ( self ) \u00b6 Show source code in stores/compound_stores.py 332 333 334 335 336 337 def name ( self ) -> str : \"\"\" A string representing this data source \"\"\" compound_name = \",\" . join ([ store . name for store in self . stores ]) return f \"Concat[ { compound_name } ]\" A string representing this data source query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in stores/compound_stores.py 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries across all Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" # TODO: skip, sort and limit are broken. implement properly for store in self . stores : for d in store . query ( criteria = criteria , properties = properties ): yield d Queries across all Store for a set of documents Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 remove_docs ( self , criteria ) \u00b6 Show source code in stores/compound_stores.py 504 505 506 507 508 509 510 511 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" raise NotImplementedError ( \"No remove_docs method for JointStore\" ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required update ( self , docs , key = None ) \u00b6 Show source code in stores/compound_stores.py 375 376 377 378 379 380 381 382 383 384 385 386 387 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Not implemented in ConcatStore Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" raise NotImplementedError ( \"No update method for ConcatStore\" ) Update documents into the Store Not implemented in ConcatStore Parameters Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Optional[Union[List, str]] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None JointStore \u00b6 Store that implements a on-the-fly join across multiple collections all in the same MongoDB database. This is a Read-Only Store designed to combine data from multiple collections. last_updated : datetime (property, readonly) \u00b6 Special last_updated for this JointStore that checks all underlying collections nonmaster_names : List (property, readonly) \u00b6 alll non-master collection names __eq__ ( self , other ) \u00b6 Show source code in stores/compound_stores.py 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 def __eq__ ( self , other : object ) -> bool : \"\"\" Check equality for JointStore Args: other: other JointStore to compare with \"\"\" if not isinstance ( other , JointStore ): return False fields = [ \"database\" , \"collection_names\" , \"host\" , \"port\" , \"master\" , \"merge_at_root\" , ] return all ( getattr ( self , f ) == getattr ( other , f ) for f in fields ) Check equality for JointStore Parameters Name Type Description Default other object other JointStore to compare with required __init__ ( self , database , collection_names , host = 'localhost' , port = 27017 , username = '' , password = '' , master = None , merge_at_root = False , ** kwargs ) \u00b6 Show source code in stores/compound_stores.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , database : str , collection_names : List [ str ], host : str = \"localhost\" , port : int = 27017 , username : str = \"\" , password : str = \"\" , master : Optional [ str ] = None , merge_at_root : bool = False , ** kwargs , ): \"\"\" Args: database: The database name collection_names: list of all collections to join host: Hostname for the database port: TCP port to connect to username: Username for the collection password: Password to connect with master: name for the master collection if not specified this defaults to the first in collection_names list \"\"\" self . database = database self . collection_names = collection_names self . host = host self . port = port self . username = username self . password = password self . _collection = None # type: Any self . master = master or collection_names [ 0 ] self . merge_at_root = merge_at_root self . kwargs = kwargs super ( JointStore , self ) . __init__ ( ** kwargs ) Parameters Name Type Description Default database str The database name required collection_names List[str] list of all collections to join required host str Hostname for the database 'localhost' port int TCP port to connect to 27017 username str Username for the collection '' password str Password to connect with '' master Optional[str] name for the master collection if not specified this defaults to the first in collection_names list None close ( self ) \u00b6 Show source code in stores/compound_stores.py 79 80 81 82 83 def close ( self ): \"\"\" Closes underlying database connections \"\"\" self . _collection . database . client . close () Closes underlying database connections connect ( self , force_reset = False ) \u00b6 Show source code in stores/compound_stores.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def connect ( self , force_reset : bool = False ): \"\"\" Connects the underlying Mongo database and all collection connections Args: force_reset: whether to forcibly reset the connection \"\"\" conn = MongoClient ( self . host , self . port ) db = conn [ self . database ] if self . username != \"\" : db . authenticate ( self . username , self . password ) self . _collection = db [ self . master ] self . _has_merge_objects = ( self . _collection . database . client . server_info ()[ \"version\" ] > \"3.6\" ) Connects the underlying Mongo database and all collection connections Parameters Name Type Description Default force_reset bool whether to forcibly reset the connection False count ( self , criteria = None ) \u00b6 Show source code in stores/compound_stores.py 215 216 217 218 219 220 221 222 223 224 225 def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" pipeline = self . _get_pipeline ( criteria = criteria ) pipeline . append ({ \"$count\" : \"count\" }) agg = list ( self . _collection . aggregate ( pipeline )) return agg [ 0 ] . get ( \"count\" , 0 ) if len ( agg ) > 0 else 0 Counts the number of documents matching the query criteria Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None ensure_index ( self , key , unique = False , ** kwargs ) \u00b6 Show source code in stores/compound_stores.py 130 131 132 133 134 def ensure_index ( self , key , unique = False , ** kwargs ): \"\"\" Can't ensure index for JointStore \"\"\" raise NotImplementedError ( \"No ensure_index method for JointStore\" ) Can't ensure index for JointStore groupby ( self , keys , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in stores/compound_stores.py 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: pipeline = self . _get_pipeline ( criteria = criteria , properties = properties , skip = skip , limit = limit ) if not isinstance ( keys , list ): keys = [ keys ] group_id = {} # type: Dict[str,Any] for key in keys : set_ ( group_id , key , \"$ {} \" . format ( key )) pipeline . append ({ \"$group\" : { \"_id\" : group_id , \"docs\" : { \"$push\" : \"$$ROOT\" }}}) agg = self . _collection . aggregate ( pipeline ) for d in agg : yield d [ \"_id\" ], d [ \"docs\" ] Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs) name ( self ) \u00b6 Show source code in stores/compound_stores.py 55 56 57 58 59 60 def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" compound_name = \",\" . join ( self . collection_names ) return f \"Compound[ { self . host } / { self . database } ][ { compound_name } ]\" Return a string representing this data source query ( self , criteria = None , properties = None , sort = None , skip = 0 , limit = 0 ) \u00b6 Show source code in stores/compound_stores.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: pipeline = self . _get_pipeline ( criteria = criteria , properties = properties , skip = skip , limit = limit ) agg = self . _collection . aggregate ( pipeline ) for d in agg : yield d Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 query_one ( self , criteria = None , properties = None , ** kwargs ) \u00b6 Show source code in stores/compound_stores.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 def query_one ( self , criteria = None , properties = None , ** kwargs ): \"\"\" Get one document Args: properties: properties to return in query criteria: filter for matching kwargs: kwargs for collection.aggregate Returns: single document \"\"\" # TODO: maybe adding explicit limit in agg pipeline is better as below? # pipeline = self._get_pipeline(properties, criteria) # pipeline.append({\"$limit\": 1}) query = self . query ( criteria = criteria , properties = properties , ** kwargs ) try : doc = next ( query ) return doc except StopIteration : return None Get one document Parameters Name Type Description Default properties properties to return in query None criteria filter for matching None **kwargs kwargs for collection.aggregate {} Returns Type Description `` single document remove_docs ( self , criteria ) \u00b6 Show source code in stores/compound_stores.py 288 289 290 291 292 293 294 295 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" raise NotImplementedError ( \"No remove_docs method for JointStore\" ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required update ( self , docs , update_lu = True , key = None , ** kwargs ) \u00b6 Show source code in stores/compound_stores.py 115 116 117 118 119 120 def update ( self , docs , update_lu = True , key = None , ** kwargs ): \"\"\" Update documents into the underlying collections Not Implemented for JointStore \"\"\" raise NotImplementedError ( \"JointStore is a read-only store\" ) Update documents into the underlying collections Not Implemented for JointStore","title":"Stores"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore","text":"A Store for access to a single or multiple JSON files","title":"JSONStore"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.connect","text":"Show source code in stores/mongolike.py 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 def connect ( self , force_reset = False , ssh_tunnel = None ): # lgtm[py/conflicting-attributes] \"\"\" Loads the files into the collection in memory \"\"\" if ssh_tunnel is not None : warnings . warn ( f \"SSH Tunnel not needed for { self . __name__ } \" ) super () . connect ( force_reset = force_reset ) for path in self . paths : with zopen ( path ) as f : data = f . read () data = data . decode () if isinstance ( data , bytes ) else data objects = json . loads ( data ) objects = [ objects ] if not isinstance ( objects , list ) else objects self . update ( objects ) Loads the files into the collection in memory","title":"connect()"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore","text":"An in-memory Store that functions similarly to a MongoStore","title":"MemoryStore"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.name","text":"Name for the store","title":"name"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.connect","text":"Show source code in stores/mongolike.py 387 388 389 390 391 392 393 394 395 396 def connect ( self , force_reset : bool = False , ssh_tunnel : SSHTunnelForwarder = None ): # lgtm[py/conflicting-attributes] \"\"\" Connect to the source data \"\"\" if ssh_tunnel is not None : warnings . warn ( f \"SSH Tunnel not needed for { self . __class__ . __name__ } \" ) if not self . _collection or force_reset : self . _collection = mongomock . MongoClient () . db [ self . name ] Connect to the source data","title":"connect()"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.groupby","text":"Show source code in stores/mongolike.py 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (key, list of elemnts) \"\"\" keys = keys if isinstance ( keys , list ) else [ keys ] data = [ doc for doc in self . query ( properties = keys , criteria = criteria ) if all ( has ( doc , k ) for k in keys ) ] def grouping_keys ( doc ): return tuple ( get ( doc , k ) for k in keys ) for vals , group in groupby ( sorted ( data , key = grouping_keys ), key = grouping_keys ): doc = {} # type: Dict[Any,Any] for k , v in zip ( keys , vals ): set_ ( doc , k , v ) yield doc , list ( group ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (key, list of elemnts)","title":"groupby()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore","text":"A Store that connects to a Mongo collection","title":"MongoStore"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.name","text":"Return a string representing this data source","title":"name"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.close","text":"Show source code in stores/mongolike.py 311 312 313 def close ( self ): \"\"\" Close up all collections \"\"\" self . _collection . database . client . close () Close up all collections","title":"close()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.connect","text":"Show source code in stores/mongolike.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def connect ( self , force_reset : bool = False , ssh_tunnel : SSHTunnelForwarder = None ): # lgtm[py/conflicting-attributes] \"\"\" Connect to the source data \"\"\" if not self . _collection or force_reset : if ssh_tunnel is None : conn = MongoClient ( self . host , self . port ) else : conn = MongoClient ( * ssh_tunnel . local_bind_address ) db = conn [ self . database ] if self . username != \"\" : db . authenticate ( self . username , self . password ) self . _collection = db [ self . collection_name ] Connect to the source data","title":"connect()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.count","text":"Show source code in stores/mongolike.py 197 198 199 200 201 202 203 204 205 206 def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" criteria = criteria if criteria else {} return self . _collection . find ( filter = criteria ) . count () Counts the number of documents matching the query criteria Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None","title":"count()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.distinct","text":"Show source code in stores/mongolike.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria or {} distinct_vals = self . _collection . distinct ( field , criteria ) return distinct_vals if distinct_vals is not None else [] Get all distinct values for a field Parameters Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None","title":"distinct()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.ensure_index","text":"Show source code in stores/mongolike.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 def ensure_index ( self , key : str , unique : Optional [ bool ] = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" if confirm_field_index ( self . _collection , key ): return True else : try : self . _collection . create_index ( key , unique = unique , background = True ) return True except Exception : return False Tries to create an index and return true if it suceeded Parameters Name Type Description Default key str single key to index required unique Optional[bool] Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.from_collection","text":"Show source code in stores/mongolike.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 @classmethod def from_collection ( cls , collection ): \"\"\" Generates a MongoStore from a pymongo collection object This is not a fully safe operation as it gives dummy information to the MongoStore As a result, this will not serialize and can not reset its connection Args: collection: the PyMongo collection to create a MongoStore around \"\"\" # TODO: How do we make this safer? coll_name = collection . name db_name = collection . database . name store = cls ( db_name , coll_name ) store . _collection = collection return store Generates a MongoStore from a pymongo collection object This is not a fully safe operation as it gives dummy information to the MongoStore As a result, this will not serialize and can not reset its connection Parameters Name Type Description Default collection the PyMongo collection to create a MongoStore around required","title":"from_collection()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.from_db_file","text":"Show source code in stores/mongolike.py 93 94 95 96 97 98 99 100 101 102 103 104 @classmethod def from_db_file ( cls , filename : str ): \"\"\" Convenience method to construct MongoStore from db_file from old QueryEngine format \"\"\" kwargs = loadfn ( filename ) if \"collection\" in kwargs : kwargs [ \"collection_name\" ] = kwargs . pop ( \"collection\" ) # Get rid of aliases from traditional query engine db docs kwargs . pop ( \"aliases\" , None ) return cls ( ** kwargs ) Convenience method to construct MongoStore from db_file from old QueryEngine format","title":"from_db_file()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.groupby","text":"Show source code in stores/mongolike.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (key, list of docs) \"\"\" pipeline = [] if isinstance ( keys , str ): keys = [ keys ] if properties is None : properties = [] if isinstance ( properties , dict ): properties = list ( properties . keys ()) if criteria is not None : pipeline . append ({ \"$match\" : criteria }) if len ( properties ) > 0 : pipeline . append ({ \"$project\" : { p : 1 for p in properties + keys }}) alpha = \"abcdefghijklmnopqrstuvwxyz\" group_id = { letter : f \"$ { key } \" for letter , key in zip ( alpha , keys )} pipeline . append ({ \"$group\" : { \"_id\" : group_id , \"docs\" : { \"$push\" : \"$$ROOT\" }}}) for d in self . _collection . aggregate ( pipeline , allowDiskUse = True ): id_doc = {} # type: Dict[str,Any] for letter , key in group_id . items (): if has ( d [ \"_id\" ], letter ): set_ ( id_doc , key [ 1 :], d [ \"_id\" ][ letter ]) yield ( id_doc , d [ \"docs\" ]) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (key, list of docs)","title":"groupby()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.query","text":"Show source code in stores/mongolike.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( properties , list ): properties = { p : 1 for p in properties } sort_list = [( k , v . value ) for k , v in sort . items ()] if sort else None for d in self . _collection . find ( filter = criteria , projection = properties , skip = skip , limit = limit , sort = sort_list , ): yield d Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0","title":"query()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.remove_docs","text":"Show source code in stores/mongolike.py 302 303 304 305 306 307 308 309 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" self . _collection . delete_many ( filter = criteria ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.update","text":"Show source code in stores/mongolike.py 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" requests = [] if not isinstance ( docs , list ): docs = [ docs ] for d in docs : d = jsanitize ( d , allow_bson = True ) # document-level validation is optional validates = True if self . validator : validates = self . validator . is_valid ( d ) if not validates : if self . validator . strict : raise ValueError ( self . validator . validation_errors ( d )) else : self . logger . error ( self . validator . validation_errors ( d )) if validates : key = key or self . key if isinstance ( key , list ): search_doc = { k : d [ k ] for k in key } else : search_doc = { key : d [ key ]} requests . append ( ReplaceOne ( search_doc , d , upsert = True )) if len ( requests ) > 0 : self . _collection . bulk_write ( requests , ordered = False ) Update documents into the Store Parameters Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str, None] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None","title":"update()"},{"location":"reference/stores/#maggma.stores.mongolike.MongoURIStore","text":"A Store that connects to a Mongo collection via a URI This is expected to be a special mongodb+srv:// URIs that include client parameters via TXT records","title":"MongoURIStore"},{"location":"reference/stores/#maggma.stores.mongolike.MongoURIStore.name","text":"Return a string representing this data source","title":"name"},{"location":"reference/stores/#maggma.stores.mongolike.MongoURIStore.connect","text":"Show source code in stores/mongolike.py 356 357 358 359 360 361 362 363 364 365 366 367 def connect ( self , force_reset : bool = False , ssh_tunnel : SSHTunnelForwarder = None ): # lgtm[py/conflicting-attributes] \"\"\" Connect to the source data \"\"\" if ssh_tunnel is not None : warnings . warn ( f \"SSH Tunnel not needed for { self . __class__ . __name__ } \" ) if not self . _collection or force_reset : conn = MongoClient ( self . uri ) db = conn [ self . database ] self . _collection = db [ self . collection_name ] Connect to the source data Module containing various definitions of Stores. Stores are a default access pattern to data and provide various utillities","title":"connect()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore","text":"A Store for GrdiFS backend. Provides a common access method consistent with other stores","title":"GridFSStore"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.last_updated","text":"Provides the most recent last_updated date time stamp from the documents in this Store","title":"last_updated"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.close","text":"Show source code in stores/gridfs.py 339 340 def close ( self ): self . _collection . database . client . close () Closes any connections","title":"close()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.connect","text":"Show source code in stores/gridfs.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" conn = MongoClient ( self . host , self . port ) if not self . _collection or force_reset : db = conn [ self . database ] if self . username != \"\" : db . authenticate ( self . username , self . password ) self . _collection = gridfs . GridFS ( db , self . collection_name ) self . _files_collection = db [ \" {} .files\" . format ( self . collection_name )] self . _files_store = MongoStore . from_collection ( self . _files_collection ) self . _files_store . last_updated_field = f \"metadata. { self . last_updated_field } \" self . _chunks_collection = db [ \" {} .chunks\" . format ( self . collection_name )] Connect to the source data","title":"connect()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.count","text":"Show source code in stores/gridfs.py 136 137 138 139 140 141 142 143 144 145 146 def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) return self . _files_store . count ( criteria ) Counts the number of documents matching the query criteria Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None","title":"count()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.distinct","text":"Show source code in stores/gridfs.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = ( self . transform_criteria ( criteria ) if isinstance ( criteria , dict ) else criteria ) field = ( f \"metadata. { field } \" if field not in self . files_collection_fields and not field . startswith ( \"metadata.\" ) else field ) return self . _files_store . distinct ( field = field , criteria = criteria ) Get all distinct values for a field Parameters Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None","title":"distinct()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.ensure_index","text":"Show source code in stores/gridfs.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 def ensure_index ( self , key : str , unique : Optional [ bool ] = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Currently operators on the GridFS files collection Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" # Transform key for gridfs first if key not in self . files_collection_fields : files_col_key = \"metadata. {} \" . format ( key ) return self . _files_store . ensure_index ( files_col_key , unique = unique ) else : return self . _files_store . ensure_index ( key , unique = unique ) Tries to create an index and return true if it suceeded Currently operators on the GridFS files collection Parameters Name Type Description Default key str single key to index required unique Optional[bool] Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.groupby","text":"Show source code in stores/gridfs.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Will only work if the keys are included in the files collection for GridFS Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" criteria = ( self . transform_criteria ( criteria ) if isinstance ( criteria , dict ) else criteria ) keys = [ keys ] if not isinstance ( keys , list ) else keys keys = [ f \"metadata. { k } \" if k not in self . files_collection_fields and not k . startswith ( \"metadata.\" ) else k for k in keys ] for group , ids in self . _files_store . groupby ( keys , criteria = criteria , properties = [ f \"metadata. { self . key } \" ] ): ids = [ get ( doc , f \"metadata. { self . key } \" ) for doc in ids if has ( doc , f \"metadata. { self . key } \" ) ] group = { k . replace ( \"metadata.\" , \"\" ): get ( group , k ) for k in keys if has ( group , k ) } yield group , list ( self . query ( criteria = { self . key : { \"$in\" : ids }})) Simple grouping function that will group documents by keys. Will only work if the keys are included in the files collection for GridFS Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs)","title":"groupby()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.name","text":"Show source code in stores/gridfs.py 83 84 85 86 87 def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" return f \"gridfs:// { self . host } / { self . database } / { self . collection_name } \" Return a string representing this data source","title":"name()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.query","text":"Show source code in stores/gridfs.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the GridFS Store for a set of documents Currently ignores properties TODO: If properties wholy in metadata, just query that Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) for f in self . _collection . find ( filter = criteria , skip = skip , limit = limit , sort = sort ): data = f . read () metadata = f . metadata if metadata . get ( \"compression\" , \"\" ) == \"zlib\" : data = zlib . decompress ( data ) . decode ( \"UTF-8\" ) try : data = json . loads ( data ) except Exception : pass yield data Queries the GridFS Store for a set of documents Currently ignores properties TODO: If properties wholy in metadata, just query that Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Union[Dict, List, None] properties to return in grouped documents None sort Optional[Dict[str, Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0","title":"query()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.remove_docs","text":"Show source code in stores/gridfs.py 325 326 327 328 329 330 331 332 333 334 335 336 337 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" if isinstance ( criteria , dict ): criteria = self . transform_criteria ( criteria ) ids = [ cursor . _id for cursor in self . _collection . find ( criteria )] for id in ids : self . _collection . delete ( id ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.transform_criteria","text":"Show source code in stores/gridfs.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 @classmethod def transform_criteria ( cls , criteria : Dict ) -> Dict : \"\"\" Allow client to not need to prepend 'metadata.' to query fields. Args: criteria: Query criteria \"\"\" new_criteria = dict () for field in criteria : if field not in cls . files_collection_fields and not field . startswith ( \"metadata.\" ): new_criteria [ \"metadata.\" + field ] = copy . copy ( criteria [ field ]) else : new_criteria [ field ] = copy . copy ( criteria [ field ]) return new_criteria Allow client to not need to prepend 'metadata.' to query fields. Parameters Name Type Description Default criteria Dict Query criteria required","title":"transform_criteria()"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.update","text":"Show source code in stores/gridfs.py 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" if not isinstance ( docs , list ): docs = [ docs ] if isinstance ( key , str ): key = [ key ] elif not key : key = [ self . key ] key = list ( set ( key ) | self . meta_keys - set ( self . files_collection_fields )) for d in docs : search_doc = { k : d [ k ] for k in key } metadata = { k : d [ k ] for k in [ self . last_updated_field ] if k in d } metadata . update ( search_doc ) data = json . dumps ( jsanitize ( d )) . encode ( \"UTF-8\" ) if self . compression : data = zlib . compress ( data ) metadata [ \"compression\" ] = \"zlib\" self . _collection . put ( data , metadata = metadata ) search_doc = self . transform_criteria ( search_doc ) # Cleans up old gridfs entries for fdoc in ( self . _files_collection . find ( search_doc , [ \"_id\" ]) . sort ( \"uploadDate\" , - 1 ) . skip ( 1 ) ): self . _collection . delete ( fdoc [ \"_id\" ]) Update documents into the Store Parameters Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Union[List, str, None] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Advanced Stores for connecting to AWS data","title":"update()"},{"location":"reference/stores/#maggma.stores.aws.S3Store","text":"GridFS like storage using Amazon S3 and a regular store for indexing Assumes Amazon AWS key and secret key are set in environment or default config file","title":"S3Store"},{"location":"reference/stores/#maggma.stores.aws.S3Store.last_updated","text":"Provides the most recent last_updated date time stamp from the documents in this Store","title":"last_updated"},{"location":"reference/stores/#maggma.stores.aws.S3Store.close","text":"Show source code in stores/aws.py 89 90 91 92 93 94 95 def close ( self ): \"\"\" Closes any connections \"\"\" self . index . close () self . s3 = None self . s3_bucket = None Closes any connections","title":"close()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.connect","text":"Show source code in stores/aws.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def connect ( self , force_reset : bool = False ): \"\"\" Connect to the source data \"\"\" self . index . connect ( force_reset = force_reset ) session = Session ( profile_name = self . s3_profile ) resource = session . resource ( \"s3\" , endpoint_url = self . endpoint_url ) if not self . s3 : self . s3 = resource if self . bucket not in [ bucket . name for bucket in self . s3 . buckets . all ()]: raise Exception ( \"Bucket not present on AWS: {} \" . format ( self . bucket )) self . s3_bucket = resource . Bucket ( self . bucket ) Connect to the source data","title":"connect()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.count","text":"Show source code in stores/aws.py 110 111 112 113 114 115 116 117 118 def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" return self . index . count ( criteria ) Counts the number of documents matching the query criteria Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None","title":"count()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.distinct","text":"Show source code in stores/aws.py 173 174 175 176 177 178 179 180 181 182 183 184 def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" # Index is a store so it should have its own distinct function return self . index . distinct ( field , criteria = criteria ) Get all distinct values for a field Parameters Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None","title":"distinct()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.ensure_index","text":"Show source code in stores/aws.py 219 220 221 222 223 224 225 226 227 228 229 230 def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Tries to create an index and return true if it suceeded Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created \"\"\" return self . index . ensure_index ( key , unique = unique ) Tries to create an index and return true if it suceeded Parameters Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.groupby","text":"Show source code in stores/aws.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" return self . index . groupby ( keys = keys , criteria = criteria , properties = properties , sort = sort , skip = skip , limit = limit , ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs)","title":"groupby()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.name","text":"Show source code in stores/aws.py 66 67 68 69 70 71 def name ( self ) -> str : \"\"\" Returns: a string representing this data source \"\"\" return f \"s3:// { self . bucket } \" Returns Type Description str a string representing this data source","title":"name()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.newer_in","text":"Show source code in stores/aws.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 def newer_in ( self , target : Store , criteria : Optional [ Dict ] = None , exhaustive : bool = False ) -> List [ str ]: \"\"\" Returns the keys of documents that are newer in the target Store than this Store. Args: target: target Store criteria: PyMongo filter for documents to search in exhaustive: triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in \"\"\" return self . index . newer_in ( target = target , criteria = criteria , exhaustive = exhaustive ) Returns the keys of documents that are newer in the target Store than this Store. Parameters Name Type Description Default target Store target Store required criteria Optional[Dict] PyMongo filter for documents to search in None exhaustive bool triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in False","title":"newer_in()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.query","text":"Show source code in stores/aws.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" prop_keys = set () if isinstance ( properties , dict ): prop_keys = set ( properties . keys ()) elif isinstance ( properties , list ): prop_keys = set ( properties ) for doc in self . index . query ( criteria = criteria , sort = sort , limit = limit , skip = skip ): if properties is not None and prop_keys . issubset ( set ( doc . keys ())): yield { p : doc [ p ] for p in properties if p in doc } else : try : # TODO: THis is ugly and unsafe, do some real checking before pulling data data = ( self . s3_bucket . Object ( self . sub_dir + doc [ self . key ]) . get ()[ \"Body\" ] . read () ) except botocore . exceptions . ClientError as e : # If a client error is thrown, then check that it was a 404 error. # If it was a 404 error, then the object does not exist. error_code = int ( e . response [ \"Error\" ][ \"Code\" ]) if error_code == 404 : self . logger . error ( \"Could not find S3 object {} \" . format ( doc [ self . key ]) ) break else : raise e if doc . get ( \"compression\" , \"\" ) == \"zlib\" : data = zlib . decompress ( data ) yield json . loads ( data ) Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0","title":"query()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.rebuild_index_from_s3_data","text":"Show source code in stores/aws.py 322 323 324 325 326 327 328 329 330 331 332 def rebuild_index_from_s3_data ( self ): \"\"\" Rebuilds the index Store from the data in S3 Relies on the index document being stores as the metadata for the file \"\"\" index_docs = [] for file in self . s3_bucket . objects . all (): # TODO: Transform the data back from strings and remove AWS S3 specific keys index_docs . append ( file . metadata ) self . index . update ( index_docs ) Rebuilds the index Store from the data in S3 Relies on the index document being stores as the metadata for the file","title":"rebuild_index_from_s3_data()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.remove_docs","text":"Show source code in stores/aws.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def remove_docs ( self , criteria : Dict , remove_s3_object : bool = False ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match remove_s3_object: whether to remove the actual S3 Object or not \"\"\" if not remove_s3_object : self . index . remove_docs ( criteria = criteria ) else : to_remove = self . index . distinct ( self . key , criteria = criteria ) self . index . remove_docs ( criteria = criteria ) # Can remove up to 1000 items at a time via boto to_remove_chunks = list ( grouper ( to_remove , n = 1000 )) for chunk_to_remove in to_remove_chunks : objlist = [{ \"Key\" : self . sub_dir + obj } for obj in chunk_to_remove ] self . s3_bucket . delete_objects ( Delete = { \"Objects\" : objlist }) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required remove_s3_object bool whether to remove the actual S3 Object or not False","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.aws.S3Store.update","text":"Show source code in stores/aws.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" search_docs = [] search_keys = [] if isinstance ( key , list ): search_keys = key elif key : search_keys = [ key ] else : search_keys = [ self . key ] for d in docs : search_doc = { k : d [ k ] for k in search_keys } search_doc [ self . key ] = d [ self . key ] # Ensure key is in metadata if self . sub_dir != \"\" : search_doc [ \"sub_dir\" ] = self . sub_dir # Remove MongoDB _id from search if \"_id\" in search_doc : del search_doc [ \"_id\" ] data = json . dumps ( jsanitize ( d )) . encode () # Compress with zlib if chosen if self . compress : search_doc [ \"compression\" ] = \"zlib\" data = zlib . compress ( data ) self . s3_bucket . put_object ( Key = self . sub_dir + str ( d [ self . key ]), Body = data , Metadata = search_doc ) search_docs . append ( search_doc ) # Use store's update to remove key clashes self . index . update ( search_docs ) Update documents into the Store Parameters Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Optional[Union[List, str]] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None Advanced Stores for behavior outside normal access patterns","title":"update()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore","text":"Special Store that aliases for the primary accessors","title":"AliasingStore"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.close","text":"Show source code in stores/advanced_stores.py 366 367 def close ( self ): self . store . close () Closes any connections","title":"close()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.connect","text":"Show source code in stores/advanced_stores.py 373 374 def connect ( self , force_reset = False ): self . store . connect ( force_reset = force_reset ) Connect to the source data Parameters Name Type Description Default force_reset whether to reset the connection or not False","title":"connect()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.count","text":"Show source code in stores/advanced_stores.py 225 226 227 228 229 230 231 232 233 234 def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" criteria = criteria if criteria else {} lazy_substitute ( criteria , self . reverse_aliases ) return self . store . count ( criteria ) Counts the number of documents matching the query criteria Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None","title":"count()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.distinct","text":"Show source code in stores/advanced_stores.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" criteria = criteria if criteria else {} lazy_substitute ( criteria , self . reverse_aliases ) # substitute forward return self . store . distinct ( self . aliases [ field ], criteria = criteria ) Get all distinct values for a field Parameters Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None","title":"distinct()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.ensure_index","text":"Show source code in stores/advanced_stores.py 361 362 363 364 def ensure_index ( self , key , unique = False , ** kwargs ): if key in self . aliases : key = self . aliases return self . store . ensure_index ( key , unique , ** kwargs ) Tries to create an index and return true if it suceeded Parameters Name Type Description Default key single key to index required unique Whether or not this index contains only unique keys False Returns Type Description `` bool indicating if the index exists/was created","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.groupby","text":"Show source code in stores/advanced_stores.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" # Convert to a list keys = keys if isinstance ( keys , list ) else [ keys ] # Make the aliasing transformations on keys keys = [ self . aliases [ k ] if k in self . aliases else k for k in keys ] # Update criteria and properties based on aliases criteria = criteria if criteria else {} if properties is not None : if isinstance ( properties , list ): properties = { p : 1 for p in properties } substitute ( properties , self . reverse_aliases ) lazy_substitute ( criteria , self . reverse_aliases ) return self . store . groupby ( keys = keys , properties = properties , criteria = criteria , skip = skip , limit = limit ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs)","title":"groupby()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.name","text":"Show source code in stores/advanced_stores.py 219 220 221 222 223 def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" return self . store . name Return a string representing this data source","title":"name()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.query","text":"Show source code in stores/advanced_stores.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" criteria = criteria if criteria else {} if properties is not None : if isinstance ( properties , list ): properties = { p : 1 for p in properties } substitute ( properties , self . reverse_aliases ) lazy_substitute ( criteria , self . reverse_aliases ) for d in self . store . query ( properties = properties , criteria = criteria , sort = sort , limit = limit , skip = skip ): substitute ( d , self . aliases ) yield d Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0","title":"query()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.remove_docs","text":"Show source code in stores/advanced_stores.py 350 351 352 353 354 355 356 357 358 359 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" # Update criteria and properties based on aliases lazy_substitute ( criteria , self . reverse_aliases ) self . store . remove_docs ( criteria ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.update","text":"Show source code in stores/advanced_stores.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" key = key if key else self . key for d in docs : substitute ( d , self . reverse_aliases ) if key in self . aliases : key = self . aliases [ key ] self . store . update ( docs , key = key ) Update documents into the Store Parameters Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Optional[Union[List, str]] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None","title":"update()"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore","text":"Initialize a Store with a mongogrant \" <role> : <host> / <db> .\" spec. Some class methods of MongoStore, e.g. from_db_file and from_collection, are not supported. mongogrant documentation: https://github.com/materialsproject/mongogrant","title":"MongograntStore"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.name","text":"Return a string representing this data source","title":"name"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.from_collection","text":"Show source code in stores/advanced_stores.py 94 95 96 97 98 99 @classmethod def from_collection ( cls , collection ): \"\"\" Raises ValueError since MongograntStores can't be initialized from a PyMongo collection \"\"\" raise ValueError ( \"MongograntStore doesn't implement from_collection\" ) Raises ValueError since MongograntStores can't be initialized from a PyMongo collection","title":"from_collection()"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.from_db_file","text":"Show source code in stores/advanced_stores.py 87 88 89 90 91 92 @classmethod def from_db_file ( cls , file ): \"\"\" Raises ValueError since MongograntStores can't be initialized from a file \"\"\" raise ValueError ( \"MongograntStore doesn't implement from_db_file\" ) Raises ValueError since MongograntStores can't be initialized from a file","title":"from_db_file()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore","text":"Provides a sandboxed view to another store","title":"SandboxStore"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.sbx_criteria","text":"Returns Type Description Dict the sandbox criteria dict used to filter the source store","title":"sbx_criteria"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.close","text":"Show source code in stores/advanced_stores.py 536 537 def close ( self ): self . store . close () Closes any connections","title":"close()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.connect","text":"Show source code in stores/advanced_stores.py 543 544 def connect ( self , force_reset = False ): self . store . connect ( force_reset = force_reset ) Connect to the source data Parameters Name Type Description Default force_reset whether to reset the connection or not False","title":"connect()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.count","text":"Show source code in stores/advanced_stores.py 432 433 434 435 436 437 438 439 440 441 442 def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . count ( criteria = criteria ) Counts the number of documents matching the query criteria Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None","title":"count()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.ensure_index","text":"Show source code in stores/advanced_stores.py 533 534 def ensure_index ( self , key , unique = False , ** kwargs ): return self . store . ensure_index ( key , unique , ** kwargs ) Tries to create an index and return true if it suceeded Parameters Name Type Description Default key single key to index required unique Whether or not this index contains only unique keys False Returns Type Description `` bool indicating if the index exists/was created","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.groupby","text":"Show source code in stores/advanced_stores.py 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . groupby ( keys = keys , properties = properties , criteria = criteria , skip = skip , limit = limit ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs)","title":"groupby()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.name","text":"Show source code in stores/advanced_stores.py 412 413 414 415 416 417 def name ( self ) -> str : \"\"\" Returns: a string representing this data source \"\"\" return f \"Sandbox[ { self . store . name } ][ { self . sandbox } ]\" Returns Type Description str a string representing this data source","title":"name()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.query","text":"Show source code in stores/advanced_stores.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries the Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) return self . store . query ( properties = properties , criteria = criteria , sort = sort , limit = limit , skip = skip ) Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0","title":"query()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.remove_docs","text":"Show source code in stores/advanced_stores.py 520 521 522 523 524 525 526 527 528 529 530 531 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" # Update criteria and properties based on aliases criteria = ( dict ( ** criteria , ** self . sbx_criteria ) if criteria else self . sbx_criteria ) self . store . remove_docs ( criteria ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.update","text":"Show source code in stores/advanced_stores.py 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" for d in docs : if \"sbxn\" in d : d [ \"sbxn\" ] = list ( set ( d [ \"sbxn\" ] + [ self . sandbox ])) else : d [ \"sbxn\" ] = [ self . sandbox ] self . store . update ( docs , key = key ) Update documents into the Store Parameters Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Optional[Union[List, str]] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None","title":"update()"},{"location":"reference/stores/#maggma.stores.advanced_stores.VaultStore","text":"Extends MongoStore to read credentials out of Vault server and uses these values to initialize MongoStore instance","title":"VaultStore"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore","text":"Store concatting multiple stores","title":"ConcatStore"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.last_updated","text":"Finds the most recent last_updated across all the stores. This might not be the most usefull way to do this for this type of Store since it could very easily over-estimate the last_updated based on what stores are used","title":"last_updated"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.close","text":"Show source code in stores/compound_stores.py 349 350 351 352 353 354 def close ( self ): \"\"\" Close all connections in this ConcatStore \"\"\" for store in self . stores : store . close () Close all connections in this ConcatStore","title":"close()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.connect","text":"Show source code in stores/compound_stores.py 339 340 341 342 343 344 345 346 347 def connect ( self , force_reset : bool = False ): \"\"\" Connect all stores in this ConcatStore Args: force_reset: Whether to forcibly reset the connection for all stores \"\"\" for store in self . stores : store . connect ( force_reset ) Connect all stores in this ConcatStore Parameters Name Type Description Default force_reset bool Whether to forcibly reset the connection for all stores False","title":"connect()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.count","text":"Show source code in stores/compound_stores.py 418 419 420 421 422 423 424 425 426 427 def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" counts = [ store . count ( criteria ) for store in self . stores ] return sum ( counts ) Counts the number of documents matching the query criteria Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None","title":"count()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.distinct","text":"Show source code in stores/compound_stores.py 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 def distinct ( self , field : str , criteria : Optional [ Dict ] = None , all_exist : bool = False ) -> List : \"\"\" Get all distinct values for a field Args: field: the field(s) to get distinct values for criteria: PyMongo filter for documents to search in \"\"\" distincts = [] for store in self . stores : distincts . extend ( store . distinct ( field = field , criteria = criteria )) return list ( set ( distincts )) Get all distinct values for a field Parameters Name Type Description Default field str the field(s) to get distinct values for required criteria Optional[Dict] PyMongo filter for documents to search in None","title":"distinct()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.ensure_index","text":"Show source code in stores/compound_stores.py 405 406 407 408 409 410 411 412 413 414 415 416 def ensure_index ( self , key : str , unique : bool = False ) -> bool : \"\"\" Ensure an index is properly set. Returns whether all stores support this index or not Args: key: single key to index unique: Whether or not this index contains only unique keys Returns: bool indicating if the index exists/was created on all stores \"\"\" return all ([ store . ensure_index ( key , unique ) for store in self . stores ]) Ensure an index is properly set. Returns whether all stores support this index or not Parameters Name Type Description Default key str single key to index required unique bool Whether or not this index contains only unique keys False Returns Type Description bool bool indicating if the index exists/was created on all stores","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.groupby","text":"Show source code in stores/compound_stores.py 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: \"\"\" Simple grouping function that will group documents by keys. Args: keys: fields to group documents criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned Returns: generator returning tuples of (dict, list of docs) \"\"\" if isinstance ( keys , str ): keys = [ keys ] docs = [] for store in self . stores : temp_docs = list ( store . groupby ( keys = keys , criteria = criteria , properties = properties , sort = sort , skip = skip , limit = limit , ) ) for key , group in temp_docs : docs . extend ( group ) def key_set ( d : Dict ) -> Tuple : \"index function based on passed in keys\" test_d = tuple ( d . get ( k , None ) for k in keys ) return test_d sorted_docs = sorted ( docs , key = key_set ) for vals , group_iter in groupby ( sorted_docs , key = key_set ): id_dict = { key : val for key , val in zip ( keys , vals )} yield id_dict , list ( group_iter ) Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs)","title":"groupby()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.name","text":"Show source code in stores/compound_stores.py 332 333 334 335 336 337 def name ( self ) -> str : \"\"\" A string representing this data source \"\"\" compound_name = \",\" . join ([ store . name for store in self . stores ]) return f \"Concat[ { compound_name } ]\" A string representing this data source","title":"name()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.query","text":"Show source code in stores/compound_stores.py 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: \"\"\" Queries across all Store for a set of documents Args: criteria: PyMongo filter for documents to search in properties: properties to return in grouped documents sort: Dictionary of sort order for fields skip: number documents to skip limit: limit on total number of documents returned \"\"\" # TODO: skip, sort and limit are broken. implement properly for store in self . stores : for d in store . query ( criteria = criteria , properties = properties ): yield d Queries across all Store for a set of documents Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0","title":"query()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.remove_docs","text":"Show source code in stores/compound_stores.py 504 505 506 507 508 509 510 511 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" raise NotImplementedError ( \"No remove_docs method for JointStore\" ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.update","text":"Show source code in stores/compound_stores.py 375 376 377 378 379 380 381 382 383 384 385 386 387 def update ( self , docs : Union [ List [ Dict ], Dict ], key : Union [ List , str , None ] = None ): \"\"\" Update documents into the Store Not implemented in ConcatStore Args: docs: the document or list of documents to update key: field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used \"\"\" raise NotImplementedError ( \"No update method for ConcatStore\" ) Update documents into the Store Not implemented in ConcatStore Parameters Name Type Description Default docs Union[List[Dict], Dict] the document or list of documents to update required key Optional[Union[List, str]] field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used None","title":"update()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore","text":"Store that implements a on-the-fly join across multiple collections all in the same MongoDB database. This is a Read-Only Store designed to combine data from multiple collections.","title":"JointStore"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.last_updated","text":"Special last_updated for this JointStore that checks all underlying collections","title":"last_updated"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.nonmaster_names","text":"alll non-master collection names","title":"nonmaster_names"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.close","text":"Show source code in stores/compound_stores.py 79 80 81 82 83 def close ( self ): \"\"\" Closes underlying database connections \"\"\" self . _collection . database . client . close () Closes underlying database connections","title":"close()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.connect","text":"Show source code in stores/compound_stores.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def connect ( self , force_reset : bool = False ): \"\"\" Connects the underlying Mongo database and all collection connections Args: force_reset: whether to forcibly reset the connection \"\"\" conn = MongoClient ( self . host , self . port ) db = conn [ self . database ] if self . username != \"\" : db . authenticate ( self . username , self . password ) self . _collection = db [ self . master ] self . _has_merge_objects = ( self . _collection . database . client . server_info ()[ \"version\" ] > \"3.6\" ) Connects the underlying Mongo database and all collection connections Parameters Name Type Description Default force_reset bool whether to forcibly reset the connection False","title":"connect()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.count","text":"Show source code in stores/compound_stores.py 215 216 217 218 219 220 221 222 223 224 225 def count ( self , criteria : Optional [ Dict ] = None ) -> int : \"\"\" Counts the number of documents matching the query criteria Args: criteria: PyMongo filter for documents to count in \"\"\" pipeline = self . _get_pipeline ( criteria = criteria ) pipeline . append ({ \"$count\" : \"count\" }) agg = list ( self . _collection . aggregate ( pipeline )) return agg [ 0 ] . get ( \"count\" , 0 ) if len ( agg ) > 0 else 0 Counts the number of documents matching the query criteria Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to count in None","title":"count()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.ensure_index","text":"Show source code in stores/compound_stores.py 130 131 132 133 134 def ensure_index ( self , key , unique = False , ** kwargs ): \"\"\" Can't ensure index for JointStore \"\"\" raise NotImplementedError ( \"No ensure_index method for JointStore\" ) Can't ensure index for JointStore","title":"ensure_index()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.groupby","text":"Show source code in stores/compound_stores.py 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def groupby ( self , keys : Union [ List [ str ], str ], criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Tuple [ Dict , List [ Dict ]]]: pipeline = self . _get_pipeline ( criteria = criteria , properties = properties , skip = skip , limit = limit ) if not isinstance ( keys , list ): keys = [ keys ] group_id = {} # type: Dict[str,Any] for key in keys : set_ ( group_id , key , \"$ {} \" . format ( key )) pipeline . append ({ \"$group\" : { \"_id\" : group_id , \"docs\" : { \"$push\" : \"$$ROOT\" }}}) agg = self . _collection . aggregate ( pipeline ) for d in agg : yield d [ \"_id\" ], d [ \"docs\" ] Simple grouping function that will group documents by keys. Parameters Name Type Description Default keys Union[List[str], str] fields to group documents required criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0 Returns Type Description Iterator[Tuple[Dict, List[Dict]]] generator returning tuples of (dict, list of docs)","title":"groupby()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.name","text":"Show source code in stores/compound_stores.py 55 56 57 58 59 60 def name ( self ) -> str : \"\"\" Return a string representing this data source \"\"\" compound_name = \",\" . join ( self . collection_names ) return f \"Compound[ { self . host } / { self . database } ][ { compound_name } ]\" Return a string representing this data source","title":"name()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.query","text":"Show source code in stores/compound_stores.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def query ( self , criteria : Optional [ Dict ] = None , properties : Union [ Dict , List , None ] = None , sort : Optional [ Dict [ str , Sort ]] = None , skip : int = 0 , limit : int = 0 , ) -> Iterator [ Dict ]: pipeline = self . _get_pipeline ( criteria = criteria , properties = properties , skip = skip , limit = limit ) agg = self . _collection . aggregate ( pipeline ) for d in agg : yield d Queries the Store for a set of documents Parameters Name Type Description Default criteria Optional[Dict] PyMongo filter for documents to search in None properties Optional[Union[Dict, List]] properties to return in grouped documents None sort Optional[Dict[str, maggma.core.store.Sort]] Dictionary of sort order for fields None skip int number documents to skip 0 limit int limit on total number of documents returned 0","title":"query()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.query_one","text":"Show source code in stores/compound_stores.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 def query_one ( self , criteria = None , properties = None , ** kwargs ): \"\"\" Get one document Args: properties: properties to return in query criteria: filter for matching kwargs: kwargs for collection.aggregate Returns: single document \"\"\" # TODO: maybe adding explicit limit in agg pipeline is better as below? # pipeline = self._get_pipeline(properties, criteria) # pipeline.append({\"$limit\": 1}) query = self . query ( criteria = criteria , properties = properties , ** kwargs ) try : doc = next ( query ) return doc except StopIteration : return None Get one document Parameters Name Type Description Default properties properties to return in query None criteria filter for matching None **kwargs kwargs for collection.aggregate {} Returns Type Description `` single document","title":"query_one()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.remove_docs","text":"Show source code in stores/compound_stores.py 288 289 290 291 292 293 294 295 def remove_docs ( self , criteria : Dict ): \"\"\" Remove docs matching the query dictionary Args: criteria: query dictionary to match \"\"\" raise NotImplementedError ( \"No remove_docs method for JointStore\" ) Remove docs matching the query dictionary Parameters Name Type Description Default criteria Dict query dictionary to match required","title":"remove_docs()"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.update","text":"Show source code in stores/compound_stores.py 115 116 117 118 119 120 def update ( self , docs , update_lu = True , key = None , ** kwargs ): \"\"\" Update documents into the underlying collections Not Implemented for JointStore \"\"\" raise NotImplementedError ( \"JointStore is a read-only store\" ) Update documents into the underlying collections Not Implemented for JointStore","title":"update()"}]}